2025-02-04 00:22:17,055 - INFO - allennlp.common.params - random_seed = 13370
2025-02-04 00:22:17,055 - INFO - allennlp.common.params - numpy_seed = 1337
2025-02-04 00:22:17,056 - INFO - allennlp.common.params - pytorch_seed = 133
2025-02-04 00:22:17,058 - INFO - allennlp.common.checks - Pytorch version: 1.12.1+cpu
2025-02-04 00:22:17,058 - INFO - allennlp.common.params - type = default
2025-02-04 00:22:17,058 - INFO - allennlp.common.params - dataset_reader.type = simple_language_modeling
2025-02-04 00:22:17,059 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2025-02-04 00:22:17,059 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2025-02-04 00:22:17,059 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False
2025-02-04 00:22:17,060 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = just_spaces
2025-02-04 00:22:17,060 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id
2025-02-04 00:22:17,060 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens
2025-02-04 00:22:17,060 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True
2025-02-04 00:22:17,061 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None
2025-02-04 00:22:17,061 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None
2025-02-04 00:22:17,061 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.feature_name = text
2025-02-04 00:22:17,061 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING
2025-02-04 00:22:17,061 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2025-02-04 00:22:17,061 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = None
2025-02-04 00:22:17,061 - INFO - allennlp.common.params - dataset_reader.start_tokens = None
2025-02-04 00:22:17,062 - INFO - allennlp.common.params - dataset_reader.end_tokens = None
2025-02-04 00:22:17,062 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Creating SimpleLanguageModelingDatasetReader
2025-02-04 00:22:17,062 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - max_sequence_length=None
2025-02-04 00:22:17,062 - INFO - allennlp.common.params - train_data_path = ./Pre-training/trainingData_ELMO.txt
2025-02-04 00:22:17,063 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x000002DF23546B20>
2025-02-04 00:22:17,063 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2025-02-04 00:22:17,063 - INFO - allennlp.common.params - validation_dataset_reader = None
2025-02-04 00:22:17,063 - INFO - allennlp.common.params - validation_data_path = ./Pre-training/testData_ELMO.txt
2025-02-04 00:22:17,063 - INFO - allennlp.common.params - validation_data_loader = None
2025-02-04 00:22:17,063 - INFO - allennlp.common.params - test_data_path = None
2025-02-04 00:22:17,064 - INFO - allennlp.common.params - evaluate_on_test = False
2025-02-04 00:22:17,064 - INFO - allennlp.common.params - batch_weight_key = 
2025-02-04 00:22:17,064 - CRITICAL - root - Uncaught exception
Traceback (most recent call last):
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\Scripts\allennlp.exe\__main__.py", line 7, in <module>
    sys.exit(run())
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\__main__.py", line 39, in run
    main(prog="allennlp")
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\__init__.py", line 120, in main
    args.func(args)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 111, in train_model_from_args
    train_model_from_file(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 177, in train_model_from_file
    return train_model(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 258, in train_model
    model = _train_worker(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 494, in _train_worker
    train_loop = TrainModel.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 636, in from_params
    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 222, in create_kwargs
    params.assert_empty(cls.__name__)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\params.py", line 394, in assert_empty
    raise ConfigurationError(
allennlp.common.checks.ConfigurationError: Extra parameters passed to TrainModel: {'checkpointer': {'keep_serialized_model_every_num_seconds': 3600}}
2025-02-04 00:27:55,260 - INFO - allennlp.common.params - random_seed = 13370
2025-02-04 00:27:55,260 - INFO - allennlp.common.params - numpy_seed = 1337
2025-02-04 00:27:55,260 - INFO - allennlp.common.params - pytorch_seed = 133
2025-02-04 00:27:55,262 - INFO - allennlp.common.checks - Pytorch version: 1.12.1+cpu
2025-02-04 00:27:55,263 - INFO - allennlp.common.params - type = default
2025-02-04 00:27:55,264 - INFO - allennlp.common.params - dataset_reader.type = simple_language_modeling
2025-02-04 00:27:55,264 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2025-02-04 00:27:55,264 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2025-02-04 00:27:55,264 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False
2025-02-04 00:27:55,265 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = just_spaces
2025-02-04 00:27:55,265 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id
2025-02-04 00:27:55,265 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens
2025-02-04 00:27:55,266 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True
2025-02-04 00:27:55,266 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None
2025-02-04 00:27:55,266 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None
2025-02-04 00:27:55,266 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.feature_name = text
2025-02-04 00:27:55,267 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING
2025-02-04 00:27:55,267 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2025-02-04 00:27:55,267 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = None
2025-02-04 00:27:55,267 - INFO - allennlp.common.params - dataset_reader.start_tokens = None
2025-02-04 00:27:55,268 - INFO - allennlp.common.params - dataset_reader.end_tokens = None
2025-02-04 00:27:55,268 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Creating SimpleLanguageModelingDatasetReader
2025-02-04 00:27:55,268 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - max_sequence_length=None
2025-02-04 00:27:55,269 - INFO - allennlp.common.params - train_data_path = ./Pre-training/trainingData_ELMO.txt
2025-02-04 00:27:55,269 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x000001DA06C079D0>
2025-02-04 00:27:55,269 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2025-02-04 00:27:55,270 - INFO - allennlp.common.params - validation_dataset_reader = None
2025-02-04 00:27:55,270 - INFO - allennlp.common.params - validation_data_path = ./Pre-training/testData_ELMO.txt
2025-02-04 00:27:55,270 - INFO - allennlp.common.params - validation_data_loader = None
2025-02-04 00:27:55,270 - INFO - allennlp.common.params - test_data_path = None
2025-02-04 00:27:55,270 - INFO - allennlp.common.params - evaluate_on_test = False
2025-02-04 00:27:55,270 - INFO - allennlp.common.params - batch_weight_key = 
2025-02-04 00:27:55,271 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:27:55,271 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:27:55,271 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:27:55,271 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:27:55,271 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:27:55,271 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:27:55,272 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:27:55,272 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:27:55,272 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:27:55,272 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:27:55,272 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:27:55,272 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x000001DA04508940>
2025-02-04 00:27:55,273 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:27:55,273 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from ./Pre-training/trainingData_ELMO.txt
2025-02-04 00:27:55,274 - CRITICAL - root - Uncaught exception
Traceback (most recent call last):
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\Scripts\allennlp.exe\__main__.py", line 7, in <module>
    sys.exit(run())
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\__main__.py", line 39, in run
    main(prog="allennlp")
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\__init__.py", line 120, in main
    args.func(args)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 111, in train_model_from_args
    train_model_from_file(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 177, in train_model_from_file
    return train_model(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 258, in train_model
    model = _train_worker(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 494, in _train_worker
    train_loop = TrainModel.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 638, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 716, in from_partial_objects
    "train": data_loader.construct(reader=dataset_reader, data_path=train_data_path)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 82, in construct
    return self.constructor(**contructor_kwargs)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 66, in constructor_to_use
    return self._constructor.from_params(  # type: ignore[union-attr]
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 638, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\data\data_loaders\multiprocess_data_loader.py", line 296, in __init__
    deque(self.iter_instances(), maxlen=0)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\data\data_loaders\multiprocess_data_loader.py", line 360, in iter_instances
    for instance in self._maybe_tqdm(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\tqdm\std.py", line 1181, in __iter__
    for obj in iterable:
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\data\dataset_readers\dataset_reader.py", line 192, in read
    for instance in self._multi_worker_islice(self._read(file_path)):  # type: ignore
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp_models\lm\dataset_readers\simple_language_modeling.py", line 81, in _read
    with open(file_path) as file:
FileNotFoundError: [Errno 2] No such file or directory: './Pre-training/trainingData_ELMO.txt'
2025-02-04 00:29:32,765 - INFO - allennlp.common.params - random_seed = 13370
2025-02-04 00:29:32,765 - INFO - allennlp.common.params - numpy_seed = 1337
2025-02-04 00:29:32,765 - INFO - allennlp.common.params - pytorch_seed = 133
2025-02-04 00:29:32,766 - INFO - allennlp.common.checks - Pytorch version: 1.12.1+cpu
2025-02-04 00:29:32,767 - INFO - allennlp.common.params - type = default
2025-02-04 00:29:32,767 - INFO - allennlp.common.params - dataset_reader.type = simple_language_modeling
2025-02-04 00:29:32,768 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2025-02-04 00:29:32,768 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2025-02-04 00:29:32,768 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False
2025-02-04 00:29:32,768 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = just_spaces
2025-02-04 00:29:32,769 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id
2025-02-04 00:29:32,769 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens
2025-02-04 00:29:32,769 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True
2025-02-04 00:29:32,769 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None
2025-02-04 00:29:32,769 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None
2025-02-04 00:29:32,769 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.feature_name = text
2025-02-04 00:29:32,770 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING
2025-02-04 00:29:32,770 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2025-02-04 00:29:32,770 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = None
2025-02-04 00:29:32,770 - INFO - allennlp.common.params - dataset_reader.start_tokens = None
2025-02-04 00:29:32,770 - INFO - allennlp.common.params - dataset_reader.end_tokens = None
2025-02-04 00:29:32,771 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Creating SimpleLanguageModelingDatasetReader
2025-02-04 00:29:32,771 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - max_sequence_length=None
2025-02-04 00:29:32,771 - INFO - allennlp.common.params - train_data_path = trainingData_ELMO.txt
2025-02-04 00:29:32,772 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x000001EE25677A30>
2025-02-04 00:29:32,772 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2025-02-04 00:29:32,772 - INFO - allennlp.common.params - validation_dataset_reader = None
2025-02-04 00:29:32,772 - INFO - allennlp.common.params - validation_data_path = testData_ELMO.txt
2025-02-04 00:29:32,772 - INFO - allennlp.common.params - validation_data_loader = None
2025-02-04 00:29:32,773 - INFO - allennlp.common.params - test_data_path = None
2025-02-04 00:29:32,773 - INFO - allennlp.common.params - evaluate_on_test = False
2025-02-04 00:29:32,773 - INFO - allennlp.common.params - batch_weight_key = 
2025-02-04 00:29:32,773 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:29:32,774 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:29:32,774 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:29:32,774 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:29:32,774 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:29:32,774 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:29:32,774 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:29:32,775 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:29:32,775 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:29:32,775 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:29:32,775 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:29:32,775 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x000001EE22F889A0>
2025-02-04 00:29:32,776 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:29:32,776 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from trainingData_ELMO.txt
2025-02-04 00:29:33,586 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from trainingData_ELMO.txt.
2025-02-04 00:29:33,586 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:29:33,587 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:29:33,587 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:29:33,587 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:29:33,587 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:29:33,587 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:29:33,588 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:29:33,588 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:29:33,588 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:29:33,588 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:29:33,588 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:29:33,589 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x000001EE22F889A0>
2025-02-04 00:29:33,589 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:29:33,589 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from testData_ELMO.txt
2025-02-04 00:29:33,866 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from testData_ELMO.txt.
2025-02-04 00:29:33,866 - INFO - allennlp.common.params - type = from_instances
2025-02-04 00:29:33,867 - INFO - allennlp.common.params - min_count = None
2025-02-04 00:29:33,867 - INFO - allennlp.common.params - max_vocab_size = None
2025-02-04 00:29:33,867 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2025-02-04 00:29:33,868 - INFO - allennlp.common.params - pretrained_files = None
2025-02-04 00:29:33,868 - INFO - allennlp.common.params - only_include_pretrained_words = False
2025-02-04 00:29:33,868 - INFO - allennlp.common.params - tokens_to_add = None
2025-02-04 00:29:33,868 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2025-02-04 00:29:33,869 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2025-02-04 00:29:33,869 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2025-02-04 00:29:33,869 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2025-02-04 00:29:33,869 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2025-02-04 00:29:34,169 - INFO - allennlp.common.params - model.type = language_model
2025-02-04 00:29:34,170 - INFO - allennlp.common.params - model.regularizer = None
2025-02-04 00:29:34,170 - CRITICAL - root - Uncaught exception
Traceback (most recent call last):
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\params.py", line 211, in pop
    value = self.params.pop(key)
KeyError: 'text_field_embedder'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\Scripts\allennlp.exe\__main__.py", line 7, in <module>
    sys.exit(run())
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\__main__.py", line 39, in run
    main(prog="allennlp")
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\__init__.py", line 120, in main
    args.func(args)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 111, in train_model_from_args
    train_model_from_file(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 177, in train_model_from_file
    return train_model(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 258, in train_model
    model = _train_worker(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 494, in _train_worker
    train_loop = TrainModel.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 638, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 770, in from_partial_objects
    model_ = model.construct(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 82, in construct
    return self.constructor(**contructor_kwargs)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 66, in constructor_to_use
    return self._constructor.from_params(  # type: ignore[union-attr]
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 636, in from_params
    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 206, in create_kwargs
    constructed_arg = pop_and_construct_arg(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 310, in pop_and_construct_arg
    popped_params = params.pop(name, default) if default != _NO_DEFAULT else params.pop(name)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\params.py", line 216, in pop
    raise ConfigurationError(msg)
allennlp.common.checks.ConfigurationError: key "text_field_embedder" is required at location "model."
2025-02-04 00:34:11,446 - INFO - allennlp.common.params - random_seed = 13370
2025-02-04 00:34:11,447 - INFO - allennlp.common.params - numpy_seed = 1337
2025-02-04 00:34:11,447 - INFO - allennlp.common.params - pytorch_seed = 133
2025-02-04 00:34:11,449 - INFO - allennlp.common.checks - Pytorch version: 1.12.1+cpu
2025-02-04 00:34:11,450 - INFO - allennlp.common.params - type = default
2025-02-04 00:34:11,450 - INFO - allennlp.common.params - dataset_reader.type = simple_language_modeling
2025-02-04 00:34:11,450 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2025-02-04 00:34:11,450 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2025-02-04 00:34:11,451 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False
2025-02-04 00:34:11,451 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = just_spaces
2025-02-04 00:34:11,451 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id
2025-02-04 00:34:11,451 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens
2025-02-04 00:34:11,452 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True
2025-02-04 00:34:11,452 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None
2025-02-04 00:34:11,452 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None
2025-02-04 00:34:11,452 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.feature_name = text
2025-02-04 00:34:11,452 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING
2025-02-04 00:34:11,452 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2025-02-04 00:34:11,453 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = None
2025-02-04 00:34:11,453 - INFO - allennlp.common.params - dataset_reader.start_tokens = None
2025-02-04 00:34:11,453 - INFO - allennlp.common.params - dataset_reader.end_tokens = None
2025-02-04 00:34:11,453 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Creating SimpleLanguageModelingDatasetReader
2025-02-04 00:34:11,453 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - max_sequence_length=None
2025-02-04 00:34:11,454 - INFO - allennlp.common.params - train_data_path = trainingData_ELMO.txt
2025-02-04 00:34:11,454 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x000002A9BA715460>
2025-02-04 00:34:11,454 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2025-02-04 00:34:11,454 - INFO - allennlp.common.params - validation_dataset_reader = None
2025-02-04 00:34:11,454 - INFO - allennlp.common.params - validation_data_path = testData_ELMO.txt
2025-02-04 00:34:11,455 - INFO - allennlp.common.params - validation_data_loader = None
2025-02-04 00:34:11,455 - INFO - allennlp.common.params - test_data_path = None
2025-02-04 00:34:11,455 - INFO - allennlp.common.params - evaluate_on_test = False
2025-02-04 00:34:11,455 - INFO - allennlp.common.params - batch_weight_key = 
2025-02-04 00:34:11,456 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:34:11,456 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:34:11,456 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:34:11,456 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:34:11,456 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:34:11,457 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:34:11,457 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:34:11,457 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:34:11,457 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:34:11,457 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:34:11,458 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:34:11,458 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x000002A9B801D3D0>
2025-02-04 00:34:11,458 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:34:11,458 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from trainingData_ELMO.txt
2025-02-04 00:34:12,284 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from trainingData_ELMO.txt.
2025-02-04 00:34:12,285 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:34:12,285 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:34:12,286 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:34:12,286 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:34:12,286 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:34:12,286 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:34:12,286 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:34:12,286 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:34:12,286 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:34:12,287 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:34:12,287 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:34:12,287 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x000002A9B801D3D0>
2025-02-04 00:34:12,287 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:34:12,287 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from testData_ELMO.txt
2025-02-04 00:34:12,571 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from testData_ELMO.txt.
2025-02-04 00:34:12,571 - INFO - allennlp.common.params - type = from_instances
2025-02-04 00:34:12,572 - INFO - allennlp.common.params - min_count = None
2025-02-04 00:34:12,572 - INFO - allennlp.common.params - max_vocab_size = None
2025-02-04 00:34:12,572 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2025-02-04 00:34:12,572 - INFO - allennlp.common.params - pretrained_files = None
2025-02-04 00:34:12,573 - INFO - allennlp.common.params - only_include_pretrained_words = False
2025-02-04 00:34:12,573 - INFO - allennlp.common.params - tokens_to_add = None
2025-02-04 00:34:12,573 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2025-02-04 00:34:12,573 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2025-02-04 00:34:12,573 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2025-02-04 00:34:12,574 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2025-02-04 00:34:12,574 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2025-02-04 00:34:12,866 - INFO - allennlp.common.params - model.type = lstm
2025-02-04 00:34:12,867 - CRITICAL - root - Uncaught exception
Traceback (most recent call last):
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\Scripts\allennlp.exe\__main__.py", line 7, in <module>
    sys.exit(run())
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\__main__.py", line 39, in run
    main(prog="allennlp")
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\__init__.py", line 120, in main
    args.func(args)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 111, in train_model_from_args
    train_model_from_file(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 177, in train_model_from_file
    return train_model(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 258, in train_model
    model = _train_worker(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 494, in _train_worker
    train_loop = TrainModel.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 638, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 770, in from_partial_objects
    model_ = model.construct(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 82, in construct
    return self.constructor(**contructor_kwargs)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 66, in constructor_to_use
    return self._constructor.from_params(  # type: ignore[union-attr]
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 585, in from_params
    choice = params.pop_choice(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\params.py", line 324, in pop_choice
    raise ConfigurationError(message)
allennlp.common.checks.ConfigurationError: lstm not in acceptable choices for model.type: ['from_archive', 'basic_classifier', 'multitask', 'simple_tagger', 'bias_mitigator_applicator', 'adversarial_bias_mitigator', 'feedforward_regression_adversary', 'bcn', 'transformer_classification_tt', 'coref', 'language_model', 'bidirectional-language-model', 'masked_language_model', 'next_token_lm', 'composed_seq2seq', 'copynet_seq2seq', 'simple_seq2seq', 'bart', 't5', 'transformer_mc', 'transformer_mc_tt', 'bimpm', 'decomposable_attention', 'esim', 'bidaf', 'bidaf-ensemble', 'dialog_qa', 'naqanet', 'rc-qanet', 'qanet', 'transformer_qa', 'biaffine_parser', 'constituency_parser', 'sp-graph-parser', 'graph_parser', 'srl', 'srl_bert', 'crf_tagger', 'vision_model', 'nlvr2_from_huggingface', 'nlvr2', 've_vilbert_from_huggingface', 've_vilbert', 'vilbert_ir_from_huggingface', 'vilbert_ir', 'vqa_vilbert_from_huggingface', 'vqa_vilbert']. You should either use the --include-package flag to make sure the correct module is loaded, or use a fully qualified class name in your config file like {"model": "my_module.models.MyModel"} to have it imported automatically.
2025-02-04 00:38:30,130 - INFO - allennlp.common.params - random_seed = 13370
2025-02-04 00:38:30,130 - INFO - allennlp.common.params - numpy_seed = 1337
2025-02-04 00:38:30,130 - INFO - allennlp.common.params - pytorch_seed = 133
2025-02-04 00:38:30,132 - INFO - allennlp.common.checks - Pytorch version: 1.12.1+cpu
2025-02-04 00:38:30,132 - INFO - allennlp.common.params - type = default
2025-02-04 00:38:30,133 - INFO - allennlp.common.params - dataset_reader.type = simple_language_modeling
2025-02-04 00:38:30,133 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2025-02-04 00:38:30,133 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2025-02-04 00:38:30,133 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False
2025-02-04 00:38:30,134 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = just_spaces
2025-02-04 00:38:30,134 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id
2025-02-04 00:38:30,134 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens
2025-02-04 00:38:30,135 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True
2025-02-04 00:38:30,135 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None
2025-02-04 00:38:30,135 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None
2025-02-04 00:38:30,135 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.feature_name = text
2025-02-04 00:38:30,135 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING
2025-02-04 00:38:30,135 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2025-02-04 00:38:30,135 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = None
2025-02-04 00:38:30,136 - INFO - allennlp.common.params - dataset_reader.start_tokens = None
2025-02-04 00:38:30,136 - INFO - allennlp.common.params - dataset_reader.end_tokens = None
2025-02-04 00:38:30,136 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Creating SimpleLanguageModelingDatasetReader
2025-02-04 00:38:30,136 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - max_sequence_length=None
2025-02-04 00:38:30,136 - INFO - allennlp.common.params - train_data_path = trainingData_ELMO.txt
2025-02-04 00:38:30,137 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x000002919B9D5D00>
2025-02-04 00:38:30,137 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2025-02-04 00:38:30,137 - INFO - allennlp.common.params - validation_dataset_reader = None
2025-02-04 00:38:30,137 - INFO - allennlp.common.params - validation_data_path = testData_ELMO.txt
2025-02-04 00:38:30,137 - INFO - allennlp.common.params - validation_data_loader = None
2025-02-04 00:38:30,138 - INFO - allennlp.common.params - test_data_path = None
2025-02-04 00:38:30,138 - INFO - allennlp.common.params - evaluate_on_test = False
2025-02-04 00:38:30,138 - INFO - allennlp.common.params - batch_weight_key = 
2025-02-04 00:38:30,138 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:38:30,138 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:38:30,139 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:38:30,139 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:38:30,139 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:38:30,139 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:38:30,139 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:38:30,139 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:38:30,139 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:38:30,140 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:38:30,140 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:38:30,140 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x00000291992D5C70>
2025-02-04 00:38:30,140 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:38:30,140 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from trainingData_ELMO.txt
2025-02-04 00:38:31,008 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from trainingData_ELMO.txt.
2025-02-04 00:38:31,009 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:38:31,010 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:38:31,010 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:38:31,010 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:38:31,010 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:38:31,010 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:38:31,010 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:38:31,011 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:38:31,011 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:38:31,011 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:38:31,011 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:38:31,011 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x00000291992D5C70>
2025-02-04 00:38:31,012 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:38:31,012 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from testData_ELMO.txt
2025-02-04 00:38:31,320 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from testData_ELMO.txt.
2025-02-04 00:38:31,321 - INFO - allennlp.common.params - type = from_instances
2025-02-04 00:38:31,321 - INFO - allennlp.common.params - min_count = None
2025-02-04 00:38:31,322 - INFO - allennlp.common.params - max_vocab_size = None
2025-02-04 00:38:31,322 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2025-02-04 00:38:31,322 - INFO - allennlp.common.params - pretrained_files = None
2025-02-04 00:38:31,322 - INFO - allennlp.common.params - only_include_pretrained_words = False
2025-02-04 00:38:31,323 - INFO - allennlp.common.params - tokens_to_add = None
2025-02-04 00:38:31,323 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2025-02-04 00:38:31,323 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2025-02-04 00:38:31,324 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2025-02-04 00:38:31,324 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2025-02-04 00:38:31,324 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2025-02-04 00:38:31,636 - INFO - allennlp.common.params - model.type = bidirectional-language-model
2025-02-04 00:38:31,637 - INFO - allennlp.common.params - model.regularizer = None
2025-02-04 00:38:31,637 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2025-02-04 00:38:31,637 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = embedding
2025-02-04 00:38:31,638 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.embedding_dim = 128
2025-02-04 00:38:31,638 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.num_embeddings = None
2025-02-04 00:38:31,638 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.projection_dim = None
2025-02-04 00:38:31,638 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.weight = None
2025-02-04 00:38:31,639 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.padding_index = None
2025-02-04 00:38:31,639 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.trainable = True
2025-02-04 00:38:31,639 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_norm = None
2025-02-04 00:38:31,639 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.norm_type = 2.0
2025-02-04 00:38:31,639 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False
2025-02-04 00:38:31,640 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sparse = False
2025-02-04 00:38:31,640 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens
2025-02-04 00:38:31,640 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.pretrained_file = path/to/glove.6B.100d.txt
2025-02-04 00:38:31,642 - INFO - allennlp.modules.token_embedders.embedding - Reading pretrained embeddings from file
2025-02-04 00:38:31,643 - CRITICAL - root - Uncaught exception
Traceback (most recent call last):
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\Scripts\allennlp.exe\__main__.py", line 7, in <module>
    sys.exit(run())
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\__main__.py", line 39, in run
    main(prog="allennlp")
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\__init__.py", line 120, in main
    args.func(args)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 111, in train_model_from_args
    train_model_from_file(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 177, in train_model_from_file
    return train_model(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 258, in train_model
    model = _train_worker(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 494, in _train_worker
    train_loop = TrainModel.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 638, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 770, in from_partial_objects
    model_ = model.construct(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 82, in construct
    return self.constructor(**contructor_kwargs)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 66, in constructor_to_use
    return self._constructor.from_params(  # type: ignore[union-attr]
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 636, in from_params
    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 206, in create_kwargs
    constructed_arg = pop_and_construct_arg(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 314, in pop_and_construct_arg
    return construct_arg(class_name, name, popped_params, annotation, default, **extras)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 348, in construct_arg
    result = annotation.from_params(params=popped_params, **subextras)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 636, in from_params
    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 206, in create_kwargs
    constructed_arg = pop_and_construct_arg(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 314, in pop_and_construct_arg
    return construct_arg(class_name, name, popped_params, annotation, default, **extras)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 394, in construct_arg
    value_dict[key] = construct_arg(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 348, in construct_arg
    result = annotation.from_params(params=popped_params, **subextras)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 638, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\modules\token_embedders\embedding.py", line 155, in __init__
    weight = _read_pretrained_embeddings_file(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\modules\token_embedders\embedding.py", line 368, in _read_pretrained_embeddings_file
    return _read_embeddings_from_text_file(file_uri, embedding_dim, vocab, namespace)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\modules\token_embedders\embedding.py", line 390, in _read_embeddings_from_text_file
    with EmbeddingsTextFile(file_uri) as embeddings_file:
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\modules\token_embedders\embedding.py", line 525, in __init__
    main_file_local_path = cached_path(main_file_uri, cache_dir=cache_dir)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\file_utils.py", line 134, in cached_path
    _cached_path.cached_path(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\cached_path\_cached_path.py", line 205, in cached_path
    raise FileNotFoundError(f"file {url_or_filename} not found")
FileNotFoundError: file path\to\glove.6B.100d.txt not found
2025-02-04 00:39:01,965 - INFO - allennlp.common.params - random_seed = 13370
2025-02-04 00:39:01,965 - INFO - allennlp.common.params - numpy_seed = 1337
2025-02-04 00:39:01,966 - INFO - allennlp.common.params - pytorch_seed = 133
2025-02-04 00:39:01,967 - INFO - allennlp.common.checks - Pytorch version: 1.12.1+cpu
2025-02-04 00:39:01,968 - INFO - allennlp.common.params - type = default
2025-02-04 00:39:01,968 - INFO - allennlp.common.params - dataset_reader.type = simple_language_modeling
2025-02-04 00:39:01,968 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2025-02-04 00:39:01,969 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2025-02-04 00:39:01,969 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False
2025-02-04 00:39:01,969 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = just_spaces
2025-02-04 00:39:01,969 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id
2025-02-04 00:39:01,970 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens
2025-02-04 00:39:01,970 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True
2025-02-04 00:39:01,970 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None
2025-02-04 00:39:01,970 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None
2025-02-04 00:39:01,970 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.feature_name = text
2025-02-04 00:39:01,970 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING
2025-02-04 00:39:01,971 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2025-02-04 00:39:01,971 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = None
2025-02-04 00:39:01,971 - INFO - allennlp.common.params - dataset_reader.start_tokens = None
2025-02-04 00:39:01,971 - INFO - allennlp.common.params - dataset_reader.end_tokens = None
2025-02-04 00:39:01,971 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Creating SimpleLanguageModelingDatasetReader
2025-02-04 00:39:01,972 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - max_sequence_length=None
2025-02-04 00:39:01,972 - INFO - allennlp.common.params - train_data_path = trainingData_ELMO.txt
2025-02-04 00:39:01,972 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x00000236B8950730>
2025-02-04 00:39:01,973 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2025-02-04 00:39:01,973 - INFO - allennlp.common.params - validation_dataset_reader = None
2025-02-04 00:39:01,973 - INFO - allennlp.common.params - validation_data_path = testData_ELMO.txt
2025-02-04 00:39:01,973 - INFO - allennlp.common.params - validation_data_loader = None
2025-02-04 00:39:01,973 - INFO - allennlp.common.params - test_data_path = None
2025-02-04 00:39:01,974 - INFO - allennlp.common.params - evaluate_on_test = False
2025-02-04 00:39:01,974 - INFO - allennlp.common.params - batch_weight_key = 
2025-02-04 00:39:01,974 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:39:01,974 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:39:01,974 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:39:01,975 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:39:01,975 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:39:01,975 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:39:01,975 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:39:01,975 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:39:01,975 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:39:01,975 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:39:01,976 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:39:01,976 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x00000236B625B6A0>
2025-02-04 00:39:01,976 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:39:01,976 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from trainingData_ELMO.txt
2025-02-04 00:39:02,786 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from trainingData_ELMO.txt.
2025-02-04 00:39:02,787 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:39:02,787 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:39:02,787 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:39:02,787 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:39:02,788 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:39:02,788 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:39:02,788 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:39:02,788 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:39:02,788 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:39:02,788 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:39:02,789 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:39:02,789 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x00000236B625B6A0>
2025-02-04 00:39:02,789 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:39:02,789 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from testData_ELMO.txt
2025-02-04 00:39:03,067 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from testData_ELMO.txt.
2025-02-04 00:39:03,067 - INFO - allennlp.common.params - type = from_instances
2025-02-04 00:39:03,068 - INFO - allennlp.common.params - min_count = None
2025-02-04 00:39:03,068 - INFO - allennlp.common.params - max_vocab_size = None
2025-02-04 00:39:03,068 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2025-02-04 00:39:03,068 - INFO - allennlp.common.params - pretrained_files = None
2025-02-04 00:39:03,069 - INFO - allennlp.common.params - only_include_pretrained_words = False
2025-02-04 00:39:03,069 - INFO - allennlp.common.params - tokens_to_add = None
2025-02-04 00:39:03,069 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2025-02-04 00:39:03,069 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2025-02-04 00:39:03,069 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2025-02-04 00:39:03,070 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2025-02-04 00:39:03,070 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2025-02-04 00:39:03,383 - INFO - allennlp.common.params - model.type = bidirectional-language-model
2025-02-04 00:39:03,384 - INFO - allennlp.common.params - model.regularizer = None
2025-02-04 00:39:03,385 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2025-02-04 00:39:03,385 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = embedding
2025-02-04 00:39:03,385 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.embedding_dim = 128
2025-02-04 00:39:03,385 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.num_embeddings = None
2025-02-04 00:39:03,385 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.projection_dim = None
2025-02-04 00:39:03,386 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.weight = None
2025-02-04 00:39:03,386 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.padding_index = None
2025-02-04 00:39:03,386 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.trainable = True
2025-02-04 00:39:03,386 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_norm = None
2025-02-04 00:39:03,386 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.norm_type = 2.0
2025-02-04 00:39:03,386 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False
2025-02-04 00:39:03,386 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sparse = False
2025-02-04 00:39:03,387 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens
2025-02-04 00:39:03,387 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.pretrained_file = None
2025-02-04 00:39:03,437 - CRITICAL - root - Uncaught exception
Traceback (most recent call last):
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\params.py", line 211, in pop
    value = self.params.pop(key)
KeyError: 'contextualizer'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\Scripts\allennlp.exe\__main__.py", line 7, in <module>
    sys.exit(run())
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\__main__.py", line 39, in run
    main(prog="allennlp")
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\__init__.py", line 120, in main
    args.func(args)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 111, in train_model_from_args
    train_model_from_file(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 177, in train_model_from_file
    return train_model(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 258, in train_model
    model = _train_worker(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 494, in _train_worker
    train_loop = TrainModel.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 638, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 770, in from_partial_objects
    model_ = model.construct(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 82, in construct
    return self.constructor(**contructor_kwargs)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 66, in constructor_to_use
    return self._constructor.from_params(  # type: ignore[union-attr]
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 636, in from_params
    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 206, in create_kwargs
    constructed_arg = pop_and_construct_arg(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 310, in pop_and_construct_arg
    popped_params = params.pop(name, default) if default != _NO_DEFAULT else params.pop(name)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\params.py", line 216, in pop
    raise ConfigurationError(msg)
allennlp.common.checks.ConfigurationError: key "contextualizer" is required at location "model."
2025-02-04 00:46:29,249 - INFO - allennlp.common.params - random_seed = 13370
2025-02-04 00:46:29,249 - INFO - allennlp.common.params - numpy_seed = 1337
2025-02-04 00:46:29,249 - INFO - allennlp.common.params - pytorch_seed = 133
2025-02-04 00:46:29,251 - INFO - allennlp.common.checks - Pytorch version: 1.12.1+cpu
2025-02-04 00:46:29,251 - INFO - allennlp.common.params - type = default
2025-02-04 00:46:29,252 - INFO - allennlp.common.params - dataset_reader.type = simple_language_modeling
2025-02-04 00:46:29,252 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2025-02-04 00:46:29,252 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2025-02-04 00:46:29,252 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False
2025-02-04 00:46:29,253 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = just_spaces
2025-02-04 00:46:29,253 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id
2025-02-04 00:46:29,253 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens
2025-02-04 00:46:29,253 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True
2025-02-04 00:46:29,254 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None
2025-02-04 00:46:29,254 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None
2025-02-04 00:46:29,254 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.feature_name = text
2025-02-04 00:46:29,254 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING
2025-02-04 00:46:29,254 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2025-02-04 00:46:29,254 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = None
2025-02-04 00:46:29,255 - INFO - allennlp.common.params - dataset_reader.start_tokens = None
2025-02-04 00:46:29,255 - INFO - allennlp.common.params - dataset_reader.end_tokens = None
2025-02-04 00:46:29,255 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Creating SimpleLanguageModelingDatasetReader
2025-02-04 00:46:29,255 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - max_sequence_length=None
2025-02-04 00:46:29,256 - INFO - allennlp.common.params - train_data_path = trainingData_ELMO.txt
2025-02-04 00:46:29,256 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x000001B127097A30>
2025-02-04 00:46:29,256 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2025-02-04 00:46:29,256 - INFO - allennlp.common.params - validation_dataset_reader = None
2025-02-04 00:46:29,257 - INFO - allennlp.common.params - validation_data_path = testData_ELMO.txt
2025-02-04 00:46:29,257 - INFO - allennlp.common.params - validation_data_loader = None
2025-02-04 00:46:29,257 - INFO - allennlp.common.params - test_data_path = None
2025-02-04 00:46:29,257 - INFO - allennlp.common.params - evaluate_on_test = False
2025-02-04 00:46:29,257 - INFO - allennlp.common.params - batch_weight_key = 
2025-02-04 00:46:29,258 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:46:29,258 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:46:29,258 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:46:29,258 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:46:29,258 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:46:29,259 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:46:29,259 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:46:29,259 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:46:29,259 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:46:29,259 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:46:29,259 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:46:29,259 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x000001B1249A89A0>
2025-02-04 00:46:29,260 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:46:29,260 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from trainingData_ELMO.txt
2025-02-04 00:46:30,108 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from trainingData_ELMO.txt.
2025-02-04 00:46:30,108 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:46:30,109 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:46:30,109 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:46:30,109 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:46:30,109 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:46:30,109 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:46:30,110 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:46:30,110 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:46:30,110 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:46:30,110 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:46:30,110 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:46:30,110 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x000001B1249A89A0>
2025-02-04 00:46:30,111 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:46:30,111 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from testData_ELMO.txt
2025-02-04 00:46:30,394 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from testData_ELMO.txt.
2025-02-04 00:46:30,395 - INFO - allennlp.common.params - type = from_instances
2025-02-04 00:46:30,395 - INFO - allennlp.common.params - min_count = None
2025-02-04 00:46:30,395 - INFO - allennlp.common.params - max_vocab_size = None
2025-02-04 00:46:30,395 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2025-02-04 00:46:30,396 - INFO - allennlp.common.params - pretrained_files = None
2025-02-04 00:46:30,396 - INFO - allennlp.common.params - only_include_pretrained_words = False
2025-02-04 00:46:30,396 - INFO - allennlp.common.params - tokens_to_add = None
2025-02-04 00:46:30,397 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2025-02-04 00:46:30,397 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2025-02-04 00:46:30,397 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2025-02-04 00:46:30,397 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2025-02-04 00:46:30,398 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2025-02-04 00:46:30,687 - INFO - allennlp.common.params - model.type = bidirectional-language-model
2025-02-04 00:46:30,688 - INFO - allennlp.common.params - model.regularizer = None
2025-02-04 00:46:30,688 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2025-02-04 00:46:30,689 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = embedding
2025-02-04 00:46:30,689 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.embedding_dim = 128
2025-02-04 00:46:30,689 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.num_embeddings = None
2025-02-04 00:46:30,689 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.projection_dim = None
2025-02-04 00:46:30,689 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.weight = None
2025-02-04 00:46:30,689 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.padding_index = None
2025-02-04 00:46:30,690 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.trainable = True
2025-02-04 00:46:30,690 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_norm = None
2025-02-04 00:46:30,690 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.norm_type = 2.0
2025-02-04 00:46:30,690 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False
2025-02-04 00:46:30,690 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sparse = False
2025-02-04 00:46:30,690 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens
2025-02-04 00:46:30,690 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.pretrained_file = None
2025-02-04 00:46:30,715 - INFO - allennlp.common.params - model.contextualizer.type = lstm
2025-02-04 00:46:30,715 - INFO - allennlp.common.params - model.contextualizer.input_size = 300
2025-02-04 00:46:30,716 - INFO - allennlp.common.params - model.contextualizer.hidden_size = 256
2025-02-04 00:46:30,716 - INFO - allennlp.common.params - model.contextualizer.num_layers = 2
2025-02-04 00:46:30,716 - INFO - allennlp.common.params - model.contextualizer.bias = True
2025-02-04 00:46:30,716 - INFO - allennlp.common.params - model.contextualizer.dropout = 0.0
2025-02-04 00:46:30,716 - INFO - allennlp.common.params - model.contextualizer.bidirectional = True
2025-02-04 00:46:30,717 - INFO - allennlp.common.params - model.contextualizer.stateful = False
2025-02-04 00:46:30,743 - INFO - allennlp.common.params - model.dropout = 0.1
2025-02-04 00:46:30,743 - INFO - allennlp.common.params - model.num_samples = None
2025-02-04 00:46:30,743 - INFO - allennlp.common.params - model.sparse_embeddings = False
2025-02-04 00:46:30,743 - INFO - allennlp.common.params - model.bidirectional = True
2025-02-04 00:46:30,744 - INFO - allennlp.common.params - model.initializer = None
2025-02-04 00:46:30,744 - CRITICAL - root - Uncaught exception
Traceback (most recent call last):
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\Scripts\allennlp.exe\__main__.py", line 7, in <module>
    sys.exit(run())
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\__main__.py", line 39, in run
    main(prog="allennlp")
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\__init__.py", line 120, in main
    args.func(args)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 111, in train_model_from_args
    train_model_from_file(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 177, in train_model_from_file
    return train_model(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 258, in train_model
    model = _train_worker(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 494, in _train_worker
    train_loop = TrainModel.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 638, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 770, in from_partial_objects
    model_ = model.construct(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 82, in construct
    return self.constructor(**contructor_kwargs)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 66, in constructor_to_use
    return self._constructor.from_params(  # type: ignore[union-attr]
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 638, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp_models\lm\models\bidirectional_lm.py", line 53, in __init__
    super().__init__(
TypeError: allennlp_models.lm.models.language_model.LanguageModel.__init__() got multiple values for keyword argument 'bidirectional'
2025-02-04 00:46:59,244 - INFO - allennlp.common.params - random_seed = 13370
2025-02-04 00:46:59,244 - INFO - allennlp.common.params - numpy_seed = 1337
2025-02-04 00:46:59,244 - INFO - allennlp.common.params - pytorch_seed = 133
2025-02-04 00:46:59,246 - INFO - allennlp.common.checks - Pytorch version: 1.12.1+cpu
2025-02-04 00:46:59,247 - INFO - allennlp.common.params - type = default
2025-02-04 00:46:59,247 - INFO - allennlp.common.params - dataset_reader.type = simple_language_modeling
2025-02-04 00:46:59,248 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2025-02-04 00:46:59,248 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2025-02-04 00:46:59,248 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False
2025-02-04 00:46:59,248 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = just_spaces
2025-02-04 00:46:59,249 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id
2025-02-04 00:46:59,249 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens
2025-02-04 00:46:59,249 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True
2025-02-04 00:46:59,250 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None
2025-02-04 00:46:59,250 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None
2025-02-04 00:46:59,250 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.feature_name = text
2025-02-04 00:46:59,251 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING
2025-02-04 00:46:59,251 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2025-02-04 00:46:59,251 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = None
2025-02-04 00:46:59,251 - INFO - allennlp.common.params - dataset_reader.start_tokens = None
2025-02-04 00:46:59,252 - INFO - allennlp.common.params - dataset_reader.end_tokens = None
2025-02-04 00:46:59,252 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Creating SimpleLanguageModelingDatasetReader
2025-02-04 00:46:59,252 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - max_sequence_length=None
2025-02-04 00:46:59,253 - INFO - allennlp.common.params - train_data_path = trainingData_ELMO.txt
2025-02-04 00:46:59,253 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x000001D879596850>
2025-02-04 00:46:59,254 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2025-02-04 00:46:59,254 - INFO - allennlp.common.params - validation_dataset_reader = None
2025-02-04 00:46:59,254 - INFO - allennlp.common.params - validation_data_path = testData_ELMO.txt
2025-02-04 00:46:59,254 - INFO - allennlp.common.params - validation_data_loader = None
2025-02-04 00:46:59,254 - INFO - allennlp.common.params - test_data_path = None
2025-02-04 00:46:59,255 - INFO - allennlp.common.params - evaluate_on_test = False
2025-02-04 00:46:59,255 - INFO - allennlp.common.params - batch_weight_key = 
2025-02-04 00:46:59,255 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:46:59,256 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:46:59,256 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:46:59,256 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:46:59,256 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:46:59,257 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:46:59,257 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:46:59,257 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:46:59,257 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:46:59,258 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:46:59,258 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:46:59,258 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x000001D876EAA7C0>
2025-02-04 00:46:59,258 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:46:59,259 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from trainingData_ELMO.txt
2025-02-04 00:47:00,178 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from trainingData_ELMO.txt.
2025-02-04 00:47:00,178 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:47:00,179 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:47:00,179 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:47:00,179 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:47:00,179 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:47:00,179 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:47:00,180 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:47:00,180 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:47:00,180 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:47:00,180 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:47:00,180 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:47:00,181 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x000001D876EAA7C0>
2025-02-04 00:47:00,181 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:47:00,181 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from testData_ELMO.txt
2025-02-04 00:47:00,493 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from testData_ELMO.txt.
2025-02-04 00:47:00,495 - INFO - allennlp.common.params - type = from_instances
2025-02-04 00:47:00,495 - INFO - allennlp.common.params - min_count = None
2025-02-04 00:47:00,495 - INFO - allennlp.common.params - max_vocab_size = None
2025-02-04 00:47:00,495 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2025-02-04 00:47:00,496 - INFO - allennlp.common.params - pretrained_files = None
2025-02-04 00:47:00,496 - INFO - allennlp.common.params - only_include_pretrained_words = False
2025-02-04 00:47:00,496 - INFO - allennlp.common.params - tokens_to_add = None
2025-02-04 00:47:00,496 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2025-02-04 00:47:00,496 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2025-02-04 00:47:00,496 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2025-02-04 00:47:00,496 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2025-02-04 00:47:00,497 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2025-02-04 00:47:00,797 - INFO - allennlp.common.params - model.type = bidirectional-language-model
2025-02-04 00:47:00,798 - INFO - allennlp.common.params - model.regularizer = None
2025-02-04 00:47:00,798 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2025-02-04 00:47:00,799 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = embedding
2025-02-04 00:47:00,799 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.embedding_dim = 128
2025-02-04 00:47:00,799 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.num_embeddings = None
2025-02-04 00:47:00,799 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.projection_dim = None
2025-02-04 00:47:00,799 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.weight = None
2025-02-04 00:47:00,800 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.padding_index = None
2025-02-04 00:47:00,800 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.trainable = True
2025-02-04 00:47:00,800 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_norm = None
2025-02-04 00:47:00,800 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.norm_type = 2.0
2025-02-04 00:47:00,800 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False
2025-02-04 00:47:00,800 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sparse = False
2025-02-04 00:47:00,801 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens
2025-02-04 00:47:00,801 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.pretrained_file = None
2025-02-04 00:47:00,825 - INFO - allennlp.common.params - model.contextualizer.type = lstm
2025-02-04 00:47:00,826 - CRITICAL - root - Uncaught exception
Traceback (most recent call last):
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\params.py", line 211, in pop
    value = self.params.pop(key)
KeyError: 'input_size'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\Scripts\allennlp.exe\__main__.py", line 7, in <module>
    sys.exit(run())
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\__main__.py", line 39, in run
    main(prog="allennlp")
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\__init__.py", line 120, in main
    args.func(args)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 111, in train_model_from_args
    train_model_from_file(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 177, in train_model_from_file
    return train_model(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 258, in train_model
    model = _train_worker(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 494, in _train_worker
    train_loop = TrainModel.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 638, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 770, in from_partial_objects
    model_ = model.construct(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 82, in construct
    return self.constructor(**contructor_kwargs)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 66, in constructor_to_use
    return self._constructor.from_params(  # type: ignore[union-attr]
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 636, in from_params
    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 206, in create_kwargs
    constructed_arg = pop_and_construct_arg(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 314, in pop_and_construct_arg
    return construct_arg(class_name, name, popped_params, annotation, default, **extras)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 348, in construct_arg
    result = annotation.from_params(params=popped_params, **subextras)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 636, in from_params
    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 206, in create_kwargs
    constructed_arg = pop_and_construct_arg(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 310, in pop_and_construct_arg
    popped_params = params.pop(name, default) if default != _NO_DEFAULT else params.pop(name)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\params.py", line 216, in pop
    raise ConfigurationError(msg)
allennlp.common.checks.ConfigurationError: key "input_size" is required at location "model.contextualizer."
2025-02-04 00:48:02,785 - INFO - allennlp.common.params - random_seed = 13370
2025-02-04 00:48:02,786 - INFO - allennlp.common.params - numpy_seed = 1337
2025-02-04 00:48:02,786 - INFO - allennlp.common.params - pytorch_seed = 133
2025-02-04 00:48:02,788 - INFO - allennlp.common.checks - Pytorch version: 1.12.1+cpu
2025-02-04 00:48:02,788 - INFO - allennlp.common.params - type = default
2025-02-04 00:48:02,789 - INFO - allennlp.common.params - dataset_reader.type = simple_language_modeling
2025-02-04 00:48:02,789 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2025-02-04 00:48:02,789 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2025-02-04 00:48:02,789 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False
2025-02-04 00:48:02,790 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = just_spaces
2025-02-04 00:48:02,790 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id
2025-02-04 00:48:02,790 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens
2025-02-04 00:48:02,791 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True
2025-02-04 00:48:02,791 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None
2025-02-04 00:48:02,791 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None
2025-02-04 00:48:02,791 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.feature_name = text
2025-02-04 00:48:02,791 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING
2025-02-04 00:48:02,792 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2025-02-04 00:48:02,792 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = None
2025-02-04 00:48:02,792 - INFO - allennlp.common.params - dataset_reader.start_tokens = None
2025-02-04 00:48:02,792 - INFO - allennlp.common.params - dataset_reader.end_tokens = None
2025-02-04 00:48:02,792 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Creating SimpleLanguageModelingDatasetReader
2025-02-04 00:48:02,792 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - max_sequence_length=None
2025-02-04 00:48:02,793 - INFO - allennlp.common.params - train_data_path = trainingData_ELMO.txt
2025-02-04 00:48:02,793 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x00000261F9217340>
2025-02-04 00:48:02,793 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2025-02-04 00:48:02,793 - INFO - allennlp.common.params - validation_dataset_reader = None
2025-02-04 00:48:02,794 - INFO - allennlp.common.params - validation_data_path = testData_ELMO.txt
2025-02-04 00:48:02,794 - INFO - allennlp.common.params - validation_data_loader = None
2025-02-04 00:48:02,794 - INFO - allennlp.common.params - test_data_path = None
2025-02-04 00:48:02,794 - INFO - allennlp.common.params - evaluate_on_test = False
2025-02-04 00:48:02,794 - INFO - allennlp.common.params - batch_weight_key = 
2025-02-04 00:48:02,795 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:48:02,795 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:48:02,795 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:48:02,795 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:48:02,795 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:48:02,795 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:48:02,796 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:48:02,796 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:48:02,796 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:48:02,796 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:48:02,796 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:48:02,796 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x00000261F6B1E2B0>
2025-02-04 00:48:02,797 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:48:02,797 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from trainingData_ELMO.txt
2025-02-04 00:48:03,624 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from trainingData_ELMO.txt.
2025-02-04 00:48:03,624 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:48:03,625 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:48:03,625 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:48:03,625 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:48:03,625 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:48:03,625 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:48:03,626 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:48:03,626 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:48:03,627 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:48:03,627 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:48:03,627 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:48:03,627 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x00000261F6B1E2B0>
2025-02-04 00:48:03,627 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:48:03,627 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from testData_ELMO.txt
2025-02-04 00:48:03,906 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from testData_ELMO.txt.
2025-02-04 00:48:03,907 - INFO - allennlp.common.params - type = from_instances
2025-02-04 00:48:03,907 - INFO - allennlp.common.params - min_count = None
2025-02-04 00:48:03,908 - INFO - allennlp.common.params - max_vocab_size = None
2025-02-04 00:48:03,908 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2025-02-04 00:48:03,908 - INFO - allennlp.common.params - pretrained_files = None
2025-02-04 00:48:03,908 - INFO - allennlp.common.params - only_include_pretrained_words = False
2025-02-04 00:48:03,908 - INFO - allennlp.common.params - tokens_to_add = None
2025-02-04 00:48:03,909 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2025-02-04 00:48:03,909 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2025-02-04 00:48:03,909 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2025-02-04 00:48:03,909 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2025-02-04 00:48:03,909 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2025-02-04 00:48:04,200 - INFO - allennlp.common.params - model.type = bidirectional-language-model
2025-02-04 00:48:04,201 - INFO - allennlp.common.params - model.regularizer = None
2025-02-04 00:48:04,201 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2025-02-04 00:48:04,201 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = embedding
2025-02-04 00:48:04,202 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.embedding_dim = 128
2025-02-04 00:48:04,202 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.num_embeddings = None
2025-02-04 00:48:04,202 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.projection_dim = None
2025-02-04 00:48:04,202 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.weight = None
2025-02-04 00:48:04,202 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.padding_index = None
2025-02-04 00:48:04,203 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.trainable = True
2025-02-04 00:48:04,203 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_norm = None
2025-02-04 00:48:04,203 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.norm_type = 2.0
2025-02-04 00:48:04,203 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False
2025-02-04 00:48:04,203 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sparse = False
2025-02-04 00:48:04,204 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens
2025-02-04 00:48:04,204 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.pretrained_file = None
2025-02-04 00:48:04,228 - INFO - allennlp.common.params - model.contextualizer.type = lstm
2025-02-04 00:48:04,228 - INFO - allennlp.common.params - model.contextualizer.input_size = 512
2025-02-04 00:48:04,229 - INFO - allennlp.common.params - model.contextualizer.hidden_size = 512
2025-02-04 00:48:04,229 - INFO - allennlp.common.params - model.contextualizer.num_layers = 2
2025-02-04 00:48:04,229 - INFO - allennlp.common.params - model.contextualizer.bias = True
2025-02-04 00:48:04,229 - INFO - allennlp.common.params - model.contextualizer.dropout = 0.0
2025-02-04 00:48:04,229 - INFO - allennlp.common.params - model.contextualizer.bidirectional = False
2025-02-04 00:48:04,230 - INFO - allennlp.common.params - model.contextualizer.stateful = False
2025-02-04 00:48:04,246 - INFO - allennlp.common.params - model.dropout = 0.1
2025-02-04 00:48:04,246 - INFO - allennlp.common.params - model.num_samples = None
2025-02-04 00:48:04,246 - INFO - allennlp.common.params - model.sparse_embeddings = False
2025-02-04 00:48:04,247 - INFO - allennlp.common.params - model.bidirectional = True
2025-02-04 00:48:04,247 - INFO - allennlp.common.params - model.initializer = None
2025-02-04 00:48:04,247 - CRITICAL - root - Uncaught exception
Traceback (most recent call last):
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\Scripts\allennlp.exe\__main__.py", line 7, in <module>
    sys.exit(run())
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\__main__.py", line 39, in run
    main(prog="allennlp")
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\__init__.py", line 120, in main
    args.func(args)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 111, in train_model_from_args
    train_model_from_file(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 177, in train_model_from_file
    return train_model(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 258, in train_model
    model = _train_worker(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 494, in _train_worker
    train_loop = TrainModel.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 638, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 770, in from_partial_objects
    model_ = model.construct(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 82, in construct
    return self.constructor(**contructor_kwargs)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 66, in constructor_to_use
    return self._constructor.from_params(  # type: ignore[union-attr]
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 638, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp_models\lm\models\bidirectional_lm.py", line 53, in __init__
    super().__init__(
TypeError: allennlp_models.lm.models.language_model.LanguageModel.__init__() got multiple values for keyword argument 'bidirectional'
2025-02-04 00:48:34,496 - INFO - allennlp.common.params - random_seed = 13370
2025-02-04 00:48:34,496 - INFO - allennlp.common.params - numpy_seed = 1337
2025-02-04 00:48:34,496 - INFO - allennlp.common.params - pytorch_seed = 133
2025-02-04 00:48:34,497 - INFO - allennlp.common.checks - Pytorch version: 1.12.1+cpu
2025-02-04 00:48:34,498 - INFO - allennlp.common.params - type = default
2025-02-04 00:48:34,498 - INFO - allennlp.common.params - dataset_reader.type = simple_language_modeling
2025-02-04 00:48:34,500 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2025-02-04 00:48:34,500 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2025-02-04 00:48:34,500 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False
2025-02-04 00:48:34,500 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = just_spaces
2025-02-04 00:48:34,500 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id
2025-02-04 00:48:34,500 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens
2025-02-04 00:48:34,500 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True
2025-02-04 00:48:34,500 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None
2025-02-04 00:48:34,500 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None
2025-02-04 00:48:34,500 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.feature_name = text
2025-02-04 00:48:34,500 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING
2025-02-04 00:48:34,500 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2025-02-04 00:48:34,502 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = None
2025-02-04 00:48:34,502 - INFO - allennlp.common.params - dataset_reader.start_tokens = None
2025-02-04 00:48:34,502 - INFO - allennlp.common.params - dataset_reader.end_tokens = None
2025-02-04 00:48:34,502 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Creating SimpleLanguageModelingDatasetReader
2025-02-04 00:48:34,502 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - max_sequence_length=None
2025-02-04 00:48:34,503 - INFO - allennlp.common.params - train_data_path = trainingData_ELMO.txt
2025-02-04 00:48:34,503 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x0000019A2D6B8940>
2025-02-04 00:48:34,503 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2025-02-04 00:48:34,504 - INFO - allennlp.common.params - validation_dataset_reader = None
2025-02-04 00:48:34,504 - INFO - allennlp.common.params - validation_data_path = testData_ELMO.txt
2025-02-04 00:48:34,504 - INFO - allennlp.common.params - validation_data_loader = None
2025-02-04 00:48:34,504 - INFO - allennlp.common.params - test_data_path = None
2025-02-04 00:48:34,504 - INFO - allennlp.common.params - evaluate_on_test = False
2025-02-04 00:48:34,504 - INFO - allennlp.common.params - batch_weight_key = 
2025-02-04 00:48:34,504 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:48:34,504 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:48:34,504 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:48:34,504 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:48:34,506 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:48:34,506 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:48:34,506 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:48:34,506 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:48:34,506 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:48:34,507 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:48:34,507 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:48:34,507 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x0000019A2AFC88B0>
2025-02-04 00:48:34,508 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:48:34,508 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from trainingData_ELMO.txt
2025-02-04 00:48:35,398 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from trainingData_ELMO.txt.
2025-02-04 00:48:35,398 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:48:35,400 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:48:35,400 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:48:35,400 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:48:35,400 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:48:35,401 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:48:35,401 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:48:35,401 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:48:35,401 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:48:35,401 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:48:35,401 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:48:35,402 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x0000019A2AFC88B0>
2025-02-04 00:48:35,402 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:48:35,402 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from testData_ELMO.txt
2025-02-04 00:48:35,712 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from testData_ELMO.txt.
2025-02-04 00:48:35,713 - INFO - allennlp.common.params - type = from_instances
2025-02-04 00:48:35,713 - INFO - allennlp.common.params - min_count = None
2025-02-04 00:48:35,714 - INFO - allennlp.common.params - max_vocab_size = None
2025-02-04 00:48:35,715 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2025-02-04 00:48:35,716 - INFO - allennlp.common.params - pretrained_files = None
2025-02-04 00:48:35,716 - INFO - allennlp.common.params - only_include_pretrained_words = False
2025-02-04 00:48:35,716 - INFO - allennlp.common.params - tokens_to_add = None
2025-02-04 00:48:35,716 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2025-02-04 00:48:35,717 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2025-02-04 00:48:35,717 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2025-02-04 00:48:35,718 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2025-02-04 00:48:35,719 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2025-02-04 00:48:36,049 - INFO - allennlp.common.params - model.type = bidirectional-language-model
2025-02-04 00:48:36,050 - INFO - allennlp.common.params - model.regularizer = None
2025-02-04 00:48:36,050 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2025-02-04 00:48:36,050 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = embedding
2025-02-04 00:48:36,050 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.embedding_dim = 128
2025-02-04 00:48:36,051 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.num_embeddings = None
2025-02-04 00:48:36,051 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.projection_dim = None
2025-02-04 00:48:36,051 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.weight = None
2025-02-04 00:48:36,051 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.padding_index = None
2025-02-04 00:48:36,051 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.trainable = True
2025-02-04 00:48:36,052 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_norm = None
2025-02-04 00:48:36,052 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.norm_type = 2.0
2025-02-04 00:48:36,052 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False
2025-02-04 00:48:36,052 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sparse = False
2025-02-04 00:48:36,052 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens
2025-02-04 00:48:36,053 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.pretrained_file = None
2025-02-04 00:48:36,082 - INFO - allennlp.common.params - model.contextualizer.type = lstm
2025-02-04 00:48:36,083 - INFO - allennlp.common.params - model.contextualizer.input_size = 512
2025-02-04 00:48:36,083 - INFO - allennlp.common.params - model.contextualizer.hidden_size = 512
2025-02-04 00:48:36,083 - INFO - allennlp.common.params - model.contextualizer.num_layers = 2
2025-02-04 00:48:36,084 - INFO - allennlp.common.params - model.contextualizer.bias = True
2025-02-04 00:48:36,084 - INFO - allennlp.common.params - model.contextualizer.dropout = 0.0
2025-02-04 00:48:36,084 - INFO - allennlp.common.params - model.contextualizer.bidirectional = False
2025-02-04 00:48:36,084 - INFO - allennlp.common.params - model.contextualizer.stateful = False
2025-02-04 00:48:36,107 - INFO - allennlp.common.params - model.dropout = 0.1
2025-02-04 00:48:36,108 - INFO - allennlp.common.params - model.num_samples = None
2025-02-04 00:48:36,108 - INFO - allennlp.common.params - model.sparse_embeddings = False
2025-02-04 00:48:36,108 - INFO - allennlp.common.params - model.bidirectional = False
2025-02-04 00:48:36,108 - INFO - allennlp.common.params - model.initializer = None
2025-02-04 00:48:36,109 - CRITICAL - root - Uncaught exception
Traceback (most recent call last):
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\Scripts\allennlp.exe\__main__.py", line 7, in <module>
    sys.exit(run())
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\__main__.py", line 39, in run
    main(prog="allennlp")
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\__init__.py", line 120, in main
    args.func(args)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 111, in train_model_from_args
    train_model_from_file(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 177, in train_model_from_file
    return train_model(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 258, in train_model
    model = _train_worker(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 494, in _train_worker
    train_loop = TrainModel.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 638, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 770, in from_partial_objects
    model_ = model.construct(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 82, in construct
    return self.constructor(**contructor_kwargs)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 66, in constructor_to_use
    return self._constructor.from_params(  # type: ignore[union-attr]
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 638, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp_models\lm\models\bidirectional_lm.py", line 53, in __init__
    super().__init__(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp_models\lm\models\language_model.py", line 73, in __init__
    super().__init__(vocab, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'tie_weights'
2025-02-04 00:49:05,020 - INFO - allennlp.common.params - random_seed = 13370
2025-02-04 00:49:05,020 - INFO - allennlp.common.params - numpy_seed = 1337
2025-02-04 00:49:05,020 - INFO - allennlp.common.params - pytorch_seed = 133
2025-02-04 00:49:05,022 - INFO - allennlp.common.checks - Pytorch version: 1.12.1+cpu
2025-02-04 00:49:05,022 - INFO - allennlp.common.params - type = default
2025-02-04 00:49:05,023 - INFO - allennlp.common.params - dataset_reader.type = simple_language_modeling
2025-02-04 00:49:05,023 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2025-02-04 00:49:05,023 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2025-02-04 00:49:05,023 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False
2025-02-04 00:49:05,024 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = just_spaces
2025-02-04 00:49:05,024 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id
2025-02-04 00:49:05,024 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens
2025-02-04 00:49:05,024 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True
2025-02-04 00:49:05,025 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None
2025-02-04 00:49:05,025 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None
2025-02-04 00:49:05,025 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.feature_name = text
2025-02-04 00:49:05,025 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING
2025-02-04 00:49:05,026 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2025-02-04 00:49:05,026 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = None
2025-02-04 00:49:05,026 - INFO - allennlp.common.params - dataset_reader.start_tokens = None
2025-02-04 00:49:05,026 - INFO - allennlp.common.params - dataset_reader.end_tokens = None
2025-02-04 00:49:05,026 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Creating SimpleLanguageModelingDatasetReader
2025-02-04 00:49:05,027 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - max_sequence_length=None
2025-02-04 00:49:05,027 - INFO - allennlp.common.params - train_data_path = trainingData_ELMO.txt
2025-02-04 00:49:05,027 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x0000018149DB0730>
2025-02-04 00:49:05,028 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2025-02-04 00:49:05,028 - INFO - allennlp.common.params - validation_dataset_reader = None
2025-02-04 00:49:05,028 - INFO - allennlp.common.params - validation_data_path = testData_ELMO.txt
2025-02-04 00:49:05,028 - INFO - allennlp.common.params - validation_data_loader = None
2025-02-04 00:49:05,028 - INFO - allennlp.common.params - test_data_path = None
2025-02-04 00:49:05,028 - INFO - allennlp.common.params - evaluate_on_test = False
2025-02-04 00:49:05,029 - INFO - allennlp.common.params - batch_weight_key = 
2025-02-04 00:49:05,029 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:49:05,029 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:49:05,029 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:49:05,030 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:49:05,030 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:49:05,030 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:49:05,030 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:49:05,030 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:49:05,030 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:49:05,030 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:49:05,031 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:49:05,031 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x00000181476BB6A0>
2025-02-04 00:49:05,031 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:49:05,031 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from trainingData_ELMO.txt
2025-02-04 00:49:05,835 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from trainingData_ELMO.txt.
2025-02-04 00:49:05,836 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:49:05,836 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:49:05,836 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:49:05,836 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:49:05,836 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:49:05,837 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:49:05,837 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:49:05,837 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:49:05,837 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:49:05,837 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:49:05,837 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:49:05,838 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x00000181476BB6A0>
2025-02-04 00:49:05,838 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:49:05,838 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from testData_ELMO.txt
2025-02-04 00:49:06,112 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from testData_ELMO.txt.
2025-02-04 00:49:06,113 - INFO - allennlp.common.params - type = from_instances
2025-02-04 00:49:06,113 - INFO - allennlp.common.params - min_count = None
2025-02-04 00:49:06,113 - INFO - allennlp.common.params - max_vocab_size = None
2025-02-04 00:49:06,114 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2025-02-04 00:49:06,114 - INFO - allennlp.common.params - pretrained_files = None
2025-02-04 00:49:06,114 - INFO - allennlp.common.params - only_include_pretrained_words = False
2025-02-04 00:49:06,114 - INFO - allennlp.common.params - tokens_to_add = None
2025-02-04 00:49:06,114 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2025-02-04 00:49:06,115 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2025-02-04 00:49:06,115 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2025-02-04 00:49:06,115 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2025-02-04 00:49:06,115 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2025-02-04 00:49:06,415 - INFO - allennlp.common.params - model.type = bidirectional-language-model
2025-02-04 00:49:06,416 - INFO - allennlp.common.params - model.regularizer = None
2025-02-04 00:49:06,416 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2025-02-04 00:49:06,417 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = embedding
2025-02-04 00:49:06,417 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.embedding_dim = 128
2025-02-04 00:49:06,417 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.num_embeddings = None
2025-02-04 00:49:06,417 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.projection_dim = None
2025-02-04 00:49:06,417 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.weight = None
2025-02-04 00:49:06,417 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.padding_index = None
2025-02-04 00:49:06,418 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.trainable = True
2025-02-04 00:49:06,418 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_norm = None
2025-02-04 00:49:06,418 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.norm_type = 2.0
2025-02-04 00:49:06,418 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False
2025-02-04 00:49:06,418 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sparse = False
2025-02-04 00:49:06,418 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens
2025-02-04 00:49:06,418 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.pretrained_file = None
2025-02-04 00:49:06,443 - INFO - allennlp.common.params - model.contextualizer.type = lstm
2025-02-04 00:49:06,443 - INFO - allennlp.common.params - model.contextualizer.input_size = 512
2025-02-04 00:49:06,444 - INFO - allennlp.common.params - model.contextualizer.hidden_size = 512
2025-02-04 00:49:06,444 - INFO - allennlp.common.params - model.contextualizer.num_layers = 2
2025-02-04 00:49:06,444 - INFO - allennlp.common.params - model.contextualizer.bias = True
2025-02-04 00:49:06,444 - INFO - allennlp.common.params - model.contextualizer.dropout = 0.0
2025-02-04 00:49:06,444 - INFO - allennlp.common.params - model.contextualizer.bidirectional = False
2025-02-04 00:49:06,444 - INFO - allennlp.common.params - model.contextualizer.stateful = False
2025-02-04 00:49:06,461 - INFO - allennlp.common.params - model.dropout = 0.1
2025-02-04 00:49:06,461 - INFO - allennlp.common.params - model.num_samples = None
2025-02-04 00:49:06,462 - INFO - allennlp.common.params - model.sparse_embeddings = False
2025-02-04 00:49:06,462 - INFO - allennlp.common.params - model.bidirectional = False
2025-02-04 00:49:06,462 - INFO - allennlp.common.params - model.initializer = None
2025-02-04 00:49:06,462 - CRITICAL - root - Uncaught exception
Traceback (most recent call last):
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\Scripts\allennlp.exe\__main__.py", line 7, in <module>
    sys.exit(run())
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\__main__.py", line 39, in run
    main(prog="allennlp")
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\__init__.py", line 120, in main
    args.func(args)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 111, in train_model_from_args
    train_model_from_file(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 177, in train_model_from_file
    return train_model(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 258, in train_model
    model = _train_worker(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 494, in _train_worker
    train_loop = TrainModel.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 638, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 770, in from_partial_objects
    model_ = model.construct(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 82, in construct
    return self.constructor(**contructor_kwargs)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 66, in constructor_to_use
    return self._constructor.from_params(  # type: ignore[union-attr]
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 638, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp_models\lm\models\bidirectional_lm.py", line 53, in __init__
    super().__init__(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp_models\lm\models\language_model.py", line 77, in __init__
    raise ConfigurationError(
allennlp.common.checks.ConfigurationError: Bidirectionality of contextualizer must match bidirectionality of language model. Contextualizer bidirectional: False, language model bidirectional: True
2025-02-04 00:49:39,543 - INFO - allennlp.common.params - random_seed = 13370
2025-02-04 00:49:39,544 - INFO - allennlp.common.params - numpy_seed = 1337
2025-02-04 00:49:39,544 - INFO - allennlp.common.params - pytorch_seed = 133
2025-02-04 00:49:39,546 - INFO - allennlp.common.checks - Pytorch version: 1.12.1+cpu
2025-02-04 00:49:39,546 - INFO - allennlp.common.params - type = default
2025-02-04 00:49:39,547 - INFO - allennlp.common.params - dataset_reader.type = simple_language_modeling
2025-02-04 00:49:39,547 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2025-02-04 00:49:39,547 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2025-02-04 00:49:39,548 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False
2025-02-04 00:49:39,548 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = just_spaces
2025-02-04 00:49:39,548 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id
2025-02-04 00:49:39,548 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens
2025-02-04 00:49:39,548 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True
2025-02-04 00:49:39,549 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None
2025-02-04 00:49:39,549 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None
2025-02-04 00:49:39,549 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.feature_name = text
2025-02-04 00:49:39,549 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING
2025-02-04 00:49:39,549 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2025-02-04 00:49:39,549 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = None
2025-02-04 00:49:39,549 - INFO - allennlp.common.params - dataset_reader.start_tokens = None
2025-02-04 00:49:39,550 - INFO - allennlp.common.params - dataset_reader.end_tokens = None
2025-02-04 00:49:39,550 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Creating SimpleLanguageModelingDatasetReader
2025-02-04 00:49:39,550 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - max_sequence_length=None
2025-02-04 00:49:39,550 - INFO - allennlp.common.params - train_data_path = trainingData_ELMO.txt
2025-02-04 00:49:39,551 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x000002B678132580>
2025-02-04 00:49:39,551 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2025-02-04 00:49:39,551 - INFO - allennlp.common.params - validation_dataset_reader = None
2025-02-04 00:49:39,551 - INFO - allennlp.common.params - validation_data_path = testData_ELMO.txt
2025-02-04 00:49:39,551 - INFO - allennlp.common.params - validation_data_loader = None
2025-02-04 00:49:39,551 - INFO - allennlp.common.params - test_data_path = None
2025-02-04 00:49:39,552 - INFO - allennlp.common.params - evaluate_on_test = False
2025-02-04 00:49:39,552 - INFO - allennlp.common.params - batch_weight_key = 
2025-02-04 00:49:39,552 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:49:39,552 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:49:39,552 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:49:39,553 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:49:39,553 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:49:39,553 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:49:39,553 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:49:39,553 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:49:39,553 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:49:39,553 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:49:39,554 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:49:39,554 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x000002B675A3C4F0>
2025-02-04 00:49:39,554 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:49:39,554 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from trainingData_ELMO.txt
2025-02-04 00:49:40,407 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from trainingData_ELMO.txt.
2025-02-04 00:49:40,407 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:49:40,408 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:49:40,408 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:49:40,408 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:49:40,409 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:49:40,409 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:49:40,409 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:49:40,409 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:49:40,409 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:49:40,409 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:49:40,410 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:49:40,410 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x000002B675A3C4F0>
2025-02-04 00:49:40,410 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:49:40,410 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from testData_ELMO.txt
2025-02-04 00:49:40,683 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from testData_ELMO.txt.
2025-02-04 00:49:40,684 - INFO - allennlp.common.params - type = from_instances
2025-02-04 00:49:40,684 - INFO - allennlp.common.params - min_count = None
2025-02-04 00:49:40,685 - INFO - allennlp.common.params - max_vocab_size = None
2025-02-04 00:49:40,685 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2025-02-04 00:49:40,685 - INFO - allennlp.common.params - pretrained_files = None
2025-02-04 00:49:40,685 - INFO - allennlp.common.params - only_include_pretrained_words = False
2025-02-04 00:49:40,685 - INFO - allennlp.common.params - tokens_to_add = None
2025-02-04 00:49:40,686 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2025-02-04 00:49:40,686 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2025-02-04 00:49:40,686 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2025-02-04 00:49:40,686 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2025-02-04 00:49:40,686 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2025-02-04 00:49:40,989 - INFO - allennlp.common.params - model.type = bidirectional-language-model
2025-02-04 00:49:40,989 - INFO - allennlp.common.params - model.regularizer = None
2025-02-04 00:49:40,990 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2025-02-04 00:49:40,990 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = embedding
2025-02-04 00:49:40,990 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.embedding_dim = 128
2025-02-04 00:49:40,991 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.num_embeddings = None
2025-02-04 00:49:40,991 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.projection_dim = None
2025-02-04 00:49:40,991 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.weight = None
2025-02-04 00:49:40,991 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.padding_index = None
2025-02-04 00:49:40,991 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.trainable = True
2025-02-04 00:49:40,991 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_norm = None
2025-02-04 00:49:40,992 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.norm_type = 2.0
2025-02-04 00:49:40,992 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False
2025-02-04 00:49:40,992 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sparse = False
2025-02-04 00:49:40,992 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens
2025-02-04 00:49:40,992 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.pretrained_file = None
2025-02-04 00:49:41,018 - INFO - allennlp.common.params - model.contextualizer.type = lstm
2025-02-04 00:49:41,018 - INFO - allennlp.common.params - model.contextualizer.input_size = 512
2025-02-04 00:49:41,018 - INFO - allennlp.common.params - model.contextualizer.hidden_size = 512
2025-02-04 00:49:41,019 - INFO - allennlp.common.params - model.contextualizer.num_layers = 2
2025-02-04 00:49:41,019 - INFO - allennlp.common.params - model.contextualizer.bias = True
2025-02-04 00:49:41,019 - INFO - allennlp.common.params - model.contextualizer.dropout = 0.0
2025-02-04 00:49:41,019 - INFO - allennlp.common.params - model.contextualizer.bidirectional = True
2025-02-04 00:49:41,019 - INFO - allennlp.common.params - model.contextualizer.stateful = False
2025-02-04 00:49:41,062 - INFO - allennlp.common.params - model.dropout = 0.1
2025-02-04 00:49:41,062 - INFO - allennlp.common.params - model.num_samples = None
2025-02-04 00:49:41,062 - INFO - allennlp.common.params - model.sparse_embeddings = False
2025-02-04 00:49:41,062 - INFO - allennlp.common.params - model.bidirectional = False
2025-02-04 00:49:41,063 - INFO - allennlp.common.params - model.initializer = None
2025-02-04 00:49:41,621 - INFO - allennlp.common.params - trainer.type = gradient_descent
2025-02-04 00:49:41,622 - INFO - allennlp.common.params - trainer.cuda_device = 0
2025-02-04 00:49:41,622 - INFO - allennlp.common.params - trainer.distributed = False
2025-02-04 00:49:41,623 - INFO - allennlp.common.params - trainer.world_size = 1
2025-02-04 00:49:41,623 - INFO - allennlp.common.params - trainer.patience = 2
2025-02-04 00:49:41,623 - INFO - allennlp.common.params - trainer.validation_metric = -perplexity
2025-02-04 00:49:41,623 - INFO - allennlp.common.params - trainer.num_epochs = 32
2025-02-04 00:49:41,624 - INFO - allennlp.common.params - trainer.grad_norm = False
2025-02-04 00:49:41,624 - INFO - allennlp.common.params - trainer.grad_clipping = None
2025-02-04 00:49:41,624 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1
2025-02-04 00:49:41,624 - INFO - allennlp.common.params - trainer.use_amp = False
2025-02-04 00:49:41,624 - INFO - allennlp.common.params - trainer.no_grad = None
2025-02-04 00:49:41,625 - INFO - allennlp.common.params - trainer.learning_rate_scheduler = None
2025-02-04 00:49:41,625 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2025-02-04 00:49:41,625 - INFO - allennlp.common.params - trainer.moving_average = None
2025-02-04 00:49:41,625 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x000002B6780F0D00>
2025-02-04 00:49:41,625 - INFO - allennlp.common.params - trainer.callbacks = None
2025-02-04 00:49:41,625 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True
2025-02-04 00:49:41,625 - INFO - allennlp.common.params - trainer.run_confidence_checks = True
2025-02-04 00:49:41,626 - INFO - allennlp.common.params - trainer.grad_scaling = True
2025-02-04 00:49:41,626 - CRITICAL - root - Uncaught exception
Traceback (most recent call last):
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\Scripts\allennlp.exe\__main__.py", line 7, in <module>
    sys.exit(run())
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\__main__.py", line 39, in run
    main(prog="allennlp")
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\__init__.py", line 120, in main
    args.func(args)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 111, in train_model_from_args
    train_model_from_file(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 177, in train_model_from_file
    return train_model(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 258, in train_model
    model = _train_worker(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 494, in _train_worker
    train_loop = TrainModel.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 638, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 786, in from_partial_objects
    trainer_ = trainer.construct(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 82, in construct
    return self.constructor(**contructor_kwargs)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\lazy.py", line 66, in constructor_to_use
    return self._constructor.from_params(  # type: ignore[union-attr]
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 604, in from_params
    return retyped_subclass.from_params(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\from_params.py", line 638, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\training\gradient_descent_trainer.py", line 1142, in from_partial_objects
    check_for_gpu(cuda_device)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\common\checks.py", line 117, in check_for_gpu
    raise ConfigurationError(
allennlp.common.checks.ConfigurationError: Experiment specified a GPU but none is available; if you want to run on CPU use the override 'trainer.cuda_device=-1' in the json config file.
2025-02-04 00:50:03,234 - INFO - allennlp.common.params - random_seed = 13370
2025-02-04 00:50:03,234 - INFO - allennlp.common.params - numpy_seed = 1337
2025-02-04 00:50:03,235 - INFO - allennlp.common.params - pytorch_seed = 133
2025-02-04 00:50:03,237 - INFO - allennlp.common.checks - Pytorch version: 1.12.1+cpu
2025-02-04 00:50:03,237 - INFO - allennlp.common.params - type = default
2025-02-04 00:50:03,237 - INFO - allennlp.common.params - dataset_reader.type = simple_language_modeling
2025-02-04 00:50:03,238 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2025-02-04 00:50:03,238 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2025-02-04 00:50:03,238 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False
2025-02-04 00:50:03,238 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = just_spaces
2025-02-04 00:50:03,239 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id
2025-02-04 00:50:03,239 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens
2025-02-04 00:50:03,239 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True
2025-02-04 00:50:03,239 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None
2025-02-04 00:50:03,240 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None
2025-02-04 00:50:03,240 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.feature_name = text
2025-02-04 00:50:03,240 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING
2025-02-04 00:50:03,240 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2025-02-04 00:50:03,240 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = None
2025-02-04 00:50:03,240 - INFO - allennlp.common.params - dataset_reader.start_tokens = None
2025-02-04 00:50:03,240 - INFO - allennlp.common.params - dataset_reader.end_tokens = None
2025-02-04 00:50:03,241 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Creating SimpleLanguageModelingDatasetReader
2025-02-04 00:50:03,241 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - max_sequence_length=None
2025-02-04 00:50:03,241 - INFO - allennlp.common.params - train_data_path = trainingData_ELMO.txt
2025-02-04 00:50:03,241 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x000001CC39EC16D0>
2025-02-04 00:50:03,242 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2025-02-04 00:50:03,242 - INFO - allennlp.common.params - validation_dataset_reader = None
2025-02-04 00:50:03,242 - INFO - allennlp.common.params - validation_data_path = testData_ELMO.txt
2025-02-04 00:50:03,242 - INFO - allennlp.common.params - validation_data_loader = None
2025-02-04 00:50:03,242 - INFO - allennlp.common.params - test_data_path = None
2025-02-04 00:50:03,242 - INFO - allennlp.common.params - evaluate_on_test = False
2025-02-04 00:50:03,242 - INFO - allennlp.common.params - batch_weight_key = 
2025-02-04 00:50:03,243 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:50:03,243 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:50:03,243 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:50:03,243 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:50:03,244 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:50:03,244 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:50:03,244 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:50:03,244 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:50:03,244 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:50:03,244 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:50:03,244 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:50:03,245 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x000001CC377CB640>
2025-02-04 00:50:03,245 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:50:03,245 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from trainingData_ELMO.txt
2025-02-04 00:50:04,055 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from trainingData_ELMO.txt.
2025-02-04 00:50:04,055 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:50:04,056 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:50:04,056 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:50:04,056 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:50:04,056 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:50:04,056 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:50:04,056 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:50:04,057 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:50:04,057 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:50:04,057 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:50:04,057 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:50:04,057 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x000001CC377CB640>
2025-02-04 00:50:04,057 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:50:04,057 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from testData_ELMO.txt
2025-02-04 00:50:04,340 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from testData_ELMO.txt.
2025-02-04 00:50:04,341 - INFO - allennlp.common.params - type = from_instances
2025-02-04 00:50:04,341 - INFO - allennlp.common.params - min_count = None
2025-02-04 00:50:04,341 - INFO - allennlp.common.params - max_vocab_size = None
2025-02-04 00:50:04,341 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2025-02-04 00:50:04,342 - INFO - allennlp.common.params - pretrained_files = None
2025-02-04 00:50:04,342 - INFO - allennlp.common.params - only_include_pretrained_words = False
2025-02-04 00:50:04,342 - INFO - allennlp.common.params - tokens_to_add = None
2025-02-04 00:50:04,342 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2025-02-04 00:50:04,342 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2025-02-04 00:50:04,343 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2025-02-04 00:50:04,343 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2025-02-04 00:50:04,343 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2025-02-04 00:50:04,638 - INFO - allennlp.common.params - model.type = bidirectional-language-model
2025-02-04 00:50:04,639 - INFO - allennlp.common.params - model.regularizer = None
2025-02-04 00:50:04,639 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2025-02-04 00:50:04,640 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = embedding
2025-02-04 00:50:04,640 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.embedding_dim = 128
2025-02-04 00:50:04,640 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.num_embeddings = None
2025-02-04 00:50:04,640 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.projection_dim = None
2025-02-04 00:50:04,640 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.weight = None
2025-02-04 00:50:04,641 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.padding_index = None
2025-02-04 00:50:04,641 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.trainable = True
2025-02-04 00:50:04,641 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_norm = None
2025-02-04 00:50:04,641 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.norm_type = 2.0
2025-02-04 00:50:04,641 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False
2025-02-04 00:50:04,641 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sparse = False
2025-02-04 00:50:04,641 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens
2025-02-04 00:50:04,642 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.pretrained_file = None
2025-02-04 00:50:04,667 - INFO - allennlp.common.params - model.contextualizer.type = lstm
2025-02-04 00:50:04,667 - INFO - allennlp.common.params - model.contextualizer.input_size = 512
2025-02-04 00:50:04,668 - INFO - allennlp.common.params - model.contextualizer.hidden_size = 512
2025-02-04 00:50:04,668 - INFO - allennlp.common.params - model.contextualizer.num_layers = 2
2025-02-04 00:50:04,668 - INFO - allennlp.common.params - model.contextualizer.bias = True
2025-02-04 00:50:04,668 - INFO - allennlp.common.params - model.contextualizer.dropout = 0.0
2025-02-04 00:50:04,668 - INFO - allennlp.common.params - model.contextualizer.bidirectional = True
2025-02-04 00:50:04,668 - INFO - allennlp.common.params - model.contextualizer.stateful = False
2025-02-04 00:50:04,714 - INFO - allennlp.common.params - model.dropout = 0.1
2025-02-04 00:50:04,715 - INFO - allennlp.common.params - model.num_samples = None
2025-02-04 00:50:04,715 - INFO - allennlp.common.params - model.sparse_embeddings = False
2025-02-04 00:50:04,715 - INFO - allennlp.common.params - model.bidirectional = False
2025-02-04 00:50:04,715 - INFO - allennlp.common.params - model.initializer = None
2025-02-04 00:50:04,854 - WARNING - allennlp.data.vocabulary - vocabulary serialization directory ./elmo\vocabulary is not empty
2025-02-04 00:50:05,211 - INFO - allennlp.common.params - trainer.type = gradient_descent
2025-02-04 00:50:05,212 - INFO - allennlp.common.params - trainer.cuda_device = -1
2025-02-04 00:50:05,212 - INFO - allennlp.common.params - trainer.distributed = False
2025-02-04 00:50:05,212 - INFO - allennlp.common.params - trainer.world_size = 1
2025-02-04 00:50:05,213 - INFO - allennlp.common.params - trainer.patience = 2
2025-02-04 00:50:05,213 - INFO - allennlp.common.params - trainer.validation_metric = -perplexity
2025-02-04 00:50:05,213 - INFO - allennlp.common.params - trainer.num_epochs = 32
2025-02-04 00:50:05,213 - INFO - allennlp.common.params - trainer.grad_norm = False
2025-02-04 00:50:05,214 - INFO - allennlp.common.params - trainer.grad_clipping = None
2025-02-04 00:50:05,214 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1
2025-02-04 00:50:05,214 - INFO - allennlp.common.params - trainer.use_amp = False
2025-02-04 00:50:05,214 - INFO - allennlp.common.params - trainer.no_grad = None
2025-02-04 00:50:05,215 - INFO - allennlp.common.params - trainer.learning_rate_scheduler = None
2025-02-04 00:50:05,215 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2025-02-04 00:50:05,215 - INFO - allennlp.common.params - trainer.moving_average = None
2025-02-04 00:50:05,215 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x000001CC39E80E50>
2025-02-04 00:50:05,215 - INFO - allennlp.common.params - trainer.callbacks = None
2025-02-04 00:50:05,215 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True
2025-02-04 00:50:05,215 - INFO - allennlp.common.params - trainer.run_confidence_checks = True
2025-02-04 00:50:05,216 - INFO - allennlp.common.params - trainer.grad_scaling = True
2025-02-04 00:50:05,216 - INFO - allennlp.common.params - trainer.optimizer.type = adam
2025-02-04 00:50:05,217 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2025-02-04 00:50:05,217 - INFO - allennlp.common.params - trainer.optimizer.lr = 0.001
2025-02-04 00:50:05,217 - INFO - allennlp.common.params - trainer.optimizer.betas = (0.9, 0.999)
2025-02-04 00:50:05,217 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-08
2025-02-04 00:50:05,217 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.0
2025-02-04 00:50:05,218 - INFO - allennlp.common.params - trainer.optimizer.amsgrad = False
2025-02-04 00:50:05,232 - INFO - allennlp.training.optimizers - Number of trainable parameters: 41977167
2025-02-04 00:50:05,232 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2025-02-04 00:50:05,232 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2025-02-04 00:50:05,233 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.weight
2025-02-04 00:50:05,233 - INFO - allennlp.common.util - _contextualizer._module.weight_ih_l0
2025-02-04 00:50:05,233 - INFO - allennlp.common.util - _contextualizer._module.weight_hh_l0
2025-02-04 00:50:05,233 - INFO - allennlp.common.util - _contextualizer._module.bias_ih_l0
2025-02-04 00:50:05,233 - INFO - allennlp.common.util - _contextualizer._module.bias_hh_l0
2025-02-04 00:50:05,233 - INFO - allennlp.common.util - _contextualizer._module.weight_ih_l0_reverse
2025-02-04 00:50:05,233 - INFO - allennlp.common.util - _contextualizer._module.weight_hh_l0_reverse
2025-02-04 00:50:05,234 - INFO - allennlp.common.util - _contextualizer._module.bias_ih_l0_reverse
2025-02-04 00:50:05,234 - INFO - allennlp.common.util - _contextualizer._module.bias_hh_l0_reverse
2025-02-04 00:50:05,234 - INFO - allennlp.common.util - _contextualizer._module.weight_ih_l1
2025-02-04 00:50:05,234 - INFO - allennlp.common.util - _contextualizer._module.weight_hh_l1
2025-02-04 00:50:05,234 - INFO - allennlp.common.util - _contextualizer._module.bias_ih_l1
2025-02-04 00:50:05,234 - INFO - allennlp.common.util - _contextualizer._module.bias_hh_l1
2025-02-04 00:50:05,235 - INFO - allennlp.common.util - _contextualizer._module.weight_ih_l1_reverse
2025-02-04 00:50:05,235 - INFO - allennlp.common.util - _contextualizer._module.weight_hh_l1_reverse
2025-02-04 00:50:05,235 - INFO - allennlp.common.util - _contextualizer._module.bias_ih_l1_reverse
2025-02-04 00:50:05,235 - INFO - allennlp.common.util - _contextualizer._module.bias_hh_l1_reverse
2025-02-04 00:50:05,235 - INFO - allennlp.common.util - _softmax_loss.softmax_w
2025-02-04 00:50:05,236 - INFO - allennlp.common.util - _softmax_loss.softmax_b
2025-02-04 00:50:05,236 - INFO - allennlp.common.params - type = default
2025-02-04 00:50:05,236 - INFO - allennlp.common.params - save_completed_epochs = True
2025-02-04 00:50:05,236 - INFO - allennlp.common.params - save_every_num_seconds = None
2025-02-04 00:50:05,236 - INFO - allennlp.common.params - save_every_num_batches = None
2025-02-04 00:50:05,237 - INFO - allennlp.common.params - keep_most_recent_by_count = 2
2025-02-04 00:50:05,237 - INFO - allennlp.common.params - keep_most_recent_by_age = None
2025-02-04 00:50:05,238 - INFO - allennlp.training.gradient_descent_trainer - Beginning training.
2025-02-04 00:50:05,238 - INFO - allennlp.training.gradient_descent_trainer - Epoch 0/31
2025-02-04 00:50:05,238 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 0B
2025-02-04 00:50:05,238 - INFO - allennlp.training.gradient_descent_trainer - Training
2025-02-04 00:50:05,239 - INFO - tqdm - 0%|          | 0/516 [00:00<?, ?it/s]
2025-02-04 00:50:05,289 - CRITICAL - root - Uncaught exception
Traceback (most recent call last):
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\Scripts\allennlp.exe\__main__.py", line 7, in <module>
    sys.exit(run())
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\__main__.py", line 39, in run
    main(prog="allennlp")
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\__init__.py", line 120, in main
    args.func(args)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 111, in train_model_from_args
    train_model_from_file(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 177, in train_model_from_file
    return train_model(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 258, in train_model
    model = _train_worker(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 508, in _train_worker
    metrics = train_loop.run()
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\commands\train.py", line 581, in run
    return self.trainer.train()
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\training\gradient_descent_trainer.py", line 771, in train
    metrics, epoch = self._try_train()
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\training\gradient_descent_trainer.py", line 793, in _try_train
    train_metrics = self._train_epoch(epoch)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\training\gradient_descent_trainer.py", line 510, in _train_epoch
    batch_outputs = self.batch_outputs(batch, for_training=True)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\training\gradient_descent_trainer.py", line 403, in batch_outputs
    output_dict = self._pytorch_model(**batch)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\torch\nn\modules\module.py", line 1148, in _call_impl
    result = forward_call(*input, **kwargs)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp_models\lm\models\language_model.py", line 256, in forward
    contextual_embeddings: Union[torch.Tensor, List[torch.Tensor]] = self._contextualizer(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\torch\nn\modules\module.py", line 1148, in _call_impl
    result = forward_call(*input, **kwargs)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\modules\seq2seq_encoders\pytorch_seq2seq_wrapper.py", line 80, in forward
    packed_sequence_output, final_states, restoration_indices = self.sort_and_run_forward(
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\allennlp\modules\encoder_base.py", line 133, in sort_and_run_forward
    module_output, final_states = module(packed_sequence_input, initial_states)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\torch\nn\modules\module.py", line 1148, in _call_impl
    result = forward_call(*input, **kwargs)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\torch\nn\modules\rnn.py", line 767, in forward
    self.check_forward_args(input, hx, batch_sizes)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\torch\nn\modules\rnn.py", line 692, in check_forward_args
    self.check_input(input, batch_sizes)
  File "c:\Users\Neptune\Documents\GitHub\Thesis Heavy Duty\lib\site-packages\torch\nn\modules\rnn.py", line 205, in check_input
    raise RuntimeError(
RuntimeError: input.size(-1) must be equal to input_size. Expected 512, got 128
2025-02-04 00:51:12,193 - INFO - allennlp.common.params - random_seed = 13370
2025-02-04 00:51:12,193 - INFO - allennlp.common.params - numpy_seed = 1337
2025-02-04 00:51:12,193 - INFO - allennlp.common.params - pytorch_seed = 133
2025-02-04 00:51:12,196 - INFO - allennlp.common.checks - Pytorch version: 1.12.1+cpu
2025-02-04 00:51:12,197 - INFO - allennlp.common.params - type = default
2025-02-04 00:51:12,197 - INFO - allennlp.common.params - dataset_reader.type = simple_language_modeling
2025-02-04 00:51:12,198 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2025-02-04 00:51:12,198 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2025-02-04 00:51:12,198 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False
2025-02-04 00:51:12,199 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = just_spaces
2025-02-04 00:51:12,199 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id
2025-02-04 00:51:12,199 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens
2025-02-04 00:51:12,199 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True
2025-02-04 00:51:12,200 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None
2025-02-04 00:51:12,200 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None
2025-02-04 00:51:12,200 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.feature_name = text
2025-02-04 00:51:12,200 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING
2025-02-04 00:51:12,200 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2025-02-04 00:51:12,200 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = None
2025-02-04 00:51:12,201 - INFO - allennlp.common.params - dataset_reader.start_tokens = None
2025-02-04 00:51:12,201 - INFO - allennlp.common.params - dataset_reader.end_tokens = None
2025-02-04 00:51:12,201 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Creating SimpleLanguageModelingDatasetReader
2025-02-04 00:51:12,201 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - max_sequence_length=None
2025-02-04 00:51:12,202 - INFO - allennlp.common.params - train_data_path = trainingData_ELMO.txt
2025-02-04 00:51:12,202 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x0000015CBD258A60>
2025-02-04 00:51:12,202 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2025-02-04 00:51:12,203 - INFO - allennlp.common.params - validation_dataset_reader = None
2025-02-04 00:51:12,203 - INFO - allennlp.common.params - validation_data_path = testData_ELMO.txt
2025-02-04 00:51:12,203 - INFO - allennlp.common.params - validation_data_loader = None
2025-02-04 00:51:12,203 - INFO - allennlp.common.params - test_data_path = None
2025-02-04 00:51:12,203 - INFO - allennlp.common.params - evaluate_on_test = False
2025-02-04 00:51:12,204 - INFO - allennlp.common.params - batch_weight_key = 
2025-02-04 00:51:12,204 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:51:12,204 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:51:12,204 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:51:12,204 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:51:12,205 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:51:12,205 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:51:12,205 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:51:12,205 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:51:12,205 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:51:12,205 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:51:12,206 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:51:12,206 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x0000015CBAB689D0>
2025-02-04 00:51:12,206 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:51:12,206 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from trainingData_ELMO.txt
2025-02-04 00:51:13,095 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from trainingData_ELMO.txt.
2025-02-04 00:51:13,096 - INFO - allennlp.common.params - data_loader.type = multiprocess
2025-02-04 00:51:13,096 - INFO - allennlp.common.params - data_loader.batch_size = 32
2025-02-04 00:51:13,097 - INFO - allennlp.common.params - data_loader.drop_last = False
2025-02-04 00:51:13,097 - INFO - allennlp.common.params - data_loader.shuffle = True
2025-02-04 00:51:13,097 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2025-02-04 00:51:13,097 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2025-02-04 00:51:13,097 - INFO - allennlp.common.params - data_loader.num_workers = 0
2025-02-04 00:51:13,097 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2025-02-04 00:51:13,098 - INFO - allennlp.common.params - data_loader.start_method = fork
2025-02-04 00:51:13,098 - INFO - allennlp.common.params - data_loader.cuda_device = None
2025-02-04 00:51:13,098 - INFO - allennlp.common.params - data_loader.quiet = False
2025-02-04 00:51:13,098 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x0000015CBAB689D0>
2025-02-04 00:51:13,098 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2025-02-04 00:51:13,098 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - Loading data from testData_ELMO.txt
2025-02-04 00:51:13,437 - INFO - allennlp_models.lm.dataset_readers.simple_language_modeling - No instances dropped from testData_ELMO.txt.
2025-02-04 00:51:13,438 - INFO - allennlp.common.params - type = from_instances
2025-02-04 00:51:13,438 - INFO - allennlp.common.params - min_count = None
2025-02-04 00:51:13,439 - INFO - allennlp.common.params - max_vocab_size = None
2025-02-04 00:51:13,439 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2025-02-04 00:51:13,439 - INFO - allennlp.common.params - pretrained_files = None
2025-02-04 00:51:13,439 - INFO - allennlp.common.params - only_include_pretrained_words = False
2025-02-04 00:51:13,440 - INFO - allennlp.common.params - tokens_to_add = None
2025-02-04 00:51:13,440 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2025-02-04 00:51:13,440 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2025-02-04 00:51:13,441 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2025-02-04 00:51:13,441 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2025-02-04 00:51:13,441 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2025-02-04 00:51:13,760 - INFO - allennlp.common.params - model.type = bidirectional-language-model
2025-02-04 00:51:13,760 - INFO - allennlp.common.params - model.regularizer = None
2025-02-04 00:51:13,761 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2025-02-04 00:51:13,761 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = embedding
2025-02-04 00:51:13,761 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.embedding_dim = 128
2025-02-04 00:51:13,761 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.num_embeddings = None
2025-02-04 00:51:13,761 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.projection_dim = None
2025-02-04 00:51:13,762 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.weight = None
2025-02-04 00:51:13,762 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.padding_index = None
2025-02-04 00:51:13,762 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.trainable = True
2025-02-04 00:51:13,762 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_norm = None
2025-02-04 00:51:13,762 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.norm_type = 2.0
2025-02-04 00:51:13,762 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False
2025-02-04 00:51:13,762 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sparse = False
2025-02-04 00:51:13,763 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens
2025-02-04 00:51:13,763 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.pretrained_file = None
2025-02-04 00:51:13,787 - INFO - allennlp.common.params - model.contextualizer.type = lstm
2025-02-04 00:51:13,788 - INFO - allennlp.common.params - model.contextualizer.input_size = 128
2025-02-04 00:51:13,788 - INFO - allennlp.common.params - model.contextualizer.hidden_size = 512
2025-02-04 00:51:13,788 - INFO - allennlp.common.params - model.contextualizer.num_layers = 2
2025-02-04 00:51:13,788 - INFO - allennlp.common.params - model.contextualizer.bias = True
2025-02-04 00:51:13,788 - INFO - allennlp.common.params - model.contextualizer.dropout = 0.0
2025-02-04 00:51:13,789 - INFO - allennlp.common.params - model.contextualizer.bidirectional = True
2025-02-04 00:51:13,789 - INFO - allennlp.common.params - model.contextualizer.stateful = False
2025-02-04 00:51:13,825 - INFO - allennlp.common.params - model.dropout = 0.1
2025-02-04 00:51:13,826 - INFO - allennlp.common.params - model.num_samples = None
2025-02-04 00:51:13,826 - INFO - allennlp.common.params - model.sparse_embeddings = False
2025-02-04 00:51:13,826 - INFO - allennlp.common.params - model.bidirectional = False
2025-02-04 00:51:13,826 - INFO - allennlp.common.params - model.initializer = None
2025-02-04 00:51:13,980 - WARNING - allennlp.data.vocabulary - vocabulary serialization directory ./elmo\vocabulary is not empty
2025-02-04 00:51:14,336 - INFO - allennlp.common.params - trainer.type = gradient_descent
2025-02-04 00:51:14,337 - INFO - allennlp.common.params - trainer.cuda_device = -1
2025-02-04 00:51:14,337 - INFO - allennlp.common.params - trainer.distributed = False
2025-02-04 00:51:14,337 - INFO - allennlp.common.params - trainer.world_size = 1
2025-02-04 00:51:14,337 - INFO - allennlp.common.params - trainer.patience = 2
2025-02-04 00:51:14,338 - INFO - allennlp.common.params - trainer.validation_metric = -perplexity
2025-02-04 00:51:14,338 - INFO - allennlp.common.params - trainer.num_epochs = 32
2025-02-04 00:51:14,338 - INFO - allennlp.common.params - trainer.grad_norm = False
2025-02-04 00:51:14,338 - INFO - allennlp.common.params - trainer.grad_clipping = None
2025-02-04 00:51:14,338 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1
2025-02-04 00:51:14,338 - INFO - allennlp.common.params - trainer.use_amp = False
2025-02-04 00:51:14,338 - INFO - allennlp.common.params - trainer.no_grad = None
2025-02-04 00:51:14,339 - INFO - allennlp.common.params - trainer.learning_rate_scheduler = None
2025-02-04 00:51:14,339 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2025-02-04 00:51:14,339 - INFO - allennlp.common.params - trainer.moving_average = None
2025-02-04 00:51:14,339 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x0000015CBD22B220>
2025-02-04 00:51:14,339 - INFO - allennlp.common.params - trainer.callbacks = None
2025-02-04 00:51:14,339 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True
2025-02-04 00:51:14,340 - INFO - allennlp.common.params - trainer.run_confidence_checks = True
2025-02-04 00:51:14,340 - INFO - allennlp.common.params - trainer.grad_scaling = True
2025-02-04 00:51:14,340 - INFO - allennlp.common.params - trainer.optimizer.type = adam
2025-02-04 00:51:14,340 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2025-02-04 00:51:14,340 - INFO - allennlp.common.params - trainer.optimizer.lr = 0.001
2025-02-04 00:51:14,340 - INFO - allennlp.common.params - trainer.optimizer.betas = (0.9, 0.999)
2025-02-04 00:51:14,341 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-08
2025-02-04 00:51:14,341 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.0
2025-02-04 00:51:14,341 - INFO - allennlp.common.params - trainer.optimizer.amsgrad = False
2025-02-04 00:51:14,341 - INFO - allennlp.training.optimizers - Number of trainable parameters: 40404303
2025-02-04 00:51:14,342 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2025-02-04 00:51:14,342 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2025-02-04 00:51:14,342 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.weight
2025-02-04 00:51:14,343 - INFO - allennlp.common.util - _contextualizer._module.weight_ih_l0
2025-02-04 00:51:14,343 - INFO - allennlp.common.util - _contextualizer._module.weight_hh_l0
2025-02-04 00:51:14,343 - INFO - allennlp.common.util - _contextualizer._module.bias_ih_l0
2025-02-04 00:51:14,343 - INFO - allennlp.common.util - _contextualizer._module.bias_hh_l0
2025-02-04 00:51:14,343 - INFO - allennlp.common.util - _contextualizer._module.weight_ih_l0_reverse
2025-02-04 00:51:14,343 - INFO - allennlp.common.util - _contextualizer._module.weight_hh_l0_reverse
2025-02-04 00:51:14,343 - INFO - allennlp.common.util - _contextualizer._module.bias_ih_l0_reverse
2025-02-04 00:51:14,343 - INFO - allennlp.common.util - _contextualizer._module.bias_hh_l0_reverse
2025-02-04 00:51:14,344 - INFO - allennlp.common.util - _contextualizer._module.weight_ih_l1
2025-02-04 00:51:14,344 - INFO - allennlp.common.util - _contextualizer._module.weight_hh_l1
2025-02-04 00:51:14,344 - INFO - allennlp.common.util - _contextualizer._module.bias_ih_l1
2025-02-04 00:51:14,344 - INFO - allennlp.common.util - _contextualizer._module.bias_hh_l1
2025-02-04 00:51:14,344 - INFO - allennlp.common.util - _contextualizer._module.weight_ih_l1_reverse
2025-02-04 00:51:14,344 - INFO - allennlp.common.util - _contextualizer._module.weight_hh_l1_reverse
2025-02-04 00:51:14,345 - INFO - allennlp.common.util - _contextualizer._module.bias_ih_l1_reverse
2025-02-04 00:51:14,345 - INFO - allennlp.common.util - _contextualizer._module.bias_hh_l1_reverse
2025-02-04 00:51:14,345 - INFO - allennlp.common.util - _softmax_loss.softmax_w
2025-02-04 00:51:14,345 - INFO - allennlp.common.util - _softmax_loss.softmax_b
2025-02-04 00:51:14,345 - INFO - allennlp.common.params - type = default
2025-02-04 00:51:14,345 - INFO - allennlp.common.params - save_completed_epochs = True
2025-02-04 00:51:14,346 - INFO - allennlp.common.params - save_every_num_seconds = None
2025-02-04 00:51:14,346 - INFO - allennlp.common.params - save_every_num_batches = None
2025-02-04 00:51:14,346 - INFO - allennlp.common.params - keep_most_recent_by_count = 2
2025-02-04 00:51:14,346 - INFO - allennlp.common.params - keep_most_recent_by_age = None
2025-02-04 00:51:14,347 - INFO - allennlp.training.gradient_descent_trainer - Beginning training.
2025-02-04 00:51:14,347 - INFO - allennlp.training.gradient_descent_trainer - Epoch 0/31
2025-02-04 00:51:14,347 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 0B
2025-02-04 00:51:14,348 - INFO - allennlp.training.gradient_descent_trainer - Training
2025-02-04 00:51:14,348 - INFO - tqdm - 0%|          | 0/516 [00:00<?, ?it/s]
2025-02-04 00:51:18,265 - INFO - allennlp.training.callbacks.console_logger - Batch inputs
2025-02-04 00:51:18,265 - INFO - allennlp.training.callbacks.console_logger - batch_input/source/tokens/tokens (Shape: 32 x 66)
tensor([[17012,     2, 33283,  ...,     0,     0,     0],
        [ 5637,     4,  1317,  ...,     0,     0,     0],
        [   66,     3,  1566,  ...,     0,     0,     0],
        ...,
        [    4,     8,  8623,  ...,     5,   164,     7],
        [   20,     9,   718,  ...,     0,     0,     0],
        [    4,  3880,     5,  ...,     0,     0,     0]])
2025-02-04 00:51:27,842 - INFO - tqdm - perplexity: 57054.7159, batch_loss: 10.8689, loss: 10.9518 ||:   1%|          | 3/516 [00:13<39:13,  4.59s/it]
2025-02-04 00:51:40,286 - INFO - tqdm - perplexity: 34932.9383, batch_loss: 9.2704, loss: 10.4612 ||:   1%|1         | 6/516 [00:25<35:35,  4.19s/it]
2025-02-04 00:51:53,490 - INFO - tqdm - perplexity: 16019.7273, batch_loss: 7.6295, loss: 9.6816 ||:   2%|1         | 9/516 [00:39<37:22,  4.42s/it]
2025-02-04 00:52:04,613 - INFO - tqdm - perplexity: 9790.9191, batch_loss: 7.6952, loss: 9.1892 ||:   2%|2         | 12/516 [00:50<33:08,  3.95s/it]
2025-02-04 00:52:16,421 - INFO - tqdm - perplexity: 6893.4386, batch_loss: 7.2221, loss: 8.8383 ||:   3%|2         | 15/516 [01:02<32:48,  3.93s/it]
2025-02-04 00:52:27,185 - INFO - tqdm - perplexity: 6189.4507, batch_loss: 7.6763, loss: 8.7306 ||:   3%|3         | 17/516 [01:12<37:58,  4.57s/it]
2025-02-04 00:52:38,166 - INFO - tqdm - perplexity: 5306.3239, batch_loss: 7.6445, loss: 8.5767 ||:   4%|3         | 20/516 [01:23<33:37,  4.07s/it]
2025-02-04 00:52:50,738 - INFO - tqdm - perplexity: 4629.2000, batch_loss: 7.3233, loss: 8.4401 ||:   4%|4         | 23/516 [01:36<33:08,  4.03s/it]
2025-02-04 00:53:04,142 - INFO - tqdm - perplexity: 4099.9244, batch_loss: 7.5018, loss: 8.3187 ||:   5%|5         | 26/516 [01:49<36:21,  4.45s/it]
2025-02-04 00:53:17,123 - INFO - tqdm - perplexity: 3716.6274, batch_loss: 7.4078, loss: 8.2206 ||:   6%|5         | 29/516 [02:02<35:51,  4.42s/it]
2025-02-04 00:53:30,763 - INFO - tqdm - perplexity: 3483.1314, batch_loss: 7.6673, loss: 8.1557 ||:   6%|6         | 32/516 [02:16<35:37,  4.42s/it]
2025-02-04 00:53:42,858 - INFO - tqdm - perplexity: 3290.6749, batch_loss: 7.4815, loss: 8.0988 ||:   7%|6         | 35/516 [02:28<33:29,  4.18s/it]
2025-02-04 00:53:56,949 - INFO - tqdm - perplexity: 3163.9279, batch_loss: 7.4613, loss: 8.0596 ||:   7%|7         | 38/516 [02:42<36:37,  4.60s/it]
2025-02-04 00:54:06,992 - INFO - tqdm - perplexity: 3057.2848, batch_loss: 7.4647, loss: 8.0253 ||:   8%|7         | 40/516 [02:52<37:51,  4.77s/it]
2025-02-04 00:54:21,214 - INFO - tqdm - perplexity: 2948.1478, batch_loss: 7.3807, loss: 7.9889 ||:   8%|8         | 43/516 [03:06<37:55,  4.81s/it]
2025-02-04 00:54:33,432 - INFO - tqdm - perplexity: 2864.4316, batch_loss: 7.3730, loss: 7.9601 ||:   9%|8         | 45/516 [03:19<41:56,  5.34s/it]
2025-02-04 00:54:43,711 - INFO - tqdm - perplexity: 2801.8288, batch_loss: 7.5561, loss: 7.9380 ||:   9%|9         | 47/516 [03:29<40:06,  5.13s/it]
2025-02-04 00:54:57,762 - INFO - tqdm - perplexity: 2679.0778, batch_loss: 7.4081, loss: 7.8932 ||:  10%|9         | 50/516 [03:43<37:07,  4.78s/it]
2025-02-04 00:55:10,478 - INFO - tqdm - perplexity: 2652.1113, batch_loss: 7.5098, loss: 7.8831 ||:  10%|#         | 52/516 [03:56<43:05,  5.57s/it]
2025-02-04 00:55:26,452 - INFO - tqdm - perplexity: 2550.4451, batch_loss: 7.0570, loss: 7.8440 ||:  11%|#         | 55/516 [04:12<42:43,  5.56s/it]
2025-02-04 00:55:37,807 - INFO - tqdm - perplexity: 2489.6720, batch_loss: 7.1877, loss: 7.8199 ||:  11%|#1        | 57/516 [04:23<43:29,  5.69s/it]
2025-02-04 00:55:49,924 - INFO - tqdm - perplexity: 2419.0446, batch_loss: 7.2299, loss: 7.7911 ||:  12%|#1        | 60/516 [04:35<35:18,  4.64s/it]
2025-02-04 00:56:00,223 - INFO - tqdm - perplexity: 2373.2630, batch_loss: 7.1341, loss: 7.7720 ||:  12%|#2        | 62/516 [04:45<36:53,  4.88s/it]
2025-02-04 00:56:10,572 - INFO - tqdm - perplexity: 2322.2197, batch_loss: 7.0208, loss: 7.7503 ||:  12%|#2        | 64/516 [04:56<38:22,  5.09s/it]
2025-02-04 00:56:25,146 - INFO - tqdm - perplexity: 2284.5865, batch_loss: 7.1362, loss: 7.7339 ||:  13%|#2        | 67/516 [05:10<37:03,  4.95s/it]
2025-02-04 00:56:39,993 - INFO - tqdm - perplexity: 2240.0995, batch_loss: 7.2070, loss: 7.7143 ||:  14%|#3        | 70/516 [05:25<37:41,  5.07s/it]
2025-02-04 00:56:52,916 - INFO - tqdm - perplexity: 2237.9320, batch_loss: 7.4509, loss: 7.7133 ||:  14%|#3        | 72/516 [05:38<41:40,  5.63s/it]
2025-02-04 00:57:03,151 - INFO - tqdm - perplexity: 2208.3181, batch_loss: 7.3032, loss: 7.7000 ||:  14%|#4        | 74/516 [05:48<39:45,  5.40s/it]
2025-02-04 00:57:17,954 - INFO - tqdm - perplexity: 2164.3836, batch_loss: 7.3625, loss: 7.6799 ||:  15%|#4        | 77/516 [06:03<37:26,  5.12s/it]
2025-02-04 00:57:28,128 - INFO - tqdm - perplexity: 2140.6482, batch_loss: 7.3417, loss: 7.6689 ||:  15%|#5        | 79/516 [06:13<37:40,  5.17s/it]
2025-02-04 00:57:44,205 - INFO - tqdm - perplexity: 2110.6500, batch_loss: 7.2376, loss: 7.6548 ||:  16%|#5        | 82/516 [06:29<39:21,  5.44s/it]
2025-02-04 00:57:54,665 - INFO - tqdm - perplexity: 2089.5221, batch_loss: 7.1269, loss: 7.6447 ||:  16%|#6        | 84/516 [06:40<38:07,  5.29s/it]
2025-02-04 00:58:08,169 - INFO - tqdm - perplexity: 2049.7565, batch_loss: 7.0093, loss: 7.6255 ||:  17%|#6        | 87/516 [06:53<34:00,  4.76s/it]
2025-02-04 00:58:18,807 - INFO - tqdm - perplexity: 2033.4438, batch_loss: 7.1541, loss: 7.6175 ||:  17%|#7        | 89/516 [07:04<36:05,  5.07s/it]
2025-02-04 00:58:29,153 - INFO - tqdm - perplexity: 2016.0220, batch_loss: 7.0513, loss: 7.6089 ||:  18%|#7        | 91/516 [07:14<37:05,  5.24s/it]
2025-02-04 00:58:41,069 - INFO - tqdm - perplexity: 2006.0222, batch_loss: 7.4855, loss: 7.6039 ||:  18%|#8        | 93/516 [07:26<39:54,  5.66s/it]
2025-02-04 00:58:51,955 - INFO - tqdm - perplexity: 1995.9994, batch_loss: 7.4208, loss: 7.5989 ||:  18%|#8        | 95/516 [07:37<38:17,  5.46s/it]
2025-02-04 00:59:06,240 - INFO - tqdm - perplexity: 1975.0450, batch_loss: 7.1105, loss: 7.5883 ||:  19%|#8        | 98/516 [07:51<34:52,  5.01s/it]
2025-02-04 00:59:19,171 - INFO - tqdm - perplexity: 1952.2913, batch_loss: 7.0874, loss: 7.5768 ||:  20%|#9        | 101/516 [08:04<31:23,  4.54s/it]
2025-02-04 00:59:30,611 - INFO - tqdm - perplexity: 1939.6835, batch_loss: 6.9803, loss: 7.5703 ||:  20%|#9        | 103/516 [08:16<36:12,  5.26s/it]
2025-02-04 00:59:44,865 - INFO - tqdm - perplexity: 1924.7423, batch_loss: 7.2110, loss: 7.5625 ||:  21%|##        | 106/516 [08:30<33:38,  4.92s/it]
2025-02-04 00:59:58,028 - INFO - tqdm - perplexity: 1908.4122, batch_loss: 7.2571, loss: 7.5540 ||:  21%|##1       | 109/516 [08:43<30:33,  4.51s/it]
2025-02-04 01:00:08,601 - INFO - tqdm - perplexity: 1900.5041, batch_loss: 7.4156, loss: 7.5499 ||:  22%|##1       | 111/516 [08:54<33:08,  4.91s/it]
2025-02-04 01:00:21,703 - INFO - tqdm - perplexity: 1886.3952, batch_loss: 7.2957, loss: 7.5424 ||:  22%|##2       | 114/516 [09:07<30:40,  4.58s/it]
2025-02-04 01:00:35,374 - INFO - tqdm - perplexity: 1864.9066, batch_loss: 7.1591, loss: 7.5310 ||:  23%|##2       | 117/516 [09:21<30:49,  4.64s/it]
2025-02-04 01:00:46,337 - INFO - tqdm - perplexity: 1853.7567, batch_loss: 7.1707, loss: 7.5250 ||:  23%|##3       | 119/516 [09:31<33:55,  5.13s/it]
2025-02-04 01:00:56,672 - INFO - tqdm - perplexity: 1851.3516, batch_loss: 7.5972, loss: 7.5237 ||:  23%|##3       | 121/516 [09:42<34:32,  5.25s/it]
2025-02-04 01:01:09,300 - INFO - tqdm - perplexity: 1830.4464, batch_loss: 7.1101, loss: 7.5123 ||:  24%|##4       | 124/516 [09:54<30:56,  4.74s/it]
2025-02-04 01:01:23,104 - INFO - tqdm - perplexity: 1817.0753, batch_loss: 7.3768, loss: 7.5050 ||:  25%|##4       | 127/516 [10:08<30:25,  4.69s/it]
2025-02-04 01:01:37,063 - INFO - tqdm - perplexity: 1807.0257, batch_loss: 7.3903, loss: 7.4994 ||:  25%|##5       | 130/516 [10:22<29:34,  4.60s/it]
2025-02-04 01:01:51,203 - INFO - tqdm - perplexity: 1790.3115, batch_loss: 6.9579, loss: 7.4901 ||:  26%|##5       | 133/516 [10:36<30:13,  4.74s/it]
2025-02-04 01:02:04,656 - INFO - tqdm - perplexity: 1784.7733, batch_loss: 7.4891, loss: 7.4870 ||:  26%|##6       | 135/516 [10:50<36:59,  5.83s/it]
2025-02-04 01:02:21,580 - INFO - tqdm - perplexity: 1775.3311, batch_loss: 7.2493, loss: 7.4817 ||:  27%|##6       | 138/516 [11:07<37:11,  5.90s/it]
2025-02-04 01:02:32,062 - INFO - tqdm - perplexity: 1767.0706, batch_loss: 7.2193, loss: 7.4771 ||:  27%|##7       | 140/516 [11:17<34:16,  5.47s/it]
2025-02-04 01:02:48,898 - INFO - tqdm - perplexity: 1759.7619, batch_loss: 7.4191, loss: 7.4729 ||:  28%|##7       | 143/516 [11:34<35:27,  5.70s/it]
2025-02-04 01:03:04,342 - INFO - tqdm - perplexity: 1749.4799, batch_loss: 7.1236, loss: 7.4671 ||:  28%|##8       | 146/516 [11:49<33:01,  5.36s/it]
2025-02-04 01:03:18,587 - INFO - tqdm - perplexity: 1744.1760, batch_loss: 7.1410, loss: 7.4640 ||:  29%|##8       | 149/516 [12:04<30:49,  5.04s/it]
2025-02-04 01:03:31,974 - INFO - tqdm - perplexity: 1728.4182, batch_loss: 7.0229, loss: 7.4550 ||:  29%|##9       | 152/516 [12:17<28:03,  4.63s/it]
2025-02-04 01:03:48,861 - INFO - tqdm - perplexity: 1719.1255, batch_loss: 7.1399, loss: 7.4496 ||:  30%|###       | 155/516 [12:34<33:09,  5.51s/it]
2025-02-04 01:03:59,351 - INFO - tqdm - perplexity: 1713.7978, batch_loss: 7.3149, loss: 7.4465 ||:  30%|###       | 157/516 [12:45<31:57,  5.34s/it]
2025-02-04 01:04:10,874 - INFO - tqdm - perplexity: 1711.5033, batch_loss: 7.1529, loss: 7.4451 ||:  31%|###       | 159/516 [12:56<32:34,  5.47s/it]
2025-02-04 01:04:21,670 - INFO - tqdm - perplexity: 1708.1883, batch_loss: 7.2535, loss: 7.4432 ||:  31%|###1      | 161/516 [13:07<32:00,  5.41s/it]
2025-02-04 01:04:32,824 - INFO - tqdm - perplexity: 1702.2177, batch_loss: 7.0970, loss: 7.4397 ||:  32%|###1      | 163/516 [13:18<32:05,  5.45s/it]
2025-02-04 01:04:43,265 - INFO - tqdm - perplexity: 1696.2192, batch_loss: 7.1504, loss: 7.4362 ||:  32%|###1      | 165/516 [13:28<31:11,  5.33s/it]
2025-02-04 01:04:57,394 - INFO - tqdm - perplexity: 1688.1126, batch_loss: 7.0564, loss: 7.4314 ||:  33%|###2      | 168/516 [13:43<29:42,  5.12s/it]
2025-02-04 01:05:12,553 - INFO - tqdm - perplexity: 1673.8116, batch_loss: 7.0024, loss: 7.4229 ||:  33%|###3      | 171/516 [13:58<30:04,  5.23s/it]
2025-02-04 01:05:22,698 - INFO - tqdm - perplexity: 1666.0489, batch_loss: 6.9683, loss: 7.4182 ||:  34%|###3      | 173/516 [14:08<29:20,  5.13s/it]
2025-02-04 01:05:34,947 - INFO - tqdm - perplexity: 1661.0201, batch_loss: 7.2280, loss: 7.4152 ||:  34%|###3      | 175/516 [14:20<32:17,  5.68s/it]
2025-02-04 01:05:45,984 - INFO - tqdm - perplexity: 1655.7919, batch_loss: 6.9568, loss: 7.4120 ||:  34%|###4      | 177/516 [14:31<31:23,  5.56s/it]
2025-02-04 01:05:57,000 - INFO - tqdm - perplexity: 1649.5589, batch_loss: 7.0420, loss: 7.4083 ||:  35%|###4      | 179/516 [14:42<30:44,  5.47s/it]
2025-02-04 01:06:12,090 - INFO - tqdm - perplexity: 1641.7178, batch_loss: 7.1513, loss: 7.4035 ||:  35%|###5      | 182/516 [14:57<29:00,  5.21s/it]
2025-02-04 01:06:24,302 - INFO - tqdm - perplexity: 1637.0854, batch_loss: 7.1276, loss: 7.4007 ||:  36%|###5      | 184/516 [15:09<31:16,  5.65s/it]
2025-02-04 01:06:36,241 - INFO - tqdm - perplexity: 1633.1745, batch_loss: 7.1297, loss: 7.3983 ||:  36%|###6      | 186/516 [15:21<31:56,  5.81s/it]
2025-02-04 01:06:49,967 - INFO - tqdm - perplexity: 1630.2682, batch_loss: 7.4091, loss: 7.3965 ||:  37%|###6      | 189/516 [15:35<27:11,  4.99s/it]
2025-02-04 01:07:01,167 - INFO - tqdm - perplexity: 1622.8808, batch_loss: 7.0457, loss: 7.3920 ||:  37%|###7      | 191/516 [15:46<28:07,  5.19s/it]
2025-02-04 01:07:13,243 - INFO - tqdm - perplexity: 1620.2108, batch_loss: 7.2526, loss: 7.3903 ||:  37%|###7      | 193/516 [15:58<29:55,  5.56s/it]
2025-02-04 01:07:24,104 - INFO - tqdm - perplexity: 1618.7652, batch_loss: 7.4139, loss: 7.3894 ||:  38%|###7      | 195/516 [16:09<28:54,  5.40s/it]
2025-02-04 01:07:39,259 - INFO - tqdm - perplexity: 1613.0795, batch_loss: 7.0965, loss: 7.3859 ||:  38%|###8      | 198/516 [16:24<27:27,  5.18s/it]
2025-02-04 01:07:49,913 - INFO - tqdm - perplexity: 1609.7921, batch_loss: 7.0454, loss: 7.3839 ||:  39%|###8      | 200/516 [16:35<27:11,  5.16s/it]
2025-02-04 01:08:00,082 - INFO - tqdm - perplexity: 1604.9094, batch_loss: 7.0265, loss: 7.3808 ||:  39%|###9      | 202/516 [16:45<26:38,  5.09s/it]
2025-02-04 01:08:14,073 - INFO - tqdm - perplexity: 1603.0015, batch_loss: 7.3769, loss: 7.3796 ||:  40%|###9      | 205/516 [16:59<24:51,  4.80s/it]
2025-02-04 01:08:30,087 - INFO - tqdm - perplexity: 1597.9962, batch_loss: 7.1562, loss: 7.3765 ||:  40%|####      | 208/516 [17:15<27:29,  5.36s/it]
2025-02-04 01:08:44,763 - INFO - tqdm - perplexity: 1587.9012, batch_loss: 6.8544, loss: 7.3702 ||:  41%|####      | 211/516 [17:30<26:39,  5.24s/it]
2025-02-04 01:09:01,185 - INFO - tqdm - perplexity: 1581.5166, batch_loss: 7.3597, loss: 7.3661 ||:  41%|####1     | 214/516 [17:46<28:12,  5.60s/it]
2025-02-04 01:09:13,320 - INFO - tqdm - perplexity: 1576.4886, batch_loss: 7.0537, loss: 7.3630 ||:  42%|####1     | 216/516 [17:58<29:02,  5.81s/it]
2025-02-04 01:09:27,709 - INFO - tqdm - perplexity: 1570.4161, batch_loss: 7.2067, loss: 7.3591 ||:  42%|####2     | 219/516 [18:13<26:25,  5.34s/it]
2025-02-04 01:09:40,823 - INFO - tqdm - perplexity: 1568.5496, batch_loss: 7.2267, loss: 7.3579 ||:  43%|####2     | 221/516 [18:26<29:03,  5.91s/it]
2025-02-04 01:09:50,942 - INFO - tqdm - perplexity: 1564.4377, batch_loss: 6.9196, loss: 7.3553 ||:  43%|####3     | 223/516 [18:36<26:26,  5.41s/it]
2025-02-04 01:10:04,259 - INFO - tqdm - perplexity: 1556.8074, batch_loss: 6.9560, loss: 7.3504 ||:  44%|####3     | 226/516 [18:49<22:54,  4.74s/it]
2025-02-04 01:10:18,617 - INFO - tqdm - perplexity: 1550.3481, batch_loss: 7.0883, loss: 7.3462 ||:  44%|####4     | 229/516 [19:04<22:50,  4.78s/it]
2025-02-04 01:10:28,950 - INFO - tqdm - perplexity: 1547.6612, batch_loss: 7.0539, loss: 7.3445 ||:  45%|####4     | 231/516 [19:14<23:41,  4.99s/it]
2025-02-04 01:10:39,656 - INFO - tqdm - perplexity: 1544.2428, batch_loss: 7.2628, loss: 7.3423 ||:  45%|####5     | 233/516 [19:25<24:11,  5.13s/it]
2025-02-04 01:10:52,910 - INFO - tqdm - perplexity: 1539.5026, batch_loss: 6.9693, loss: 7.3392 ||:  46%|####5     | 236/516 [19:38<21:36,  4.63s/it]
2025-02-04 01:11:05,446 - INFO - tqdm - perplexity: 1539.4130, batch_loss: 7.4831, loss: 7.3392 ||:  46%|####6     | 239/516 [19:51<19:32,  4.23s/it]
2025-02-04 01:11:16,873 - INFO - tqdm - perplexity: 1537.9504, batch_loss: 7.0969, loss: 7.3382 ||:  47%|####6     | 241/516 [20:02<23:34,  5.14s/it]
2025-02-04 01:11:31,350 - INFO - tqdm - perplexity: 1536.0497, batch_loss: 7.1260, loss: 7.3370 ||:  47%|####7     | 243/516 [20:17<28:18,  6.22s/it]
2025-02-04 01:11:44,853 - INFO - tqdm - perplexity: 1533.6153, batch_loss: 7.1488, loss: 7.3354 ||:  48%|####7     | 246/516 [20:30<23:18,  5.18s/it]
2025-02-04 01:11:58,123 - INFO - tqdm - perplexity: 1526.8860, batch_loss: 6.9407, loss: 7.3310 ||:  48%|####8     | 249/516 [20:43<20:49,  4.68s/it]
2025-02-04 01:12:11,528 - INFO - tqdm - perplexity: 1523.1886, batch_loss: 7.1092, loss: 7.3286 ||:  49%|####8     | 252/516 [20:57<19:45,  4.49s/it]
2025-02-04 01:12:26,712 - INFO - tqdm - perplexity: 1521.3474, batch_loss: 7.4437, loss: 7.3274 ||:  49%|####9     | 255/516 [21:12<21:39,  4.98s/it]
2025-02-04 01:12:39,853 - INFO - tqdm - perplexity: 1518.1960, batch_loss: 7.2014, loss: 7.3253 ||:  50%|#####     | 258/516 [21:25<19:22,  4.51s/it]
2025-02-04 01:12:53,419 - INFO - tqdm - perplexity: 1514.9192, batch_loss: 7.1644, loss: 7.3231 ||:  51%|#####     | 261/516 [21:39<18:50,  4.43s/it]
2025-02-04 01:13:07,550 - INFO - tqdm - perplexity: 1511.2688, batch_loss: 6.9125, loss: 7.3207 ||:  51%|#####1    | 264/516 [21:53<19:33,  4.66s/it]
2025-02-04 01:13:18,093 - INFO - tqdm - perplexity: 1507.7072, batch_loss: 6.9104, loss: 7.3183 ||:  52%|#####1    | 266/516 [22:03<20:58,  5.03s/it]
2025-02-04 01:13:33,072 - INFO - tqdm - perplexity: 1502.9566, batch_loss: 7.0656, loss: 7.3152 ||:  52%|#####2    | 269/516 [22:18<20:37,  5.01s/it]
2025-02-04 01:13:45,696 - INFO - tqdm - perplexity: 1500.8148, batch_loss: 7.2603, loss: 7.3138 ||:  53%|#####2    | 272/516 [22:31<18:01,  4.43s/it]
2025-02-04 01:14:00,265 - INFO - tqdm - perplexity: 1497.2986, batch_loss: 7.0865, loss: 7.3114 ||:  53%|#####3    | 275/516 [22:45<19:22,  4.82s/it]
2025-02-04 01:14:10,782 - INFO - tqdm - perplexity: 1495.5042, batch_loss: 7.3704, loss: 7.3102 ||:  54%|#####3    | 277/516 [22:56<20:18,  5.10s/it]
2025-02-04 01:14:21,643 - INFO - tqdm - perplexity: 1493.9123, batch_loss: 7.0726, loss: 7.3092 ||:  54%|#####4    | 279/516 [23:07<20:17,  5.14s/it]
2025-02-04 01:14:34,568 - INFO - tqdm - perplexity: 1491.4680, batch_loss: 7.0048, loss: 7.3075 ||:  54%|#####4    | 281/516 [23:20<22:27,  5.73s/it]
2025-02-04 01:14:48,401 - INFO - tqdm - perplexity: 1487.3606, batch_loss: 6.9720, loss: 7.3048 ||:  55%|#####5    | 284/516 [23:34<19:05,  4.94s/it]
2025-02-04 01:15:01,652 - INFO - tqdm - perplexity: 1483.8419, batch_loss: 6.9674, loss: 7.3024 ||:  56%|#####5    | 287/516 [23:47<17:52,  4.68s/it]
2025-02-04 01:15:17,465 - INFO - tqdm - perplexity: 1480.4677, batch_loss: 6.9073, loss: 7.3001 ||:  56%|#####6    | 290/516 [24:03<19:57,  5.30s/it]
2025-02-04 01:15:27,632 - INFO - tqdm - perplexity: 1476.8392, batch_loss: 7.0122, loss: 7.2977 ||:  57%|#####6    | 292/516 [24:13<19:19,  5.18s/it]
2025-02-04 01:15:41,932 - INFO - tqdm - perplexity: 1473.2670, batch_loss: 6.9909, loss: 7.2952 ||:  57%|#####7    | 295/516 [24:27<17:55,  4.87s/it]
2025-02-04 01:15:54,257 - INFO - tqdm - perplexity: 1470.7533, batch_loss: 7.0677, loss: 7.2935 ||:  58%|#####7    | 298/516 [24:39<15:51,  4.36s/it]
2025-02-04 01:16:08,127 - INFO - tqdm - perplexity: 1467.7516, batch_loss: 6.9965, loss: 7.2915 ||:  58%|#####8    | 301/516 [24:53<16:05,  4.49s/it]
2025-02-04 01:16:21,188 - INFO - tqdm - perplexity: 1464.6559, batch_loss: 6.9008, loss: 7.2894 ||:  59%|#####8    | 304/516 [25:06<15:58,  4.52s/it]
2025-02-04 01:16:31,782 - INFO - tqdm - perplexity: 1461.6360, batch_loss: 6.8583, loss: 7.2873 ||:  59%|#####9    | 306/516 [25:17<16:55,  4.84s/it]
2025-02-04 01:16:45,131 - INFO - tqdm - perplexity: 1456.6669, batch_loss: 7.0067, loss: 7.2839 ||:  60%|#####9    | 309/516 [25:30<15:47,  4.58s/it]
2025-02-04 01:16:57,171 - INFO - tqdm - perplexity: 1452.9706, batch_loss: 6.9710, loss: 7.2814 ||:  60%|######    | 311/516 [25:42<18:21,  5.37s/it]
2025-02-04 01:17:11,306 - INFO - tqdm - perplexity: 1449.0325, batch_loss: 6.9602, loss: 7.2787 ||:  61%|######    | 314/516 [25:56<16:17,  4.84s/it]
2025-02-04 01:17:22,107 - INFO - tqdm - perplexity: 1446.0066, batch_loss: 7.1049, loss: 7.2766 ||:  61%|######1   | 316/516 [26:07<17:24,  5.22s/it]
2025-02-04 01:17:32,144 - INFO - tqdm - perplexity: 1442.0440, batch_loss: 6.8576, loss: 7.2738 ||:  62%|######1   | 318/516 [26:17<17:00,  5.15s/it]
2025-02-04 01:17:44,537 - INFO - tqdm - perplexity: 1438.6528, batch_loss: 6.9926, loss: 7.2715 ||:  62%|######2   | 320/516 [26:30<18:42,  5.73s/it]
2025-02-04 01:17:57,463 - INFO - tqdm - perplexity: 1433.2342, batch_loss: 6.6990, loss: 7.2677 ||:  63%|######2   | 323/516 [26:43<15:22,  4.78s/it]
2025-02-04 01:18:11,188 - INFO - tqdm - perplexity: 1427.1918, batch_loss: 6.8639, loss: 7.2635 ||:  63%|######3   | 326/516 [26:56<14:05,  4.45s/it]
2025-02-04 01:18:26,113 - INFO - tqdm - perplexity: 1419.6768, batch_loss: 6.5472, loss: 7.2582 ||:  64%|######3   | 329/516 [27:11<15:15,  4.90s/it]
2025-02-04 01:18:38,650 - INFO - tqdm - perplexity: 1414.2504, batch_loss: 6.6406, loss: 7.2544 ||:  64%|######4   | 331/516 [27:24<17:02,  5.52s/it]
2025-02-04 01:18:50,028 - INFO - tqdm - perplexity: 1408.0077, batch_loss: 6.6938, loss: 7.2499 ||:  65%|######4   | 334/516 [27:35<13:14,  4.37s/it]
2025-02-04 01:19:00,710 - INFO - tqdm - perplexity: 1402.6967, batch_loss: 6.6646, loss: 7.2462 ||:  65%|######5   | 336/516 [27:46<14:49,  4.94s/it]
2025-02-04 01:19:11,647 - INFO - tqdm - perplexity: 1397.2627, batch_loss: 6.4178, loss: 7.2423 ||:  66%|######5   | 338/516 [27:57<15:16,  5.15s/it]
2025-02-04 01:19:22,190 - INFO - tqdm - perplexity: 1392.8751, batch_loss: 6.5479, loss: 7.2391 ||:  66%|######5   | 340/516 [28:07<15:18,  5.22s/it]
2025-02-04 01:19:33,681 - INFO - tqdm - perplexity: 1387.6121, batch_loss: 6.5703, loss: 7.2353 ||:  66%|######6   | 342/516 [28:19<15:57,  5.50s/it]
2025-02-04 01:19:47,583 - INFO - tqdm - perplexity: 1379.8118, batch_loss: 6.5320, loss: 7.2297 ||:  67%|######6   | 345/516 [28:33<13:58,  4.91s/it]
2025-02-04 01:20:00,150 - INFO - tqdm - perplexity: 1371.5390, batch_loss: 6.6154, loss: 7.2237 ||:  67%|######7   | 348/516 [28:45<12:34,  4.49s/it]
2025-02-04 01:20:13,597 - INFO - tqdm - perplexity: 1364.4708, batch_loss: 6.6316, loss: 7.2185 ||:  68%|######8   | 351/516 [28:59<12:24,  4.51s/it]
2025-02-04 01:20:28,893 - INFO - tqdm - perplexity: 1360.5789, batch_loss: 6.5707, loss: 7.2157 ||:  69%|######8   | 354/516 [29:14<13:39,  5.06s/it]
2025-02-04 01:20:39,686 - INFO - tqdm - perplexity: 1355.0908, batch_loss: 6.4696, loss: 7.2116 ||:  69%|######8   | 356/516 [29:25<14:03,  5.27s/it]
2025-02-04 01:20:53,411 - INFO - tqdm - perplexity: 1347.6944, batch_loss: 6.6368, loss: 7.2062 ||:  70%|######9   | 359/516 [29:39<12:38,  4.83s/it]
2025-02-04 01:21:07,793 - INFO - tqdm - perplexity: 1338.4722, batch_loss: 6.1426, loss: 7.1993 ||:  70%|#######   | 362/516 [29:53<12:28,  4.86s/it]
2025-02-04 01:21:17,806 - INFO - tqdm - perplexity: 1333.1741, batch_loss: 6.4559, loss: 7.1953 ||:  71%|#######   | 364/516 [30:03<12:38,  4.99s/it]
2025-02-04 01:21:28,761 - INFO - tqdm - perplexity: 1331.9024, batch_loss: 6.8470, loss: 7.1944 ||:  71%|#######   | 365/516 [30:14<17:03,  6.78s/it]
2025-02-04 01:21:38,791 - INFO - tqdm - perplexity: 1326.5959, batch_loss: 6.4702, loss: 7.1904 ||:  71%|#######1  | 367/516 [30:24<14:33,  5.86s/it]
2025-02-04 01:21:49,190 - INFO - tqdm - perplexity: 1321.7650, batch_loss: 6.5003, loss: 7.1867 ||:  72%|#######1  | 369/516 [30:34<13:38,  5.57s/it]
2025-02-04 01:21:59,841 - INFO - tqdm - perplexity: 1316.9634, batch_loss: 6.4776, loss: 7.1831 ||:  72%|#######1  | 371/516 [30:45<13:08,  5.44s/it]
2025-02-04 01:22:12,894 - INFO - tqdm - perplexity: 1308.6335, batch_loss: 6.5590, loss: 7.1767 ||:  72%|#######2  | 374/516 [30:58<11:07,  4.70s/it]
2025-02-04 01:22:23,490 - INFO - tqdm - perplexity: 1303.0163, batch_loss: 6.2933, loss: 7.1724 ||:  73%|#######2  | 376/516 [31:09<11:29,  4.92s/it]
2025-02-04 01:22:35,942 - INFO - tqdm - perplexity: 1298.9507, batch_loss: 6.4192, loss: 7.1693 ||:  73%|#######3  | 378/516 [31:21<12:50,  5.58s/it]
2025-02-04 01:22:47,136 - INFO - tqdm - perplexity: 1292.8509, batch_loss: 6.2117, loss: 7.1646 ||:  74%|#######3  | 380/516 [31:32<12:39,  5.58s/it]
2025-02-04 01:23:01,233 - INFO - tqdm - perplexity: 1285.0372, batch_loss: 6.4796, loss: 7.1585 ||:  74%|#######4  | 383/516 [31:46<10:58,  4.95s/it]
2025-02-04 01:23:15,371 - INFO - tqdm - perplexity: 1276.8363, batch_loss: 6.4124, loss: 7.1521 ||:  75%|#######4  | 386/516 [32:01<10:23,  4.79s/it]
2025-02-04 01:23:25,507 - INFO - tqdm - perplexity: 1270.7297, batch_loss: 6.2702, loss: 7.1473 ||:  75%|#######5  | 388/516 [32:11<10:26,  4.90s/it]
2025-02-04 01:23:36,764 - INFO - tqdm - perplexity: 1266.2560, batch_loss: 6.5213, loss: 7.1438 ||:  76%|#######5  | 390/516 [32:22<10:59,  5.23s/it]
2025-02-04 01:23:49,166 - INFO - tqdm - perplexity: 1258.1622, batch_loss: 6.3180, loss: 7.1374 ||:  76%|#######6  | 393/516 [32:34<09:12,  4.49s/it]
2025-02-04 01:23:59,906 - INFO - tqdm - perplexity: 1253.4315, batch_loss: 6.3667, loss: 7.1336 ||:  77%|#######6  | 395/516 [32:45<10:04,  4.99s/it]
2025-02-04 01:24:10,578 - INFO - tqdm - perplexity: 1248.8644, batch_loss: 6.3662, loss: 7.1300 ||:  77%|#######6  | 397/516 [32:56<10:00,  5.05s/it]
2025-02-04 01:24:22,589 - INFO - tqdm - perplexity: 1244.5168, batch_loss: 6.5292, loss: 7.1265 ||:  77%|#######7  | 399/516 [33:08<10:49,  5.55s/it]
2025-02-04 01:24:38,906 - INFO - tqdm - perplexity: 1237.6169, batch_loss: 6.3654, loss: 7.1209 ||:  78%|#######7  | 402/516 [33:24<10:49,  5.70s/it]
2025-02-04 01:24:52,231 - INFO - tqdm - perplexity: 1229.5214, batch_loss: 6.1773, loss: 7.1144 ||:  78%|#######8  | 405/516 [33:37<09:18,  5.03s/it]
2025-02-04 01:25:06,515 - INFO - tqdm - perplexity: 1223.1634, batch_loss: 6.2219, loss: 7.1092 ||:  79%|#######9  | 408/516 [33:52<08:32,  4.75s/it]
2025-02-04 01:25:21,293 - INFO - tqdm - perplexity: 1217.0727, batch_loss: 6.7177, loss: 7.1042 ||:  80%|#######9  | 411/516 [34:06<08:40,  4.95s/it]
2025-02-04 01:25:34,530 - INFO - tqdm - perplexity: 1209.2567, batch_loss: 6.0724, loss: 7.0978 ||:  80%|########  | 414/516 [34:20<07:44,  4.55s/it]
2025-02-04 01:25:48,282 - INFO - tqdm - perplexity: 1200.5983, batch_loss: 6.1475, loss: 7.0906 ||:  81%|########  | 417/516 [34:33<07:37,  4.62s/it]
2025-02-04 01:26:03,394 - INFO - tqdm - perplexity: 1192.9236, batch_loss: 6.1564, loss: 7.0842 ||:  81%|########1 | 420/516 [34:49<07:56,  4.96s/it]
2025-02-04 01:26:18,412 - INFO - tqdm - perplexity: 1184.7044, batch_loss: 6.1200, loss: 7.0772 ||:  82%|########1 | 423/516 [35:04<08:00,  5.16s/it]
2025-02-04 01:26:33,459 - INFO - tqdm - perplexity: 1176.8877, batch_loss: 5.9235, loss: 7.0706 ||:  83%|########2 | 426/516 [35:19<07:42,  5.14s/it]
2025-02-04 01:26:44,508 - INFO - tqdm - perplexity: 1171.6009, batch_loss: 6.1039, loss: 7.0661 ||:  83%|########2 | 428/516 [35:30<07:51,  5.35s/it]
2025-02-04 01:26:57,491 - INFO - tqdm - perplexity: 1163.4194, batch_loss: 6.2004, loss: 7.0591 ||:  84%|########3 | 431/516 [35:43<06:30,  4.60s/it]
2025-02-04 01:27:11,946 - INFO - tqdm - perplexity: 1155.0441, batch_loss: 5.9817, loss: 7.0519 ||:  84%|########4 | 434/516 [35:57<06:35,  4.83s/it]
2025-02-04 01:27:26,014 - INFO - tqdm - perplexity: 1146.7479, batch_loss: 6.1231, loss: 7.0447 ||:  85%|########4 | 437/516 [36:11<06:13,  4.73s/it]
2025-02-04 01:27:37,275 - INFO - tqdm - perplexity: 1142.5359, batch_loss: 6.1815, loss: 7.0410 ||:  85%|########5 | 439/516 [36:22<06:35,  5.13s/it]
2025-02-04 01:27:51,009 - INFO - tqdm - perplexity: 1135.0991, batch_loss: 5.9657, loss: 7.0345 ||:  86%|########5 | 442/516 [36:36<05:46,  4.69s/it]
2025-02-04 01:28:05,437 - INFO - tqdm - perplexity: 1127.9900, batch_loss: 5.7939, loss: 7.0282 ||:  86%|########6 | 445/516 [36:51<05:35,  4.73s/it]
2025-02-04 01:28:19,848 - INFO - tqdm - perplexity: 1119.3288, batch_loss: 5.9146, loss: 7.0205 ||:  87%|########6 | 448/516 [37:05<05:26,  4.80s/it]
2025-02-04 01:28:33,276 - INFO - tqdm - perplexity: 1111.4230, batch_loss: 5.9039, loss: 7.0134 ||:  87%|########7 | 451/516 [37:18<05:01,  4.64s/it]
2025-02-04 01:28:43,642 - INFO - tqdm - perplexity: 1105.8882, batch_loss: 5.9486, loss: 7.0084 ||:  88%|########7 | 453/516 [37:29<05:08,  4.89s/it]
2025-02-04 01:28:59,052 - INFO - tqdm - perplexity: 1098.9544, batch_loss: 6.1034, loss: 7.0021 ||:  88%|########8 | 456/516 [37:44<05:07,  5.13s/it]
2025-02-04 01:29:12,550 - INFO - tqdm - perplexity: 1091.9455, batch_loss: 6.0057, loss: 6.9957 ||:  89%|########8 | 459/516 [37:58<04:27,  4.70s/it]
2025-02-04 01:29:26,698 - INFO - tqdm - perplexity: 1084.9965, batch_loss: 6.0352, loss: 6.9893 ||:  90%|########9 | 462/516 [38:12<04:15,  4.73s/it]
2025-02-04 01:29:37,165 - INFO - tqdm - perplexity: 1079.4744, batch_loss: 5.8043, loss: 6.9842 ||:  90%|########9 | 464/516 [38:22<04:23,  5.06s/it]
2025-02-04 01:29:49,473 - INFO - tqdm - perplexity: 1073.9993, batch_loss: 5.7054, loss: 6.9791 ||:  90%|######### | 466/516 [38:35<04:40,  5.61s/it]
2025-02-04 01:30:04,121 - INFO - tqdm - perplexity: 1066.7939, batch_loss: 6.0511, loss: 6.9724 ||:  91%|######### | 469/516 [38:49<04:00,  5.13s/it]
2025-02-04 01:30:17,387 - INFO - tqdm - perplexity: 1059.1690, batch_loss: 5.7856, loss: 6.9652 ||:  91%|#########1| 472/516 [39:03<03:23,  4.63s/it]
2025-02-04 01:30:30,084 - INFO - tqdm - perplexity: 1054.0914, batch_loss: 6.0418, loss: 6.9604 ||:  92%|#########2| 475/516 [39:15<03:02,  4.45s/it]
2025-02-04 01:30:43,337 - INFO - tqdm - perplexity: 1047.2326, batch_loss: 5.5743, loss: 6.9539 ||:  93%|#########2| 478/516 [39:28<02:49,  4.46s/it]
2025-02-04 01:30:53,608 - INFO - tqdm - perplexity: 1042.8514, batch_loss: 6.0361, loss: 6.9497 ||:  93%|#########3| 480/516 [39:39<02:56,  4.90s/it]
2025-02-04 01:31:07,169 - INFO - tqdm - perplexity: 1035.3948, batch_loss: 5.6916, loss: 6.9425 ||:  94%|#########3| 483/516 [39:52<02:36,  4.73s/it]
2025-02-04 01:31:22,145 - INFO - tqdm - perplexity: 1028.3598, batch_loss: 5.5356, loss: 6.9357 ||:  94%|#########4| 486/516 [40:07<02:30,  5.02s/it]
2025-02-04 01:31:35,790 - INFO - tqdm - perplexity: 1021.9916, batch_loss: 5.8530, loss: 6.9295 ||:  95%|#########4| 489/516 [40:21<02:08,  4.77s/it]
2025-02-04 01:31:47,220 - INFO - tqdm - perplexity: 1018.2044, batch_loss: 5.9589, loss: 6.9258 ||:  95%|#########5| 491/516 [40:32<02:10,  5.24s/it]
2025-02-04 01:31:57,387 - INFO - tqdm - perplexity: 1013.4960, batch_loss: 5.6759, loss: 6.9212 ||:  96%|#########5| 493/516 [40:43<01:58,  5.17s/it]
2025-02-04 01:32:09,552 - INFO - tqdm - perplexity: 1007.3757, batch_loss: 5.6300, loss: 6.9151 ||:  96%|#########6| 496/516 [40:55<01:29,  4.49s/it]
2025-02-04 01:32:20,604 - INFO - tqdm - perplexity: 1002.9204, batch_loss: 5.6673, loss: 6.9107 ||:  97%|#########6| 498/516 [41:06<01:29,  4.95s/it]
2025-02-04 01:32:35,550 - INFO - tqdm - perplexity: 995.7482, batch_loss: 5.8612, loss: 6.9035 ||:  97%|#########7| 501/516 [41:21<01:14,  4.98s/it]
2025-02-04 01:32:48,963 - INFO - tqdm - perplexity: 989.2201, batch_loss: 5.7168, loss: 6.8969 ||:  98%|#########7| 504/516 [41:34<00:57,  4.79s/it]
2025-02-04 01:33:05,686 - INFO - tqdm - perplexity: 982.9572, batch_loss: 5.9350, loss: 6.8906 ||:  98%|#########8| 507/516 [41:51<00:49,  5.46s/it]
2025-02-04 01:33:19,251 - INFO - tqdm - perplexity: 977.1259, batch_loss: 5.7153, loss: 6.8846 ||:  99%|#########8| 510/516 [42:04<00:29,  4.84s/it]
2025-02-04 01:33:29,538 - INFO - tqdm - perplexity: 972.5541, batch_loss: 5.5822, loss: 6.8799 ||:  99%|#########9| 512/516 [42:15<00:20,  5.08s/it]
2025-02-04 01:33:39,657 - INFO - tqdm - perplexity: 968.3177, batch_loss: 5.6905, loss: 6.8756 ||: 100%|#########9| 514/516 [42:25<00:10,  5.10s/it]
2025-02-04 01:33:44,651 - INFO - tqdm - perplexity: 966.4242, batch_loss: 5.8675, loss: 6.8736 ||: 100%|#########9| 515/516 [42:30<00:05,  5.07s/it]
2025-02-04 01:33:45,330 - INFO - tqdm - perplexity: 964.0094, batch_loss: 5.5827, loss: 6.8711 ||: 100%|##########| 516/516 [42:30<00:00,  3.75s/it]
2025-02-04 01:33:45,330 - INFO - tqdm - perplexity: 964.0094, batch_loss: 5.5827, loss: 6.8711 ||: 100%|##########| 516/516 [42:30<00:00,  4.94s/it]
2025-02-04 01:33:45,331 - INFO - allennlp.training.gradient_descent_trainer - Validating
2025-02-04 01:33:45,331 - INFO - tqdm - 0%|          | 0/129 [00:00<?, ?it/s]
2025-02-04 01:33:46,520 - INFO - allennlp.training.callbacks.console_logger - Batch inputs
2025-02-04 01:33:46,520 - INFO - allennlp.training.callbacks.console_logger - batch_input/source/tokens/tokens (Shape: 32 x 56)
tensor([[    4,   350,    52,  ...,     0,     0,     0],
        [   72, 48319,     4,  ...,     0,     0,     0],
        [   11,   123, 47442,  ...,     0,     0,     0],
        ...,
        [    3, 46146, 46147,  ...,     0,     0,     0],
        [  188,   975,     4,  ...,     0,     0,     0],
        [  645,     2,  1858,  ...,     0,     0,     0]])
2025-02-04 01:33:55,496 - INFO - tqdm - perplexity: 302.8859, batch_loss: 5.7892, loss: 5.7134 ||:   6%|6         | 8/129 [00:10<02:30,  1.25s/it]
2025-02-04 01:34:06,582 - INFO - tqdm - perplexity: 309.5492, batch_loss: 5.9997, loss: 5.7351 ||:  12%|#2        | 16/129 [00:21<02:43,  1.45s/it]
2025-02-04 01:34:17,649 - INFO - tqdm - perplexity: 307.5767, batch_loss: 5.9881, loss: 5.7287 ||:  19%|#9        | 25/129 [00:32<02:10,  1.25s/it]
2025-02-04 01:34:27,833 - INFO - tqdm - perplexity: 311.3233, batch_loss: 5.7873, loss: 5.7408 ||:  26%|##5       | 33/129 [00:42<02:01,  1.27s/it]
2025-02-04 01:34:38,294 - INFO - tqdm - perplexity: 305.5632, batch_loss: 5.4745, loss: 5.7222 ||:  32%|###1      | 41/129 [00:52<01:51,  1.27s/it]
2025-02-04 01:34:48,549 - INFO - tqdm - perplexity: 301.2304, batch_loss: 5.4867, loss: 5.7079 ||:  38%|###7      | 49/129 [01:03<01:42,  1.28s/it]
2025-02-04 01:34:58,846 - INFO - tqdm - perplexity: 300.0767, batch_loss: 5.7307, loss: 5.7040 ||:  44%|####4     | 57/129 [01:13<01:32,  1.28s/it]
2025-02-04 01:35:10,077 - INFO - tqdm - perplexity: 299.5495, batch_loss: 5.6366, loss: 5.7023 ||:  51%|#####1    | 66/129 [01:24<01:17,  1.23s/it]
2025-02-04 01:35:20,392 - INFO - tqdm - perplexity: 302.8131, batch_loss: 5.5787, loss: 5.7131 ||:  57%|#####7    | 74/129 [01:35<01:08,  1.25s/it]
2025-02-04 01:35:31,373 - INFO - tqdm - perplexity: 303.7993, batch_loss: 5.7024, loss: 5.7164 ||:  64%|######4   | 83/129 [01:46<00:57,  1.25s/it]
2025-02-04 01:35:42,389 - INFO - tqdm - perplexity: 302.8212, batch_loss: 5.6545, loss: 5.7131 ||:  71%|#######1  | 92/129 [01:57<00:44,  1.21s/it]
2025-02-04 01:35:52,963 - INFO - tqdm - perplexity: 303.7674, batch_loss: 5.8209, loss: 5.7163 ||:  78%|#######7  | 100/129 [02:07<00:38,  1.33s/it]
2025-02-04 01:36:03,197 - INFO - tqdm - perplexity: 303.9797, batch_loss: 5.9032, loss: 5.7170 ||:  84%|########3 | 108/129 [02:17<00:26,  1.27s/it]
2025-02-04 01:36:13,643 - INFO - tqdm - perplexity: 302.9645, batch_loss: 5.6585, loss: 5.7136 ||:  90%|########9 | 116/129 [02:28<00:16,  1.31s/it]
2025-02-04 01:36:23,712 - INFO - tqdm - perplexity: 301.7883, batch_loss: 5.4807, loss: 5.7097 ||:  96%|#########6| 124/129 [02:38<00:06,  1.27s/it]
2025-02-04 01:36:29,426 - INFO - tqdm - perplexity: 302.1540, batch_loss: 5.5678, loss: 5.7109 ||: 100%|##########| 129/129 [02:44<00:00,  1.19s/it]
2025-02-04 01:36:29,426 - INFO - tqdm - perplexity: 302.1540, batch_loss: 5.5678, loss: 5.7109 ||: 100%|##########| 129/129 [02:44<00:00,  1.27s/it]
2025-02-04 01:36:29,428 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2025-02-04 01:36:29,428 - INFO - allennlp.training.callbacks.console_logger - loss               |     6.871  |     5.711
2025-02-04 01:36:29,428 - INFO - allennlp.training.callbacks.console_logger - perplexity         |   964.009  |   302.154
2025-02-04 01:36:29,429 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |     0.000  |       N/A
2025-02-04 01:36:29,999 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:45:15.651917
2025-02-04 01:36:30,000 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 23:22:47
2025-02-04 01:36:30,000 - INFO - allennlp.training.gradient_descent_trainer - Epoch 1/31
2025-02-04 01:36:30,000 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 0B
2025-02-04 01:36:30,001 - INFO - allennlp.training.gradient_descent_trainer - Training
2025-02-04 01:36:30,001 - INFO - tqdm - 0%|          | 0/516 [00:00<?, ?it/s]
2025-02-04 01:36:40,632 - INFO - tqdm - perplexity: 231.9137, batch_loss: 5.5977, loss: 5.4464 ||:   0%|          | 2/516 [00:10<43:59,  5.14s/it]
2025-02-04 01:36:54,908 - INFO - tqdm - perplexity: 261.0041, batch_loss: 5.5861, loss: 5.5645 ||:   1%|          | 5/516 [00:24<42:40,  5.01s/it]
2025-02-04 01:37:06,245 - INFO - tqdm - perplexity: 258.3023, batch_loss: 5.5598, loss: 5.5541 ||:   1%|1         | 7/516 [00:36<45:29,  5.36s/it]
2025-02-04 01:37:19,921 - INFO - tqdm - perplexity: 260.8221, batch_loss: 5.4393, loss: 5.5638 ||:   2%|1         | 10/516 [00:49<39:35,  4.69s/it]
2025-02-04 01:37:34,845 - INFO - tqdm - perplexity: 265.1662, batch_loss: 5.8527, loss: 5.5804 ||:   3%|2         | 13/516 [01:04<42:23,  5.06s/it]
2025-02-04 01:37:45,584 - INFO - tqdm - perplexity: 265.5777, batch_loss: 5.6858, loss: 5.5819 ||:   3%|2         | 15/516 [01:15<43:10,  5.17s/it]
2025-02-04 01:37:59,349 - INFO - tqdm - perplexity: 264.8400, batch_loss: 5.5341, loss: 5.5791 ||:   3%|3         | 18/516 [01:29<40:15,  4.85s/it]
2025-02-04 01:38:15,244 - INFO - tqdm - perplexity: 266.6273, batch_loss: 5.6476, loss: 5.5859 ||:   4%|4         | 21/516 [01:45<42:52,  5.20s/it]
2025-02-04 01:38:29,582 - INFO - tqdm - perplexity: 262.4630, batch_loss: 5.3646, loss: 5.5701 ||:   5%|4         | 24/516 [01:59<40:07,  4.89s/it]
2025-02-04 01:38:41,273 - INFO - tqdm - perplexity: 259.8258, batch_loss: 5.5216, loss: 5.5600 ||:   5%|5         | 27/516 [02:11<34:49,  4.27s/it]
2025-02-04 01:38:55,994 - INFO - tqdm - perplexity: 257.4332, batch_loss: 5.6631, loss: 5.5508 ||:   6%|5         | 30/516 [02:25<38:07,  4.71s/it]
2025-02-04 01:39:07,109 - INFO - tqdm - perplexity: 256.0472, batch_loss: 5.6096, loss: 5.5454 ||:   6%|6         | 32/516 [02:37<40:29,  5.02s/it]
2025-02-04 01:39:17,667 - INFO - tqdm - perplexity: 254.9112, batch_loss: 5.4313, loss: 5.5409 ||:   7%|6         | 34/516 [02:47<41:27,  5.16s/it]
2025-02-04 01:39:31,388 - INFO - tqdm - perplexity: 253.7025, batch_loss: 5.6012, loss: 5.5362 ||:   7%|7         | 37/516 [03:01<37:32,  4.70s/it]
2025-02-04 01:39:42,120 - INFO - tqdm - perplexity: 255.3628, batch_loss: 5.8567, loss: 5.5427 ||:   8%|7         | 39/516 [03:12<40:27,  5.09s/it]
2025-02-04 01:39:54,456 - INFO - tqdm - perplexity: 255.5808, batch_loss: 5.5717, loss: 5.5435 ||:   8%|7         | 41/516 [03:24<45:38,  5.76s/it]
2025-02-04 01:40:06,464 - INFO - tqdm - perplexity: 252.3754, batch_loss: 5.1449, loss: 5.5309 ||:   8%|8         | 43/516 [03:36<46:03,  5.84s/it]
2025-02-04 01:40:21,722 - INFO - tqdm - perplexity: 251.9654, batch_loss: 5.4481, loss: 5.5293 ||:   9%|8         | 46/516 [03:51<43:04,  5.50s/it]
2025-02-04 01:40:35,628 - INFO - tqdm - perplexity: 251.7432, batch_loss: 5.6492, loss: 5.5284 ||:   9%|9         | 49/516 [04:05<38:19,  4.92s/it]
2025-02-04 01:40:47,296 - INFO - tqdm - perplexity: 251.0278, batch_loss: 5.3953, loss: 5.5256 ||:  10%|#         | 52/516 [04:17<32:54,  4.25s/it]
2025-02-04 01:40:58,291 - INFO - tqdm - perplexity: 248.7632, batch_loss: 5.2421, loss: 5.5165 ||:  10%|#         | 54/516 [04:28<37:29,  4.87s/it]
2025-02-04 01:41:12,034 - INFO - tqdm - perplexity: 247.8142, batch_loss: 5.3246, loss: 5.5127 ||:  11%|#1        | 57/516 [04:42<36:00,  4.71s/it]
2025-02-04 01:41:24,358 - INFO - tqdm - perplexity: 245.3541, batch_loss: 5.3861, loss: 5.5027 ||:  12%|#1        | 60/516 [04:54<32:25,  4.27s/it]
2025-02-04 01:41:35,241 - INFO - tqdm - perplexity: 243.3163, batch_loss: 5.2959, loss: 5.4944 ||:  12%|#2        | 62/516 [05:05<36:07,  4.77s/it]
2025-02-04 01:41:46,668 - INFO - tqdm - perplexity: 242.1237, batch_loss: 5.2750, loss: 5.4894 ||:  12%|#2        | 64/516 [05:16<40:44,  5.41s/it]
2025-02-04 01:41:57,826 - INFO - tqdm - perplexity: 241.1239, batch_loss: 5.4615, loss: 5.4853 ||:  13%|#2        | 66/516 [05:27<42:10,  5.62s/it]
2025-02-04 01:42:09,351 - INFO - tqdm - perplexity: 239.6599, batch_loss: 5.1952, loss: 5.4792 ||:  13%|#3        | 68/516 [05:39<42:27,  5.69s/it]
2025-02-04 01:42:21,741 - INFO - tqdm - perplexity: 237.7044, batch_loss: 5.2087, loss: 5.4710 ||:  14%|#3        | 71/516 [05:51<34:47,  4.69s/it]
2025-02-04 01:42:35,812 - INFO - tqdm - perplexity: 236.0699, batch_loss: 5.5218, loss: 5.4641 ||:  14%|#4        | 74/516 [06:05<34:54,  4.74s/it]
2025-02-04 01:42:48,571 - INFO - tqdm - perplexity: 234.4560, batch_loss: 5.3568, loss: 5.4573 ||:  15%|#4        | 77/516 [06:18<32:23,  4.43s/it]
2025-02-04 01:43:01,246 - INFO - tqdm - perplexity: 233.3920, batch_loss: 5.3567, loss: 5.4527 ||:  16%|#5        | 80/516 [06:31<31:52,  4.39s/it]
2025-02-04 01:43:11,513 - INFO - tqdm - perplexity: 232.5520, batch_loss: 5.3988, loss: 5.4491 ||:  16%|#5        | 82/516 [06:41<33:48,  4.67s/it]
2025-02-04 01:43:22,031 - INFO - tqdm - perplexity: 231.6764, batch_loss: 5.1617, loss: 5.4453 ||:  16%|#6        | 84/516 [06:52<35:52,  4.98s/it]
2025-02-04 01:43:36,690 - INFO - tqdm - perplexity: 230.4962, batch_loss: 5.4120, loss: 5.4402 ||:  17%|#6        | 87/516 [07:06<35:06,  4.91s/it]
2025-02-04 01:43:46,813 - INFO - tqdm - perplexity: 229.6086, batch_loss: 5.1923, loss: 5.4364 ||:  17%|#7        | 89/516 [07:16<35:37,  5.01s/it]
2025-02-04 01:44:00,287 - INFO - tqdm - perplexity: 228.6806, batch_loss: 5.4544, loss: 5.4323 ||:  18%|#7        | 92/516 [07:30<32:26,  4.59s/it]
2025-02-04 01:44:13,336 - INFO - tqdm - perplexity: 227.0186, batch_loss: 5.0876, loss: 5.4250 ||:  18%|#8        | 95/516 [07:43<31:38,  4.51s/it]
2025-02-04 01:44:27,892 - INFO - tqdm - perplexity: 226.0854, batch_loss: 5.2558, loss: 5.4209 ||:  19%|#8        | 98/516 [07:57<33:17,  4.78s/it]
2025-02-04 01:44:38,138 - INFO - tqdm - perplexity: 224.9276, batch_loss: 5.1509, loss: 5.4158 ||:  19%|#9        | 100/516 [08:08<34:16,  4.94s/it]
2025-02-04 01:44:50,818 - INFO - tqdm - perplexity: 224.0652, batch_loss: 5.3021, loss: 5.4119 ||:  20%|#9        | 103/516 [08:20<30:39,  4.45s/it]
2025-02-04 01:45:03,919 - INFO - tqdm - perplexity: 222.8970, batch_loss: 5.2449, loss: 5.4067 ||:  21%|##        | 106/516 [08:33<29:59,  4.39s/it]
2025-02-04 01:45:13,941 - INFO - tqdm - perplexity: 222.3501, batch_loss: 5.3488, loss: 5.4043 ||:  21%|##        | 108/516 [08:43<31:56,  4.70s/it]
2025-02-04 01:45:29,565 - INFO - tqdm - perplexity: 220.3429, batch_loss: 4.9526, loss: 5.3952 ||:  22%|##1       | 111/516 [08:59<34:10,  5.06s/it]
2025-02-04 01:45:40,552 - INFO - tqdm - perplexity: 219.0328, batch_loss: 5.0099, loss: 5.3892 ||:  22%|##1       | 113/516 [09:10<36:17,  5.40s/it]
2025-02-04 01:45:54,510 - INFO - tqdm - perplexity: 217.4450, batch_loss: 5.0099, loss: 5.3819 ||:  22%|##2       | 116/516 [09:24<32:43,  4.91s/it]
2025-02-04 01:46:04,656 - INFO - tqdm - perplexity: 216.2129, batch_loss: 5.1616, loss: 5.3763 ||:  23%|##2       | 118/516 [09:34<33:21,  5.03s/it]
2025-02-04 01:46:15,788 - INFO - tqdm - perplexity: 215.5803, batch_loss: 5.0741, loss: 5.3733 ||:  23%|##3       | 120/516 [09:45<34:21,  5.21s/it]
2025-02-04 01:46:26,598 - INFO - tqdm - perplexity: 215.4622, batch_loss: 5.5376, loss: 5.3728 ||:  24%|##3       | 122/516 [09:56<35:01,  5.33s/it]
2025-02-04 01:46:36,914 - INFO - tqdm - perplexity: 214.2022, batch_loss: 4.9708, loss: 5.3669 ||:  24%|##4       | 124/516 [10:06<34:31,  5.28s/it]
2025-02-04 01:46:49,726 - INFO - tqdm - perplexity: 213.4476, batch_loss: 4.9869, loss: 5.3634 ||:  24%|##4       | 126/516 [10:19<37:46,  5.81s/it]
2025-02-04 01:47:00,746 - INFO - tqdm - perplexity: 212.6703, batch_loss: 5.1214, loss: 5.3597 ||:  25%|##4       | 128/516 [10:30<36:19,  5.62s/it]
2025-02-04 01:47:13,019 - INFO - tqdm - perplexity: 212.2057, batch_loss: 5.0812, loss: 5.3576 ||:  25%|##5       | 130/516 [10:43<37:42,  5.86s/it]
2025-02-04 01:47:26,977 - INFO - tqdm - perplexity: 211.4135, batch_loss: 5.3086, loss: 5.3538 ||:  26%|##5       | 133/516 [10:56<32:15,  5.05s/it]
2025-02-04 01:47:41,774 - INFO - tqdm - perplexity: 210.8506, batch_loss: 5.0672, loss: 5.3511 ||:  26%|##6       | 136/516 [11:11<31:36,  4.99s/it]
2025-02-04 01:47:55,998 - INFO - tqdm - perplexity: 209.4769, batch_loss: 4.9748, loss: 5.3446 ||:  27%|##6       | 139/516 [11:25<31:08,  4.96s/it]
2025-02-04 01:48:09,947 - INFO - tqdm - perplexity: 208.7090, batch_loss: 5.1947, loss: 5.3409 ||:  28%|##7       | 142/516 [11:39<29:01,  4.66s/it]
2025-02-04 01:48:24,324 - INFO - tqdm - perplexity: 207.6719, batch_loss: 5.3212, loss: 5.3360 ||:  28%|##8       | 145/516 [11:54<29:20,  4.75s/it]
2025-02-04 01:48:37,274 - INFO - tqdm - perplexity: 207.1891, batch_loss: 5.1212, loss: 5.3336 ||:  28%|##8       | 147/516 [12:07<34:27,  5.60s/it]
2025-02-04 01:48:49,871 - INFO - tqdm - perplexity: 206.2057, batch_loss: 5.0816, loss: 5.3289 ||:  29%|##9       | 150/516 [12:19<28:14,  4.63s/it]
2025-02-04 01:49:03,064 - INFO - tqdm - perplexity: 205.0573, batch_loss: 4.9431, loss: 5.3233 ||:  30%|##9       | 153/516 [12:33<27:19,  4.52s/it]
2025-02-04 01:49:16,813 - INFO - tqdm - perplexity: 203.7714, batch_loss: 4.9754, loss: 5.3170 ||:  30%|###       | 156/516 [12:46<27:39,  4.61s/it]
2025-02-04 01:49:30,341 - INFO - tqdm - perplexity: 203.0793, batch_loss: 5.2276, loss: 5.3136 ||:  31%|###       | 159/516 [13:00<26:55,  4.52s/it]
2025-02-04 01:49:44,092 - INFO - tqdm - perplexity: 201.9803, batch_loss: 5.0245, loss: 5.3082 ||:  31%|###1      | 162/516 [13:14<27:20,  4.64s/it]
2025-02-04 01:49:58,057 - INFO - tqdm - perplexity: 200.8631, batch_loss: 4.8996, loss: 5.3026 ||:  32%|###1      | 165/516 [13:28<27:32,  4.71s/it]
2025-02-04 01:50:08,777 - INFO - tqdm - perplexity: 200.3061, batch_loss: 4.9874, loss: 5.2998 ||:  32%|###2      | 167/516 [13:38<29:44,  5.11s/it]
2025-02-04 01:50:19,517 - INFO - tqdm - perplexity: 200.1292, batch_loss: 5.2733, loss: 5.2990 ||:  33%|###2      | 169/516 [13:49<30:38,  5.30s/it]
2025-02-04 01:50:31,146 - INFO - tqdm - perplexity: 199.3455, batch_loss: 4.7418, loss: 5.2950 ||:  33%|###3      | 171/516 [14:01<32:12,  5.60s/it]
2025-02-04 01:50:42,775 - INFO - tqdm - perplexity: 199.3742, batch_loss: 5.2695, loss: 5.2952 ||:  34%|###3      | 173/516 [14:12<33:12,  5.81s/it]
2025-02-04 01:50:52,998 - INFO - tqdm - perplexity: 198.3955, batch_loss: 4.8585, loss: 5.2903 ||:  34%|###3      | 175/516 [14:22<30:37,  5.39s/it]
2025-02-04 01:51:06,642 - INFO - tqdm - perplexity: 197.1531, batch_loss: 4.9966, loss: 5.2840 ||:  34%|###4      | 178/516 [14:36<26:38,  4.73s/it]
2025-02-04 01:51:17,297 - INFO - tqdm - perplexity: 196.6377, batch_loss: 4.8807, loss: 5.2814 ||:  35%|###4      | 180/516 [14:47<28:48,  5.14s/it]
2025-02-04 01:51:27,408 - INFO - tqdm - perplexity: 195.9719, batch_loss: 5.0131, loss: 5.2780 ||:  35%|###5      | 182/516 [14:57<28:22,  5.10s/it]
2025-02-04 01:51:37,555 - INFO - tqdm - perplexity: 195.4688, batch_loss: 5.2477, loss: 5.2754 ||:  36%|###5      | 184/516 [15:07<28:24,  5.13s/it]
2025-02-04 01:51:50,769 - INFO - tqdm - perplexity: 194.9480, batch_loss: 5.1990, loss: 5.2727 ||:  36%|###6      | 187/516 [15:20<25:39,  4.68s/it]
2025-02-04 01:52:02,981 - INFO - tqdm - perplexity: 194.0330, batch_loss: 4.7476, loss: 5.2680 ||:  37%|###6      | 189/516 [15:32<29:27,  5.41s/it]
2025-02-04 01:52:17,296 - INFO - tqdm - perplexity: 193.5688, batch_loss: 4.9870, loss: 5.2656 ||:  37%|###7      | 192/516 [15:47<27:14,  5.05s/it]
2025-02-04 01:52:28,731 - INFO - tqdm - perplexity: 192.8674, batch_loss: 4.8021, loss: 5.2620 ||:  38%|###7      | 194/516 [15:58<28:23,  5.29s/it]
2025-02-04 01:52:41,242 - INFO - tqdm - perplexity: 191.2785, batch_loss: 4.8381, loss: 5.2537 ||:  38%|###8      | 197/516 [16:11<24:18,  4.57s/it]
2025-02-04 01:52:51,813 - INFO - tqdm - perplexity: 190.6727, batch_loss: 4.8573, loss: 5.2506 ||:  39%|###8      | 199/516 [16:21<26:10,  4.95s/it]
2025-02-04 01:53:02,327 - INFO - tqdm - perplexity: 189.9350, batch_loss: 4.9334, loss: 5.2467 ||:  39%|###8      | 201/516 [16:32<26:22,  5.02s/it]
2025-02-04 01:53:12,560 - INFO - tqdm - perplexity: 189.1800, batch_loss: 4.8035, loss: 5.2427 ||:  39%|###9      | 203/516 [16:42<26:19,  5.05s/it]
2025-02-04 01:53:27,677 - INFO - tqdm - perplexity: 188.3634, batch_loss: 4.9694, loss: 5.2384 ||:  40%|###9      | 206/516 [16:57<26:38,  5.16s/it]
2025-02-04 01:53:38,961 - INFO - tqdm - perplexity: 187.8513, batch_loss: 4.8905, loss: 5.2357 ||:  40%|####      | 208/516 [17:08<27:37,  5.38s/it]
2025-02-04 01:53:51,650 - INFO - tqdm - perplexity: 186.6152, batch_loss: 4.7330, loss: 5.2290 ||:  41%|####      | 211/516 [17:21<23:25,  4.61s/it]
2025-02-04 01:54:02,139 - INFO - tqdm - perplexity: 185.9184, batch_loss: 4.7734, loss: 5.2253 ||:  41%|####1     | 213/516 [17:32<24:43,  4.90s/it]
2025-02-04 01:54:12,430 - INFO - tqdm - perplexity: 184.9041, batch_loss: 4.6400, loss: 5.2198 ||:  42%|####1     | 215/516 [17:42<25:24,  5.07s/it]
2025-02-04 01:54:27,034 - INFO - tqdm - perplexity: 183.6063, batch_loss: 4.8320, loss: 5.2128 ||:  42%|####2     | 218/516 [17:57<24:48,  5.00s/it]
2025-02-04 01:54:39,842 - INFO - tqdm - perplexity: 182.9789, batch_loss: 5.0337, loss: 5.2094 ||:  43%|####2     | 221/516 [18:09<21:59,  4.47s/it]
2025-02-04 01:54:53,396 - INFO - tqdm - perplexity: 182.1031, batch_loss: 4.7914, loss: 5.2046 ||:  43%|####3     | 224/516 [18:23<21:39,  4.45s/it]
2025-02-04 01:55:07,951 - INFO - tqdm - perplexity: 181.0632, batch_loss: 4.8623, loss: 5.1988 ||:  44%|####3     | 227/516 [18:37<23:23,  4.85s/it]
2025-02-04 01:55:20,774 - INFO - tqdm - perplexity: 180.4316, batch_loss: 4.8217, loss: 5.1954 ||:  44%|####4     | 229/516 [18:50<26:55,  5.63s/it]
2025-02-04 01:55:34,431 - INFO - tqdm - perplexity: 179.3389, batch_loss: 4.5076, loss: 5.1893 ||:  45%|####4     | 232/516 [19:04<23:21,  4.94s/it]
2025-02-04 01:55:47,093 - INFO - tqdm - perplexity: 178.4397, batch_loss: 4.7863, loss: 5.1843 ||:  46%|####5     | 235/516 [19:17<20:20,  4.34s/it]
2025-02-04 01:56:01,846 - INFO - tqdm - perplexity: 177.9025, batch_loss: 5.1085, loss: 5.1812 ||:  46%|####6     | 238/516 [19:31<21:57,  4.74s/it]
2025-02-04 01:56:16,321 - INFO - tqdm - perplexity: 176.9807, batch_loss: 4.6622, loss: 5.1760 ||:  47%|####6     | 241/516 [19:46<21:48,  4.76s/it]
2025-02-04 01:56:31,652 - INFO - tqdm - perplexity: 175.9398, batch_loss: 4.6710, loss: 5.1701 ||:  47%|####7     | 244/516 [20:01<23:25,  5.17s/it]
2025-02-04 01:56:46,336 - INFO - tqdm - perplexity: 175.1459, batch_loss: 4.9060, loss: 5.1656 ||:  48%|####7     | 247/516 [20:16<22:10,  4.95s/it]
2025-02-04 01:57:01,896 - INFO - tqdm - perplexity: 174.2693, batch_loss: 4.7773, loss: 5.1606 ||:  48%|####8     | 250/516 [20:31<23:02,  5.20s/it]
2025-02-04 01:57:14,486 - INFO - tqdm - perplexity: 173.7346, batch_loss: 4.8213, loss: 5.1575 ||:  49%|####8     | 252/516 [20:44<24:42,  5.62s/it]
2025-02-04 01:57:26,568 - INFO - tqdm - perplexity: 173.2742, batch_loss: 4.8162, loss: 5.1549 ||:  49%|####9     | 254/516 [20:56<25:50,  5.92s/it]
2025-02-04 01:57:39,077 - INFO - tqdm - perplexity: 172.7182, batch_loss: 4.8572, loss: 5.1517 ||:  50%|####9     | 256/516 [21:09<26:28,  6.11s/it]
2025-02-04 01:57:52,512 - INFO - tqdm - perplexity: 171.8773, batch_loss: 4.8084, loss: 5.1468 ||:  50%|#####     | 259/516 [21:22<21:28,  5.01s/it]
2025-02-04 01:58:06,295 - INFO - tqdm - perplexity: 171.0642, batch_loss: 4.8157, loss: 5.1420 ||:  51%|#####     | 262/516 [21:36<20:16,  4.79s/it]
2025-02-04 01:58:19,759 - INFO - tqdm - perplexity: 170.1496, batch_loss: 4.7639, loss: 5.1367 ||:  51%|#####1    | 265/516 [21:49<19:00,  4.55s/it]
2025-02-04 01:58:33,077 - INFO - tqdm - perplexity: 169.4168, batch_loss: 4.6589, loss: 5.1324 ||:  52%|#####1    | 267/516 [22:03<23:27,  5.65s/it]
2025-02-04 01:58:43,347 - INFO - tqdm - perplexity: 168.8237, batch_loss: 4.5868, loss: 5.1289 ||:  52%|#####2    | 269/516 [22:13<21:54,  5.32s/it]
2025-02-04 01:58:57,467 - INFO - tqdm - perplexity: 168.0789, batch_loss: 4.6919, loss: 5.1244 ||:  53%|#####2    | 272/516 [22:27<19:46,  4.86s/it]
2025-02-04 01:59:12,267 - INFO - tqdm - perplexity: 167.2231, batch_loss: 4.5491, loss: 5.1193 ||:  53%|#####3    | 275/516 [22:42<19:43,  4.91s/it]
2025-02-04 01:59:25,642 - INFO - tqdm - perplexity: 166.4397, batch_loss: 4.7745, loss: 5.1146 ||:  54%|#####3    | 278/516 [22:55<18:18,  4.61s/it]
2025-02-04 01:59:38,237 - INFO - tqdm - perplexity: 166.0681, batch_loss: 4.8648, loss: 5.1124 ||:  54%|#####4    | 280/516 [23:08<20:55,  5.32s/it]
2025-02-04 01:59:49,773 - INFO - tqdm - perplexity: 165.3253, batch_loss: 4.2608, loss: 5.1079 ||:  55%|#####4    | 282/516 [23:19<21:49,  5.60s/it]
2025-02-04 02:00:02,441 - INFO - tqdm - perplexity: 164.6526, batch_loss: 4.8016, loss: 5.1038 ||:  55%|#####5    | 285/516 [23:32<18:07,  4.71s/it]
2025-02-04 02:00:13,033 - INFO - tqdm - perplexity: 164.2233, batch_loss: 4.7569, loss: 5.1012 ||:  56%|#####5    | 287/516 [23:43<18:38,  4.88s/it]
2025-02-04 02:00:23,522 - INFO - tqdm - perplexity: 163.6748, batch_loss: 4.5522, loss: 5.0979 ||:  56%|#####6    | 289/516 [23:53<19:32,  5.17s/it]
2025-02-04 02:00:36,796 - INFO - tqdm - perplexity: 162.8082, batch_loss: 4.5660, loss: 5.0926 ||:  57%|#####6    | 292/516 [24:06<17:39,  4.73s/it]
2025-02-04 02:00:52,055 - INFO - tqdm - perplexity: 161.9918, batch_loss: 4.5564, loss: 5.0875 ||:  57%|#####7    | 295/516 [24:22<18:28,  5.02s/it]
2025-02-04 02:01:05,817 - INFO - tqdm - perplexity: 161.1802, batch_loss: 4.5187, loss: 5.0825 ||:  58%|#####7    | 298/516 [24:35<17:28,  4.81s/it]
2025-02-04 02:01:20,883 - INFO - tqdm - perplexity: 160.5180, batch_loss: 4.7360, loss: 5.0784 ||:  58%|#####8    | 301/516 [24:50<17:56,  5.01s/it]
2025-02-04 02:01:35,301 - INFO - tqdm - perplexity: 159.9789, batch_loss: 4.4946, loss: 5.0750 ||:  59%|#####8    | 304/516 [25:05<17:46,  5.03s/it]
2025-02-04 02:01:51,142 - INFO - tqdm - perplexity: 159.2155, batch_loss: 4.5515, loss: 5.0703 ||:  59%|#####9    | 307/516 [25:21<18:13,  5.23s/it]
2025-02-04 02:02:02,485 - INFO - tqdm - perplexity: 158.6801, batch_loss: 4.6310, loss: 5.0669 ||:  60%|#####9    | 309/516 [25:32<18:31,  5.37s/it]
2025-02-04 02:02:17,608 - INFO - tqdm - perplexity: 158.0361, batch_loss: 4.7451, loss: 5.0628 ||:  60%|######    | 312/516 [25:47<17:56,  5.28s/it]
2025-02-04 02:02:27,981 - INFO - tqdm - perplexity: 157.5448, batch_loss: 4.6407, loss: 5.0597 ||:  61%|######    | 314/516 [25:57<17:22,  5.16s/it]
2025-02-04 02:02:38,141 - INFO - tqdm - perplexity: 157.0753, batch_loss: 4.6537, loss: 5.0567 ||:  61%|######1   | 316/516 [26:08<16:52,  5.06s/it]
2025-02-04 02:02:48,792 - INFO - tqdm - perplexity: 156.6111, batch_loss: 4.6242, loss: 5.0538 ||:  62%|######1   | 318/516 [26:18<16:52,  5.11s/it]
2025-02-04 02:02:59,876 - INFO - tqdm - perplexity: 156.2055, batch_loss: 4.4902, loss: 5.0512 ||:  62%|######2   | 320/516 [26:29<17:36,  5.39s/it]
2025-02-04 02:03:11,167 - INFO - tqdm - perplexity: 155.5736, batch_loss: 4.4813, loss: 5.0471 ||:  62%|######2   | 322/516 [26:41<17:43,  5.48s/it]
2025-02-04 02:03:27,019 - INFO - tqdm - perplexity: 154.9494, batch_loss: 4.5662, loss: 5.0431 ||:  63%|######2   | 325/516 [26:57<17:38,  5.54s/it]
2025-02-04 02:03:38,344 - INFO - tqdm - perplexity: 154.4755, batch_loss: 4.3398, loss: 5.0400 ||:  63%|######3   | 327/516 [27:08<17:54,  5.68s/it]
2025-02-04 02:03:53,257 - INFO - tqdm - perplexity: 153.6863, batch_loss: 4.4564, loss: 5.0349 ||:  64%|######3   | 330/516 [27:23<16:16,  5.25s/it]
2025-02-04 02:04:06,099 - INFO - tqdm - perplexity: 153.3828, batch_loss: 4.9639, loss: 5.0329 ||:  65%|######4   | 333/516 [27:36<14:23,  4.72s/it]
2025-02-04 02:04:17,295 - INFO - tqdm - perplexity: 152.8216, batch_loss: 4.4080, loss: 5.0293 ||:  65%|######4   | 335/516 [27:47<15:35,  5.17s/it]
2025-02-04 02:04:27,720 - INFO - tqdm - perplexity: 152.3880, batch_loss: 4.5272, loss: 5.0264 ||:  65%|######5   | 337/516 [27:57<15:31,  5.20s/it]
2025-02-04 02:04:41,070 - INFO - tqdm - perplexity: 151.6185, batch_loss: 4.5407, loss: 5.0214 ||:  66%|######5   | 340/516 [28:11<13:49,  4.71s/it]
2025-02-04 02:04:51,631 - INFO - tqdm - perplexity: 151.2484, batch_loss: 4.6339, loss: 5.0189 ||:  66%|######6   | 342/516 [28:21<14:43,  5.08s/it]
2025-02-04 02:05:02,384 - INFO - tqdm - perplexity: 151.0321, batch_loss: 4.9457, loss: 5.0175 ||:  67%|######6   | 344/516 [28:32<14:49,  5.17s/it]
2025-02-04 02:05:14,083 - INFO - tqdm - perplexity: 150.6070, batch_loss: 4.4695, loss: 5.0147 ||:  67%|######7   | 346/516 [28:44<15:49,  5.58s/it]
2025-02-04 02:05:27,323 - INFO - tqdm - perplexity: 149.8513, batch_loss: 4.5188, loss: 5.0096 ||:  68%|######7   | 349/516 [28:57<13:28,  4.84s/it]
2025-02-04 02:05:41,975 - INFO - tqdm - perplexity: 149.1757, batch_loss: 4.5306, loss: 5.0051 ||:  68%|######8   | 352/516 [29:11<13:17,  4.86s/it]
2025-02-04 02:05:59,087 - INFO - tqdm - perplexity: 148.4793, batch_loss: 4.4105, loss: 5.0004 ||:  69%|######8   | 355/516 [29:29<15:06,  5.63s/it]
2025-02-04 02:06:11,390 - INFO - tqdm - perplexity: 147.9137, batch_loss: 4.4778, loss: 4.9966 ||:  69%|######9   | 358/516 [29:41<12:16,  4.66s/it]
2025-02-04 02:06:22,336 - INFO - tqdm - perplexity: 147.6458, batch_loss: 4.8000, loss: 4.9948 ||:  70%|######9   | 360/516 [29:52<13:20,  5.13s/it]
2025-02-04 02:06:35,053 - INFO - tqdm - perplexity: 147.1031, batch_loss: 4.3391, loss: 4.9911 ||:  70%|#######   | 363/516 [30:05<11:49,  4.64s/it]
2025-02-04 02:06:48,385 - INFO - tqdm - perplexity: 146.4622, batch_loss: 4.4749, loss: 4.9868 ||:  71%|#######   | 366/516 [30:18<11:14,  4.50s/it]
2025-02-04 02:07:01,906 - INFO - tqdm - perplexity: 146.0294, batch_loss: 4.6529, loss: 4.9838 ||:  72%|#######1  | 369/516 [30:31<11:09,  4.55s/it]
2025-02-04 02:07:14,709 - INFO - tqdm - perplexity: 145.6414, batch_loss: 4.5348, loss: 4.9811 ||:  72%|#######1  | 371/516 [30:44<13:28,  5.57s/it]
2025-02-04 02:07:27,210 - INFO - tqdm - perplexity: 145.1861, batch_loss: 4.5511, loss: 4.9780 ||:  72%|#######2  | 374/516 [30:57<10:56,  4.63s/it]
2025-02-04 02:07:40,477 - INFO - tqdm - perplexity: 144.5616, batch_loss: 4.3238, loss: 4.9737 ||:  73%|#######3  | 377/516 [31:10<10:28,  4.52s/it]
2025-02-04 02:07:55,114 - INFO - tqdm - perplexity: 143.8544, batch_loss: 4.3416, loss: 4.9688 ||:  74%|#######3  | 380/516 [31:25<10:53,  4.80s/it]
2025-02-04 02:08:10,020 - INFO - tqdm - perplexity: 143.2730, batch_loss: 4.5326, loss: 4.9648 ||:  74%|#######4  | 383/516 [31:40<11:12,  5.06s/it]
2025-02-04 02:08:21,178 - INFO - tqdm - perplexity: 143.1730, batch_loss: 5.0186, loss: 4.9641 ||:  75%|#######4  | 385/516 [31:51<11:46,  5.39s/it]
2025-02-04 02:08:35,535 - INFO - tqdm - perplexity: 142.7365, batch_loss: 4.5003, loss: 4.9610 ||:  75%|#######5  | 388/516 [32:05<10:39,  5.00s/it]
2025-02-04 02:08:49,705 - INFO - tqdm - perplexity: 142.1682, batch_loss: 4.3816, loss: 4.9570 ||:  76%|#######5  | 391/516 [32:19<10:11,  4.89s/it]
2025-02-04 02:09:04,158 - INFO - tqdm - perplexity: 141.4835, batch_loss: 4.4057, loss: 4.9522 ||:  76%|#######6  | 394/516 [32:34<09:52,  4.86s/it]
2025-02-04 02:09:15,243 - INFO - tqdm - perplexity: 141.2128, batch_loss: 4.4372, loss: 4.9503 ||:  77%|#######6  | 396/516 [32:45<10:32,  5.27s/it]
2025-02-04 02:09:27,495 - INFO - tqdm - perplexity: 140.7942, batch_loss: 4.5703, loss: 4.9473 ||:  77%|#######7  | 399/516 [32:57<08:43,  4.47s/it]
2025-02-04 02:09:48,091 - INFO - tqdm - perplexity: 140.2150, batch_loss: 4.3633, loss: 4.9432 ||:  78%|#######7  | 402/516 [33:18<12:36,  6.63s/it]
2025-02-04 02:09:58,283 - INFO - tqdm - perplexity: 139.8830, batch_loss: 4.5591, loss: 4.9408 ||:  78%|#######8  | 404/516 [33:28<10:46,  5.77s/it]
2025-02-04 02:10:11,231 - INFO - tqdm - perplexity: 139.3642, batch_loss: 4.5867, loss: 4.9371 ||:  79%|#######8  | 407/516 [33:41<08:41,  4.78s/it]
2025-02-04 02:10:26,358 - INFO - tqdm - perplexity: 138.7369, batch_loss: 4.4111, loss: 4.9326 ||:  79%|#######9  | 410/516 [33:56<08:54,  5.04s/it]
2025-02-04 02:10:39,295 - INFO - tqdm - perplexity: 138.2450, batch_loss: 4.5013, loss: 4.9290 ||:  80%|########  | 413/516 [34:09<07:53,  4.59s/it]
2025-02-04 02:10:53,006 - INFO - tqdm - perplexity: 137.8200, batch_loss: 4.3607, loss: 4.9259 ||:  81%|########  | 416/516 [34:23<07:35,  4.55s/it]
2025-02-04 02:11:04,787 - INFO - tqdm - perplexity: 137.5732, batch_loss: 4.4170, loss: 4.9242 ||:  81%|########1 | 418/516 [34:34<08:21,  5.12s/it]
2025-02-04 02:11:15,045 - INFO - tqdm - perplexity: 137.1625, batch_loss: 4.2891, loss: 4.9212 ||:  81%|########1 | 420/516 [34:45<08:17,  5.19s/it]
2025-02-04 02:11:25,988 - INFO - tqdm - perplexity: 136.8161, batch_loss: 4.2979, loss: 4.9186 ||:  82%|########1 | 422/516 [34:55<08:20,  5.33s/it]
2025-02-04 02:11:39,559 - INFO - tqdm - perplexity: 136.3429, batch_loss: 4.4018, loss: 4.9152 ||:  82%|########2 | 425/516 [35:09<07:15,  4.79s/it]
2025-02-04 02:11:53,182 - INFO - tqdm - perplexity: 135.7911, batch_loss: 4.2986, loss: 4.9111 ||:  83%|########2 | 428/516 [35:23<06:46,  4.62s/it]
2025-02-04 02:12:03,372 - INFO - tqdm - perplexity: 135.4665, batch_loss: 4.3086, loss: 4.9087 ||:  83%|########3 | 430/516 [35:33<06:49,  4.76s/it]
2025-02-04 02:12:17,066 - INFO - tqdm - perplexity: 134.9806, batch_loss: 4.4290, loss: 4.9051 ||:  84%|########3 | 433/516 [35:47<06:24,  4.63s/it]
2025-02-04 02:12:29,617 - INFO - tqdm - perplexity: 134.4685, batch_loss: 4.2919, loss: 4.9013 ||:  84%|########4 | 436/516 [35:59<05:49,  4.37s/it]
2025-02-04 02:12:44,957 - INFO - tqdm - perplexity: 133.9178, batch_loss: 4.0632, loss: 4.8972 ||:  85%|########5 | 439/516 [36:14<06:27,  5.04s/it]
2025-02-04 02:12:57,130 - INFO - tqdm - perplexity: 133.4346, batch_loss: 4.3682, loss: 4.8936 ||:  86%|########5 | 442/516 [36:27<05:27,  4.43s/it]
2025-02-04 02:13:08,173 - INFO - tqdm - perplexity: 133.2002, batch_loss: 4.6145, loss: 4.8919 ||:  86%|########6 | 444/516 [36:38<05:50,  4.86s/it]
2025-02-04 02:13:18,623 - INFO - tqdm - perplexity: 132.7662, batch_loss: 4.1752, loss: 4.8886 ||:  86%|########6 | 446/516 [36:48<05:55,  5.08s/it]
2025-02-04 02:13:29,256 - INFO - tqdm - perplexity: 132.3199, batch_loss: 4.1142, loss: 4.8852 ||:  87%|########6 | 448/516 [36:59<05:54,  5.21s/it]
2025-02-04 02:13:44,267 - INFO - tqdm - perplexity: 131.7584, batch_loss: 4.0998, loss: 4.8810 ||:  87%|########7 | 451/516 [37:14<05:37,  5.19s/it]
2025-02-04 02:13:58,983 - INFO - tqdm - perplexity: 131.4298, batch_loss: 4.3259, loss: 4.8785 ||:  88%|########7 | 454/516 [37:28<05:12,  5.04s/it]
2025-02-04 02:14:09,737 - INFO - tqdm - perplexity: 131.0241, batch_loss: 4.0197, loss: 4.8754 ||:  88%|########8 | 456/516 [37:39<05:15,  5.26s/it]
2025-02-04 02:14:23,324 - INFO - tqdm - perplexity: 130.5963, batch_loss: 4.5187, loss: 4.8721 ||:  89%|########8 | 459/516 [37:53<04:35,  4.84s/it]
2025-02-04 02:14:37,057 - INFO - tqdm - perplexity: 130.1784, batch_loss: 4.1354, loss: 4.8689 ||:  90%|########9 | 462/516 [38:07<04:13,  4.70s/it]
2025-02-04 02:14:50,200 - INFO - tqdm - perplexity: 129.6851, batch_loss: 4.0958, loss: 4.8651 ||:  90%|######### | 465/516 [38:20<03:47,  4.46s/it]
2025-02-04 02:15:00,981 - INFO - tqdm - perplexity: 129.2455, batch_loss: 4.1390, loss: 4.8617 ||:  91%|######### | 467/516 [38:30<03:56,  4.84s/it]
2025-02-04 02:15:11,616 - INFO - tqdm - perplexity: 128.9140, batch_loss: 4.3104, loss: 4.8591 ||:  91%|######### | 469/516 [38:41<04:02,  5.17s/it]
2025-02-04 02:15:25,642 - INFO - tqdm - perplexity: 128.3728, batch_loss: 4.2063, loss: 4.8549 ||:  91%|#########1| 472/516 [38:55<03:38,  4.97s/it]
2025-02-04 02:15:36,813 - INFO - tqdm - perplexity: 128.1503, batch_loss: 4.2971, loss: 4.8532 ||:  92%|#########1| 474/516 [39:06<03:43,  5.31s/it]
2025-02-04 02:15:46,819 - INFO - tqdm - perplexity: 127.8542, batch_loss: 4.1716, loss: 4.8509 ||:  92%|#########2| 476/516 [39:16<03:28,  5.22s/it]
2025-02-04 02:16:02,803 - INFO - tqdm - perplexity: 127.2786, batch_loss: 4.0821, loss: 4.8464 ||:  93%|#########2| 479/516 [39:32<03:19,  5.39s/it]
2025-02-04 02:16:13,818 - INFO - tqdm - perplexity: 126.9853, batch_loss: 4.2741, loss: 4.8441 ||:  93%|#########3| 481/516 [39:43<03:11,  5.46s/it]
2025-02-04 02:16:24,436 - INFO - tqdm - perplexity: 126.6898, batch_loss: 4.2644, loss: 4.8417 ||:  94%|#########3| 483/516 [39:54<02:55,  5.31s/it]
2025-02-04 02:16:35,266 - INFO - tqdm - perplexity: 126.3130, batch_loss: 4.1036, loss: 4.8388 ||:  94%|#########3| 485/516 [40:05<02:46,  5.38s/it]
2025-02-04 02:16:48,132 - INFO - tqdm - perplexity: 125.9873, batch_loss: 4.1502, loss: 4.8362 ||:  94%|#########4| 487/516 [40:18<02:56,  6.08s/it]
2025-02-04 02:17:00,439 - INFO - tqdm - perplexity: 125.5890, batch_loss: 4.0736, loss: 4.8330 ||:  95%|#########4| 490/516 [40:30<02:04,  4.79s/it]
2025-02-04 02:17:10,682 - INFO - tqdm - perplexity: 125.2970, batch_loss: 4.1415, loss: 4.8307 ||:  95%|#########5| 492/516 [40:40<01:59,  4.97s/it]
2025-02-04 02:17:25,537 - INFO - tqdm - perplexity: 124.8136, batch_loss: 4.1202, loss: 4.8268 ||:  96%|#########5| 495/516 [40:55<01:44,  5.00s/it]
2025-02-04 02:17:40,269 - INFO - tqdm - perplexity: 124.4360, batch_loss: 4.2388, loss: 4.8238 ||:  97%|#########6| 498/516 [41:10<01:29,  4.97s/it]
2025-02-04 02:17:51,318 - INFO - tqdm - perplexity: 124.1683, batch_loss: 4.2790, loss: 4.8216 ||:  97%|#########6| 500/516 [41:21<01:25,  5.37s/it]
2025-02-04 02:18:04,431 - INFO - tqdm - perplexity: 123.7828, batch_loss: 4.2494, loss: 4.8185 ||:  97%|#########7| 503/516 [41:34<01:02,  4.81s/it]
2025-02-04 02:18:16,687 - INFO - tqdm - perplexity: 123.3788, batch_loss: 4.2913, loss: 4.8153 ||:  98%|#########8| 506/516 [41:46<00:43,  4.34s/it]
2025-02-04 02:18:31,167 - INFO - tqdm - perplexity: 122.9995, batch_loss: 4.5538, loss: 4.8122 ||:  99%|#########8| 509/516 [42:01<00:32,  4.69s/it]
2025-02-04 02:18:42,670 - INFO - tqdm - perplexity: 122.6701, batch_loss: 4.2419, loss: 4.8095 ||:  99%|#########9| 511/516 [42:12<00:26,  5.20s/it]
2025-02-04 02:18:56,580 - INFO - tqdm - perplexity: 122.1934, batch_loss: 4.1508, loss: 4.8056 ||: 100%|#########9| 514/516 [42:26<00:09,  4.84s/it]
2025-02-04 02:19:01,231 - INFO - tqdm - perplexity: 122.0402, batch_loss: 4.1595, loss: 4.8044 ||: 100%|#########9| 515/516 [42:31<00:04,  4.78s/it]
2025-02-04 02:19:01,970 - INFO - tqdm - perplexity: 121.7698, batch_loss: 3.6601, loss: 4.8021 ||: 100%|##########| 516/516 [42:31<00:00,  3.57s/it]
2025-02-04 02:19:01,971 - INFO - tqdm - perplexity: 121.7698, batch_loss: 3.6601, loss: 4.8021 ||: 100%|##########| 516/516 [42:31<00:00,  4.95s/it]
2025-02-04 02:19:01,972 - INFO - allennlp.training.gradient_descent_trainer - Validating
2025-02-04 02:19:01,972 - INFO - tqdm - 0%|          | 0/129 [00:00<?, ?it/s]
2025-02-04 02:19:12,217 - INFO - tqdm - perplexity: 72.2335, batch_loss: 4.2710, loss: 4.2799 ||:   6%|6         | 8/129 [00:10<02:29,  1.24s/it]
2025-02-04 02:19:22,780 - INFO - tqdm - perplexity: 77.7879, batch_loss: 4.8322, loss: 4.3540 ||:  12%|#2        | 16/129 [00:20<02:23,  1.27s/it]
2025-02-04 02:19:33,663 - INFO - tqdm - perplexity: 79.9986, batch_loss: 4.4259, loss: 4.3820 ||:  19%|#9        | 25/129 [00:31<02:00,  1.16s/it]
2025-02-04 02:19:44,643 - INFO - tqdm - perplexity: 78.4846, batch_loss: 4.2293, loss: 4.3629 ||:  26%|##6       | 34/129 [00:42<01:57,  1.24s/it]
2025-02-04 02:19:55,281 - INFO - tqdm - perplexity: 79.3470, batch_loss: 4.3505, loss: 4.3738 ||:  33%|###2      | 42/129 [00:53<01:54,  1.32s/it]
2025-02-04 02:20:05,444 - INFO - tqdm - perplexity: 78.6294, batch_loss: 4.4789, loss: 4.3647 ||:  39%|###8      | 50/129 [01:03<01:38,  1.24s/it]
2025-02-04 02:20:15,463 - INFO - tqdm - perplexity: 78.5862, batch_loss: 4.4778, loss: 4.3642 ||:  45%|####4     | 58/129 [01:13<01:33,  1.32s/it]
2025-02-04 02:20:26,630 - INFO - tqdm - perplexity: 77.6795, batch_loss: 4.0361, loss: 4.3526 ||:  52%|#####1    | 67/129 [01:24<01:16,  1.24s/it]
2025-02-04 02:20:36,738 - INFO - tqdm - perplexity: 77.0371, batch_loss: 4.3626, loss: 4.3443 ||:  58%|#####8    | 75/129 [01:34<01:06,  1.24s/it]
2025-02-04 02:20:47,450 - INFO - tqdm - perplexity: 76.6108, batch_loss: 4.4082, loss: 4.3387 ||:  64%|######4   | 83/129 [01:45<00:59,  1.29s/it]
2025-02-04 02:20:57,772 - INFO - tqdm - perplexity: 76.4865, batch_loss: 4.4320, loss: 4.3371 ||:  71%|#######   | 91/129 [01:55<00:51,  1.36s/it]
2025-02-04 02:21:08,162 - INFO - tqdm - perplexity: 76.0696, batch_loss: 4.0967, loss: 4.3316 ||:  77%|#######6  | 99/129 [02:06<00:40,  1.34s/it]
2025-02-04 02:21:19,066 - INFO - tqdm - perplexity: 76.1842, batch_loss: 4.1936, loss: 4.3332 ||:  84%|########3 | 108/129 [02:17<00:25,  1.20s/it]
2025-02-04 02:21:29,520 - INFO - tqdm - perplexity: 76.2297, batch_loss: 4.5028, loss: 4.3338 ||:  90%|########9 | 116/129 [02:27<00:17,  1.32s/it]
2025-02-04 02:21:40,881 - INFO - tqdm - perplexity: 76.2969, batch_loss: 4.3613, loss: 4.3346 ||:  97%|#########6| 125/129 [02:38<00:05,  1.31s/it]
2025-02-04 02:21:46,358 - INFO - tqdm - perplexity: 75.9630, batch_loss: 3.9776, loss: 4.3302 ||: 100%|##########| 129/129 [02:44<00:00,  1.34s/it]
2025-02-04 02:21:46,359 - INFO - tqdm - perplexity: 75.9630, batch_loss: 3.9776, loss: 4.3302 ||: 100%|##########| 129/129 [02:44<00:00,  1.27s/it]
2025-02-04 02:21:46,361 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2025-02-04 02:21:46,362 - INFO - allennlp.training.callbacks.console_logger - loss               |     4.802  |     4.330
2025-02-04 02:21:46,362 - INFO - allennlp.training.callbacks.console_logger - perplexity         |   121.770  |    75.963
2025-02-04 02:21:46,362 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |     0.000  |       N/A
2025-02-04 02:21:46,958 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:45:16.958468
2025-02-04 02:21:46,958 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 22:38:00
2025-02-04 02:21:46,959 - INFO - allennlp.training.gradient_descent_trainer - Epoch 2/31
2025-02-04 02:21:46,959 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 0B
2025-02-04 02:21:46,959 - INFO - allennlp.training.gradient_descent_trainer - Training
2025-02-04 02:21:46,959 - INFO - tqdm - 0%|          | 0/516 [00:00<?, ?it/s]
2025-02-04 02:22:02,051 - INFO - tqdm - perplexity: 52.1812, batch_loss: 3.9168, loss: 3.9547 ||:   1%|          | 3/516 [00:15<44:04,  5.15s/it]
2025-02-04 02:22:14,751 - INFO - tqdm - perplexity: 58.2474, batch_loss: 4.1716, loss: 4.0647 ||:   1%|1         | 6/516 [00:27<39:11,  4.61s/it]
2025-02-04 02:22:30,542 - INFO - tqdm - perplexity: 58.4265, batch_loss: 3.9723, loss: 4.0678 ||:   2%|1         | 9/516 [00:43<44:45,  5.30s/it]
2025-02-04 02:22:42,803 - INFO - tqdm - perplexity: 58.1598, batch_loss: 4.1012, loss: 4.0632 ||:   2%|2         | 12/516 [00:55<37:25,  4.46s/it]
2025-02-04 02:22:53,899 - INFO - tqdm - perplexity: 58.6245, batch_loss: 4.3627, loss: 4.0712 ||:   3%|2         | 14/516 [01:06<41:09,  4.92s/it]
2025-02-04 02:23:05,240 - INFO - tqdm - perplexity: 57.8850, batch_loss: 4.2114, loss: 4.0585 ||:   3%|3         | 16/516 [01:18<44:13,  5.31s/it]
2025-02-04 02:23:15,770 - INFO - tqdm - perplexity: 58.6796, batch_loss: 4.1440, loss: 4.0721 ||:   3%|3         | 18/516 [01:28<44:47,  5.40s/it]
2025-02-04 02:23:26,255 - INFO - tqdm - perplexity: 58.8101, batch_loss: 4.0182, loss: 4.0743 ||:   4%|3         | 20/516 [01:39<44:12,  5.35s/it]
2025-02-04 02:23:39,678 - INFO - tqdm - perplexity: 59.0414, batch_loss: 4.1714, loss: 4.0782 ||:   4%|4         | 23/516 [01:52<39:08,  4.76s/it]
2025-02-04 02:23:52,262 - INFO - tqdm - perplexity: 58.5965, batch_loss: 3.8454, loss: 4.0707 ||:   5%|5         | 26/516 [02:05<35:31,  4.35s/it]
2025-02-04 02:24:05,958 - INFO - tqdm - perplexity: 58.3073, batch_loss: 4.1242, loss: 4.0657 ||:   6%|5         | 29/516 [02:18<36:21,  4.48s/it]
2025-02-04 02:24:19,448 - INFO - tqdm - perplexity: 58.6715, batch_loss: 4.4684, loss: 4.0720 ||:   6%|6         | 32/516 [02:32<36:40,  4.55s/it]
2025-02-04 02:24:30,292 - INFO - tqdm - perplexity: 57.9768, batch_loss: 3.7885, loss: 4.0600 ||:   7%|6         | 34/516 [02:43<39:46,  4.95s/it]
2025-02-04 02:24:40,916 - INFO - tqdm - perplexity: 57.4640, batch_loss: 3.8792, loss: 4.0512 ||:   7%|6         | 36/516 [02:53<41:31,  5.19s/it]
2025-02-04 02:24:54,982 - INFO - tqdm - perplexity: 57.2557, batch_loss: 3.8327, loss: 4.0475 ||:   8%|7         | 39/516 [03:08<38:46,  4.88s/it]
2025-02-04 02:25:08,476 - INFO - tqdm - perplexity: 56.3733, batch_loss: 3.6874, loss: 4.0320 ||:   8%|8         | 42/516 [03:21<37:03,  4.69s/it]
2025-02-04 02:25:24,036 - INFO - tqdm - perplexity: 55.7135, batch_loss: 3.8775, loss: 4.0202 ||:   9%|8         | 45/516 [03:37<39:59,  5.09s/it]
2025-02-04 02:25:37,501 - INFO - tqdm - perplexity: 55.8750, batch_loss: 4.0887, loss: 4.0231 ||:   9%|9         | 48/516 [03:50<36:15,  4.65s/it]
2025-02-04 02:25:49,511 - INFO - tqdm - perplexity: 55.8693, batch_loss: 4.0697, loss: 4.0230 ||:  10%|9         | 50/516 [04:02<40:26,  5.21s/it]
2025-02-04 02:26:00,160 - INFO - tqdm - perplexity: 56.0252, batch_loss: 4.0960, loss: 4.0258 ||:  10%|#         | 52/516 [04:13<40:40,  5.26s/it]
2025-02-04 02:26:10,778 - INFO - tqdm - perplexity: 55.9324, batch_loss: 3.9855, loss: 4.0241 ||:  10%|#         | 54/516 [04:23<40:36,  5.27s/it]
2025-02-04 02:26:25,087 - INFO - tqdm - perplexity: 55.6277, batch_loss: 4.1092, loss: 4.0187 ||:  11%|#1        | 57/516 [04:38<37:31,  4.90s/it]
2025-02-04 02:26:40,184 - INFO - tqdm - perplexity: 55.6748, batch_loss: 4.1564, loss: 4.0195 ||:  12%|#1        | 60/516 [04:53<38:21,  5.05s/it]
2025-02-04 02:26:55,852 - INFO - tqdm - perplexity: 55.0503, batch_loss: 3.8256, loss: 4.0082 ||:  12%|#2        | 63/516 [05:08<39:59,  5.30s/it]
2025-02-04 02:27:10,879 - INFO - tqdm - perplexity: 54.8590, batch_loss: 3.8279, loss: 4.0048 ||:  13%|#2        | 66/516 [05:23<38:46,  5.17s/it]
2025-02-04 02:27:22,996 - INFO - tqdm - perplexity: 54.6243, batch_loss: 4.0271, loss: 4.0005 ||:  13%|#3        | 69/516 [05:36<33:23,  4.48s/it]
2025-02-04 02:27:33,378 - INFO - tqdm - perplexity: 54.6763, batch_loss: 4.0704, loss: 4.0014 ||:  14%|#3        | 71/516 [05:46<35:15,  4.75s/it]
2025-02-04 02:27:47,763 - INFO - tqdm - perplexity: 54.6946, batch_loss: 3.8289, loss: 4.0018 ||:  14%|#4        | 74/516 [06:00<34:37,  4.70s/it]
2025-02-04 02:28:01,661 - INFO - tqdm - perplexity: 54.6583, batch_loss: 4.0347, loss: 4.0011 ||:  15%|#4        | 77/516 [06:14<34:17,  4.69s/it]
2025-02-04 02:28:12,117 - INFO - tqdm - perplexity: 54.5137, batch_loss: 3.9412, loss: 3.9985 ||:  15%|#5        | 79/516 [06:25<36:23,  5.00s/it]
2025-02-04 02:28:22,937 - INFO - tqdm - perplexity: 54.4912, batch_loss: 3.8846, loss: 3.9980 ||:  16%|#5        | 81/516 [06:35<38:12,  5.27s/it]
2025-02-04 02:28:34,349 - INFO - tqdm - perplexity: 54.5025, batch_loss: 3.9968, loss: 3.9982 ||:  16%|#6        | 83/516 [06:47<39:51,  5.52s/it]
2025-02-04 02:28:45,375 - INFO - tqdm - perplexity: 54.3047, batch_loss: 3.7867, loss: 3.9946 ||:  16%|#6        | 85/516 [06:58<40:16,  5.61s/it]
2025-02-04 02:28:58,486 - INFO - tqdm - perplexity: 54.2538, batch_loss: 4.0726, loss: 3.9937 ||:  17%|#7        | 88/516 [07:11<34:16,  4.80s/it]
2025-02-04 02:29:11,408 - INFO - tqdm - perplexity: 54.0723, batch_loss: 3.9741, loss: 3.9903 ||:  18%|#7        | 91/516 [07:24<31:37,  4.46s/it]
2025-02-04 02:29:23,694 - INFO - tqdm - perplexity: 53.9443, batch_loss: 4.0138, loss: 3.9880 ||:  18%|#8        | 93/516 [07:36<37:11,  5.27s/it]
2025-02-04 02:29:36,482 - INFO - tqdm - perplexity: 53.8498, batch_loss: 3.7771, loss: 3.9862 ||:  19%|#8        | 96/516 [07:49<32:32,  4.65s/it]
2025-02-04 02:29:48,261 - INFO - tqdm - perplexity: 53.6624, batch_loss: 3.7515, loss: 3.9827 ||:  19%|#8        | 98/516 [08:01<37:25,  5.37s/it]
2025-02-04 02:29:58,371 - INFO - tqdm - perplexity: 53.7618, batch_loss: 4.0970, loss: 3.9846 ||:  19%|#9        | 100/516 [08:11<35:07,  5.07s/it]
2025-02-04 02:30:10,235 - INFO - tqdm - perplexity: 53.7725, batch_loss: 4.1370, loss: 3.9848 ||:  20%|#9        | 102/516 [08:23<38:12,  5.54s/it]
2025-02-04 02:30:22,332 - INFO - tqdm - perplexity: 53.6521, batch_loss: 3.7888, loss: 3.9825 ||:  20%|##        | 104/516 [08:35<39:33,  5.76s/it]
2025-02-04 02:30:35,737 - INFO - tqdm - perplexity: 53.5671, batch_loss: 4.0221, loss: 3.9809 ||:  21%|##        | 107/516 [08:48<33:01,  4.85s/it]
2025-02-04 02:30:47,223 - INFO - tqdm - perplexity: 53.4264, batch_loss: 4.0100, loss: 3.9783 ||:  21%|##1       | 109/516 [09:00<35:25,  5.22s/it]
2025-02-04 02:30:57,494 - INFO - tqdm - perplexity: 53.4467, batch_loss: 3.8895, loss: 3.9787 ||:  22%|##1       | 111/516 [09:10<35:23,  5.24s/it]
2025-02-04 02:31:10,086 - INFO - tqdm - perplexity: 53.4634, batch_loss: 4.1965, loss: 3.9790 ||:  22%|##2       | 114/516 [09:23<30:18,  4.52s/it]
2025-02-04 02:31:23,806 - INFO - tqdm - perplexity: 53.4803, batch_loss: 4.0152, loss: 3.9793 ||:  23%|##2       | 117/516 [09:36<29:51,  4.49s/it]
2025-02-04 02:31:41,122 - INFO - tqdm - perplexity: 53.2746, batch_loss: 3.8764, loss: 3.9755 ||:  23%|##3       | 120/516 [09:54<37:02,  5.61s/it]
2025-02-04 02:31:55,818 - INFO - tqdm - perplexity: 52.9985, batch_loss: 3.8891, loss: 3.9703 ||:  24%|##3       | 123/516 [10:08<33:36,  5.13s/it]
2025-02-04 02:32:09,625 - INFO - tqdm - perplexity: 52.9832, batch_loss: 4.0421, loss: 3.9700 ||:  24%|##4       | 126/516 [10:22<31:25,  4.83s/it]
2025-02-04 02:32:19,817 - INFO - tqdm - perplexity: 52.9217, batch_loss: 3.9914, loss: 3.9688 ||:  25%|##4       | 128/516 [10:32<32:08,  4.97s/it]
2025-02-04 02:32:32,712 - INFO - tqdm - perplexity: 52.7833, batch_loss: 3.9479, loss: 3.9662 ||:  25%|##5       | 131/516 [10:45<28:38,  4.46s/it]
2025-02-04 02:32:47,349 - INFO - tqdm - perplexity: 52.6523, batch_loss: 3.7504, loss: 3.9637 ||:  26%|##5       | 134/516 [11:00<30:31,  4.80s/it]
2025-02-04 02:32:58,334 - INFO - tqdm - perplexity: 52.4699, batch_loss: 3.7903, loss: 3.9602 ||:  26%|##6       | 136/516 [11:11<32:42,  5.17s/it]
2025-02-04 02:33:12,816 - INFO - tqdm - perplexity: 52.3468, batch_loss: 3.8105, loss: 3.9579 ||:  27%|##6       | 139/516 [11:25<30:59,  4.93s/it]
2025-02-04 02:33:26,792 - INFO - tqdm - perplexity: 52.1642, batch_loss: 3.8389, loss: 3.9544 ||:  28%|##7       | 142/516 [11:39<29:52,  4.79s/it]
2025-02-04 02:33:39,230 - INFO - tqdm - perplexity: 52.2198, batch_loss: 3.7765, loss: 3.9555 ||:  28%|##8       | 145/516 [11:52<27:04,  4.38s/it]
2025-02-04 02:33:51,419 - INFO - tqdm - perplexity: 52.0578, batch_loss: 3.8822, loss: 3.9524 ||:  28%|##8       | 147/516 [12:04<32:13,  5.24s/it]
2025-02-04 02:34:05,413 - INFO - tqdm - perplexity: 51.8857, batch_loss: 3.7210, loss: 3.9490 ||:  29%|##9       | 150/516 [12:18<29:40,  4.86s/it]
2025-02-04 02:34:15,815 - INFO - tqdm - perplexity: 51.7737, batch_loss: 3.8388, loss: 3.9469 ||:  29%|##9       | 152/516 [12:28<30:29,  5.03s/it]
2025-02-04 02:34:29,602 - INFO - tqdm - perplexity: 51.7390, batch_loss: 3.7311, loss: 3.9462 ||:  30%|###       | 155/516 [12:42<28:40,  4.77s/it]
2025-02-04 02:34:42,876 - INFO - tqdm - perplexity: 51.5673, batch_loss: 3.6899, loss: 3.9429 ||:  31%|###       | 158/516 [12:55<27:44,  4.65s/it]
2025-02-04 02:34:55,813 - INFO - tqdm - perplexity: 51.5164, batch_loss: 3.8603, loss: 3.9419 ||:  31%|###1      | 160/516 [13:08<33:10,  5.59s/it]
2025-02-04 02:35:06,243 - INFO - tqdm - perplexity: 51.4693, batch_loss: 3.8942, loss: 3.9410 ||:  31%|###1      | 162/516 [13:19<31:36,  5.36s/it]
2025-02-04 02:35:16,902 - INFO - tqdm - perplexity: 51.3403, batch_loss: 3.5401, loss: 3.9385 ||:  32%|###1      | 164/516 [13:29<31:10,  5.31s/it]
2025-02-04 02:35:30,464 - INFO - tqdm - perplexity: 51.1132, batch_loss: 3.7424, loss: 3.9340 ||:  32%|###2      | 167/516 [13:43<28:00,  4.82s/it]
2025-02-04 02:35:43,903 - INFO - tqdm - perplexity: 50.9919, batch_loss: 3.6526, loss: 3.9317 ||:  33%|###2      | 170/516 [13:56<27:09,  4.71s/it]
2025-02-04 02:35:59,552 - INFO - tqdm - perplexity: 51.0179, batch_loss: 3.8874, loss: 3.9322 ||:  34%|###3      | 173/516 [14:12<29:31,  5.16s/it]
2025-02-04 02:36:13,247 - INFO - tqdm - perplexity: 50.8986, batch_loss: 3.8368, loss: 3.9298 ||:  34%|###4      | 176/516 [14:26<26:59,  4.76s/it]
2025-02-04 02:36:27,919 - INFO - tqdm - perplexity: 50.7481, batch_loss: 3.8141, loss: 3.9269 ||:  35%|###4      | 179/516 [14:40<27:09,  4.84s/it]
2025-02-04 02:36:42,214 - INFO - tqdm - perplexity: 50.6491, batch_loss: 3.6927, loss: 3.9249 ||:  35%|###5      | 182/516 [14:55<26:41,  4.79s/it]
2025-02-04 02:36:56,909 - INFO - tqdm - perplexity: 50.5338, batch_loss: 3.6754, loss: 3.9226 ||:  36%|###5      | 185/516 [15:09<27:40,  5.02s/it]
2025-02-04 02:37:09,083 - INFO - tqdm - perplexity: 50.4898, batch_loss: 3.7255, loss: 3.9218 ||:  36%|###6      | 187/516 [15:22<30:58,  5.65s/it]
2025-02-04 02:37:19,276 - INFO - tqdm - perplexity: 50.3996, batch_loss: 3.6668, loss: 3.9200 ||:  37%|###6      | 189/516 [15:32<29:08,  5.35s/it]
2025-02-04 02:37:32,746 - INFO - tqdm - perplexity: 50.2538, batch_loss: 3.6686, loss: 3.9171 ||:  37%|###7      | 191/516 [15:45<33:59,  6.28s/it]
2025-02-04 02:37:44,125 - INFO - tqdm - perplexity: 50.1980, batch_loss: 3.8397, loss: 3.9160 ||:  37%|###7      | 193/516 [15:57<31:55,  5.93s/it]
2025-02-04 02:37:54,296 - INFO - tqdm - perplexity: 50.1325, batch_loss: 3.7835, loss: 3.9147 ||:  38%|###7      | 195/516 [16:07<28:52,  5.40s/it]
2025-02-04 02:38:06,787 - INFO - tqdm - perplexity: 50.0697, batch_loss: 3.8781, loss: 3.9134 ||:  38%|###8      | 198/516 [16:19<24:20,  4.59s/it]
2025-02-04 02:38:20,752 - INFO - tqdm - perplexity: 49.9468, batch_loss: 3.6704, loss: 3.9110 ||:  39%|###8      | 201/516 [16:33<24:10,  4.60s/it]
2025-02-04 02:38:35,023 - INFO - tqdm - perplexity: 49.8197, batch_loss: 3.6880, loss: 3.9084 ||:  40%|###9      | 204/516 [16:48<25:01,  4.81s/it]
2025-02-04 02:38:46,070 - INFO - tqdm - perplexity: 49.7109, batch_loss: 3.7142, loss: 3.9062 ||:  40%|###9      | 206/516 [16:59<26:50,  5.19s/it]
2025-02-04 02:38:58,809 - INFO - tqdm - perplexity: 49.5725, batch_loss: 3.7435, loss: 3.9034 ||:  41%|####      | 209/516 [17:11<23:45,  4.64s/it]
2025-02-04 02:39:10,432 - INFO - tqdm - perplexity: 49.4480, batch_loss: 3.7797, loss: 3.9009 ||:  41%|####      | 211/516 [17:23<26:11,  5.15s/it]
2025-02-04 02:39:20,701 - INFO - tqdm - perplexity: 49.3654, batch_loss: 3.7029, loss: 3.8993 ||:  41%|####1     | 213/516 [17:33<25:51,  5.12s/it]
2025-02-04 02:39:34,704 - INFO - tqdm - perplexity: 49.2738, batch_loss: 3.9319, loss: 3.8974 ||:  42%|####1     | 216/516 [17:47<23:40,  4.73s/it]
2025-02-04 02:39:48,945 - INFO - tqdm - perplexity: 49.0902, batch_loss: 3.6346, loss: 3.8937 ||:  42%|####2     | 219/516 [18:01<23:41,  4.79s/it]
2025-02-04 02:39:59,426 - INFO - tqdm - perplexity: 49.0802, batch_loss: 3.6430, loss: 3.8935 ||:  43%|####2     | 221/516 [18:12<24:25,  4.97s/it]
2025-02-04 02:40:10,441 - INFO - tqdm - perplexity: 48.9970, batch_loss: 3.6458, loss: 3.8918 ||:  43%|####3     | 223/516 [18:23<25:57,  5.32s/it]
2025-02-04 02:40:23,078 - INFO - tqdm - perplexity: 48.9841, batch_loss: 3.9158, loss: 3.8915 ||:  44%|####3     | 225/516 [18:36<28:28,  5.87s/it]
2025-02-04 02:40:36,531 - INFO - tqdm - perplexity: 48.9804, batch_loss: 4.0167, loss: 3.8914 ||:  44%|####4     | 228/516 [18:49<23:18,  4.86s/it]
2025-02-04 02:40:47,610 - INFO - tqdm - perplexity: 48.9149, batch_loss: 3.8955, loss: 3.8901 ||:  45%|####4     | 230/516 [19:00<24:24,  5.12s/it]
2025-02-04 02:40:59,443 - INFO - tqdm - perplexity: 48.8032, batch_loss: 3.5791, loss: 3.8878 ||:  45%|####4     | 232/516 [19:12<26:35,  5.62s/it]
2025-02-04 02:41:10,423 - INFO - tqdm - perplexity: 48.8095, batch_loss: 4.0170, loss: 3.8879 ||:  45%|####5     | 234/516 [19:23<25:45,  5.48s/it]
2025-02-04 02:41:20,450 - INFO - tqdm - perplexity: 48.7551, batch_loss: 3.9130, loss: 3.8868 ||:  46%|####5     | 236/516 [19:33<24:15,  5.20s/it]
2025-02-04 02:41:31,602 - INFO - tqdm - perplexity: 48.7415, batch_loss: 3.8754, loss: 3.8865 ||:  46%|####6     | 238/516 [19:44<24:35,  5.31s/it]
2025-02-04 02:41:44,346 - INFO - tqdm - perplexity: 48.5813, batch_loss: 3.8092, loss: 3.8832 ||:  47%|####6     | 241/516 [19:57<20:54,  4.56s/it]
2025-02-04 02:41:55,115 - INFO - tqdm - perplexity: 48.4465, batch_loss: 3.4830, loss: 3.8805 ||:  47%|####7     | 243/516 [20:08<22:40,  4.98s/it]
2025-02-04 02:42:06,036 - INFO - tqdm - perplexity: 48.3245, batch_loss: 3.6411, loss: 3.8779 ||:  47%|####7     | 245/516 [20:19<23:24,  5.18s/it]
2025-02-04 02:42:16,051 - INFO - tqdm - perplexity: 48.2561, batch_loss: 3.5748, loss: 3.8765 ||:  48%|####7     | 247/516 [20:29<23:04,  5.15s/it]
2025-02-04 02:42:29,697 - INFO - tqdm - perplexity: 48.1128, batch_loss: 3.6964, loss: 3.8735 ||:  48%|####8     | 250/516 [20:42<21:09,  4.77s/it]
2025-02-04 02:42:40,577 - INFO - tqdm - perplexity: 47.9745, batch_loss: 3.5836, loss: 3.8707 ||:  49%|####8     | 252/516 [20:53<22:02,  5.01s/it]
2025-02-04 02:42:50,616 - INFO - tqdm - perplexity: 47.9269, batch_loss: 3.6820, loss: 3.8697 ||:  49%|####9     | 254/516 [21:03<21:36,  4.95s/it]
2025-02-04 02:43:05,035 - INFO - tqdm - perplexity: 47.9322, batch_loss: 3.8501, loss: 3.8698 ||:  50%|####9     | 257/516 [21:18<22:04,  5.11s/it]
2025-02-04 02:43:20,011 - INFO - tqdm - perplexity: 47.7929, batch_loss: 3.5162, loss: 3.8669 ||:  50%|#####     | 260/516 [21:33<21:50,  5.12s/it]
2025-02-04 02:43:35,241 - INFO - tqdm - perplexity: 47.7346, batch_loss: 3.7289, loss: 3.8657 ||:  51%|#####     | 263/516 [21:48<21:46,  5.16s/it]
2025-02-04 02:43:46,923 - INFO - tqdm - perplexity: 47.7303, batch_loss: 3.8418, loss: 3.8656 ||:  51%|#####1    | 264/516 [21:59<29:53,  7.12s/it]
2025-02-04 02:43:57,417 - INFO - tqdm - perplexity: 47.6868, batch_loss: 3.7717, loss: 3.8647 ||:  52%|#####1    | 266/516 [22:10<25:26,  6.11s/it]
2025-02-04 02:44:13,109 - INFO - tqdm - perplexity: 47.6236, batch_loss: 3.5563, loss: 3.8633 ||:  52%|#####2    | 269/516 [22:26<23:07,  5.62s/it]
2025-02-04 02:44:23,131 - INFO - tqdm - perplexity: 47.5203, batch_loss: 3.6068, loss: 3.8612 ||:  53%|#####2    | 271/516 [22:36<21:26,  5.25s/it]
2025-02-04 02:44:34,539 - INFO - tqdm - perplexity: 47.4487, batch_loss: 3.5332, loss: 3.8596 ||:  53%|#####2    | 273/516 [22:47<22:46,  5.62s/it]
2025-02-04 02:44:48,518 - INFO - tqdm - perplexity: 47.3422, batch_loss: 3.5334, loss: 3.8574 ||:  53%|#####3    | 276/516 [23:01<19:59,  5.00s/it]
2025-02-04 02:45:02,127 - INFO - tqdm - perplexity: 47.1973, batch_loss: 3.6683, loss: 3.8543 ||:  54%|#####4    | 279/516 [23:15<18:35,  4.71s/it]
2025-02-04 02:45:18,009 - INFO - tqdm - perplexity: 47.1241, batch_loss: 3.5330, loss: 3.8528 ||:  55%|#####4    | 282/516 [23:31<20:19,  5.21s/it]
2025-02-04 02:45:34,110 - INFO - tqdm - perplexity: 47.0156, batch_loss: 3.4334, loss: 3.8505 ||:  55%|#####5    | 285/516 [23:47<21:14,  5.52s/it]
2025-02-04 02:45:44,297 - INFO - tqdm - perplexity: 46.9704, batch_loss: 3.7254, loss: 3.8495 ||:  56%|#####5    | 287/516 [23:57<20:12,  5.30s/it]
2025-02-04 02:45:54,927 - INFO - tqdm - perplexity: 46.8755, batch_loss: 3.5025, loss: 3.8475 ||:  56%|#####6    | 289/516 [24:07<19:42,  5.21s/it]
2025-02-04 02:46:09,868 - INFO - tqdm - perplexity: 46.8334, batch_loss: 3.8070, loss: 3.8466 ||:  57%|#####6    | 292/516 [24:22<18:53,  5.06s/it]
2025-02-04 02:46:20,175 - INFO - tqdm - perplexity: 46.7408, batch_loss: 3.4589, loss: 3.8446 ||:  57%|#####6    | 294/516 [24:33<19:02,  5.15s/it]
2025-02-04 02:46:34,285 - INFO - tqdm - perplexity: 46.6475, batch_loss: 3.6783, loss: 3.8426 ||:  58%|#####7    | 297/516 [24:47<17:42,  4.85s/it]
2025-02-04 02:46:48,560 - INFO - tqdm - perplexity: 46.5741, batch_loss: 3.6435, loss: 3.8410 ||:  58%|#####8    | 300/516 [25:01<17:32,  4.87s/it]
2025-02-04 02:47:01,424 - INFO - tqdm - perplexity: 46.4407, batch_loss: 3.4945, loss: 3.8382 ||:  59%|#####8    | 303/516 [25:14<15:52,  4.47s/it]
2025-02-04 02:47:12,068 - INFO - tqdm - perplexity: 46.3622, batch_loss: 3.5614, loss: 3.8365 ||:  59%|#####9    | 305/516 [25:25<16:59,  4.83s/it]
2025-02-04 02:47:23,825 - INFO - tqdm - perplexity: 46.2596, batch_loss: 3.5462, loss: 3.8343 ||:  59%|#####9    | 307/516 [25:36<18:35,  5.34s/it]
2025-02-04 02:47:36,138 - INFO - tqdm - perplexity: 46.1319, batch_loss: 3.5668, loss: 3.8315 ||:  60%|######    | 310/516 [25:49<15:35,  4.54s/it]
2025-02-04 02:47:48,239 - INFO - tqdm - perplexity: 46.0375, batch_loss: 3.4646, loss: 3.8295 ||:  61%|######    | 313/516 [26:01<14:13,  4.20s/it]
2025-02-04 02:48:03,747 - INFO - tqdm - perplexity: 45.9038, batch_loss: 3.4308, loss: 3.8265 ||:  61%|######1   | 316/516 [26:16<16:21,  4.91s/it]
2025-02-04 02:48:17,828 - INFO - tqdm - perplexity: 45.9119, batch_loss: 3.6085, loss: 3.8267 ||:  62%|######1   | 319/516 [26:30<15:54,  4.85s/it]
2025-02-04 02:48:32,226 - INFO - tqdm - perplexity: 45.8213, batch_loss: 3.5695, loss: 3.8247 ||:  62%|######2   | 322/516 [26:45<15:35,  4.82s/it]
2025-02-04 02:48:42,507 - INFO - tqdm - perplexity: 45.7931, batch_loss: 3.8307, loss: 3.8241 ||:  63%|######2   | 324/516 [26:55<16:12,  5.06s/it]
2025-02-04 02:48:55,855 - INFO - tqdm - perplexity: 45.7022, batch_loss: 3.5158, loss: 3.8221 ||:  63%|######3   | 327/516 [27:08<14:46,  4.69s/it]
2025-02-04 02:49:07,409 - INFO - tqdm - perplexity: 45.6861, batch_loss: 3.5763, loss: 3.8218 ||:  64%|######3   | 329/516 [27:20<15:52,  5.10s/it]
2025-02-04 02:49:18,246 - INFO - tqdm - perplexity: 45.6445, batch_loss: 3.5695, loss: 3.8209 ||:  64%|######4   | 331/516 [27:31<16:15,  5.27s/it]
2025-02-04 02:49:28,352 - INFO - tqdm - perplexity: 45.6293, batch_loss: 4.1454, loss: 3.8205 ||:  65%|######4   | 333/516 [27:41<15:44,  5.16s/it]
2025-02-04 02:49:38,425 - INFO - tqdm - perplexity: 45.5770, batch_loss: 3.7243, loss: 3.8194 ||:  65%|######4   | 335/516 [27:51<15:32,  5.15s/it]
2025-02-04 02:49:48,761 - INFO - tqdm - perplexity: 45.5046, batch_loss: 3.3471, loss: 3.8178 ||:  65%|######5   | 337/516 [28:01<15:34,  5.22s/it]
2025-02-04 02:50:02,632 - INFO - tqdm - perplexity: 45.4558, batch_loss: 3.4859, loss: 3.8167 ||:  66%|######5   | 340/516 [28:15<14:03,  4.79s/it]
2025-02-04 02:50:12,993 - INFO - tqdm - perplexity: 45.3943, batch_loss: 3.6252, loss: 3.8154 ||:  66%|######6   | 342/516 [28:26<14:42,  5.07s/it]
2025-02-04 02:50:28,021 - INFO - tqdm - perplexity: 45.3483, batch_loss: 3.5980, loss: 3.8144 ||:  67%|######6   | 345/516 [28:41<14:30,  5.09s/it]
2025-02-04 02:50:42,667 - INFO - tqdm - perplexity: 45.2358, batch_loss: 3.6098, loss: 3.8119 ||:  67%|######7   | 348/516 [28:55<14:02,  5.01s/it]
2025-02-04 02:50:52,886 - INFO - tqdm - perplexity: 45.1772, batch_loss: 3.4158, loss: 3.8106 ||:  68%|######7   | 350/516 [29:05<14:09,  5.12s/it]
2025-02-04 02:51:04,545 - INFO - tqdm - perplexity: 45.0860, batch_loss: 3.4432, loss: 3.8086 ||:  68%|######8   | 352/516 [29:17<15:02,  5.50s/it]
2025-02-04 02:51:19,472 - INFO - tqdm - perplexity: 44.9849, batch_loss: 3.3205, loss: 3.8063 ||:  69%|######8   | 355/516 [29:32<14:11,  5.29s/it]
2025-02-04 02:51:32,800 - INFO - tqdm - perplexity: 44.8781, batch_loss: 3.6328, loss: 3.8039 ||:  69%|######9   | 358/516 [29:45<12:37,  4.80s/it]
2025-02-04 02:51:42,842 - INFO - tqdm - perplexity: 44.8313, batch_loss: 3.7547, loss: 3.8029 ||:  70%|######9   | 360/516 [29:55<12:35,  4.84s/it]
2025-02-04 02:51:57,799 - INFO - tqdm - perplexity: 44.7570, batch_loss: 3.4890, loss: 3.8012 ||:  70%|#######   | 363/516 [30:10<13:08,  5.15s/it]
2025-02-04 02:52:11,598 - INFO - tqdm - perplexity: 44.6761, batch_loss: 3.5151, loss: 3.7994 ||:  71%|#######   | 366/516 [30:24<12:30,  5.00s/it]
2025-02-04 02:52:22,362 - INFO - tqdm - perplexity: 44.5889, batch_loss: 3.2985, loss: 3.7975 ||:  71%|#######1  | 368/516 [30:35<12:55,  5.24s/it]
2025-02-04 02:52:36,171 - INFO - tqdm - perplexity: 44.5012, batch_loss: 3.5675, loss: 3.7955 ||:  72%|#######1  | 371/516 [30:49<11:35,  4.80s/it]
2025-02-04 02:52:49,494 - INFO - tqdm - perplexity: 44.4359, batch_loss: 3.6146, loss: 3.7940 ||:  72%|#######2  | 374/516 [31:02<10:54,  4.61s/it]
2025-02-04 02:53:00,489 - INFO - tqdm - perplexity: 44.3630, batch_loss: 3.5121, loss: 3.7924 ||:  73%|#######2  | 376/516 [31:13<11:41,  5.01s/it]
2025-02-04 02:53:14,467 - INFO - tqdm - perplexity: 44.2827, batch_loss: 3.6217, loss: 3.7906 ||:  73%|#######3  | 379/516 [31:27<11:01,  4.83s/it]
2025-02-04 02:53:25,753 - INFO - tqdm - perplexity: 44.1941, batch_loss: 3.4654, loss: 3.7886 ||:  74%|#######3  | 381/516 [31:38<11:36,  5.16s/it]
2025-02-04 02:53:36,477 - INFO - tqdm - perplexity: 44.1313, batch_loss: 3.5053, loss: 3.7872 ||:  74%|#######4  | 383/516 [31:49<11:28,  5.17s/it]
2025-02-04 02:53:46,588 - INFO - tqdm - perplexity: 44.0821, batch_loss: 3.5271, loss: 3.7861 ||:  75%|#######4  | 385/516 [31:59<11:05,  5.08s/it]
2025-02-04 02:54:00,497 - INFO - tqdm - perplexity: 43.9716, batch_loss: 3.5642, loss: 3.7835 ||:  75%|#######5  | 388/516 [32:13<10:03,  4.72s/it]
2025-02-04 02:54:11,567 - INFO - tqdm - perplexity: 43.9037, batch_loss: 3.5500, loss: 3.7820 ||:  76%|#######5  | 390/516 [32:24<10:36,  5.05s/it]
2025-02-04 02:54:23,640 - INFO - tqdm - perplexity: 43.8480, batch_loss: 3.5945, loss: 3.7807 ||:  76%|#######5  | 392/516 [32:36<11:45,  5.69s/it]
2025-02-04 02:54:35,196 - INFO - tqdm - perplexity: 43.7765, batch_loss: 3.4206, loss: 3.7791 ||:  77%|#######6  | 395/516 [32:48<09:06,  4.51s/it]
2025-02-04 02:54:47,506 - INFO - tqdm - perplexity: 43.7823, batch_loss: 3.8314, loss: 3.7792 ||:  77%|#######6  | 397/516 [33:00<10:47,  5.44s/it]
2025-02-04 02:54:58,143 - INFO - tqdm - perplexity: 43.7034, batch_loss: 3.4127, loss: 3.7774 ||:  77%|#######7  | 399/516 [33:11<10:22,  5.32s/it]
2025-02-04 02:55:13,696 - INFO - tqdm - perplexity: 43.6335, batch_loss: 3.3838, loss: 3.7758 ||:  78%|#######7  | 402/516 [33:26<10:13,  5.38s/it]
2025-02-04 02:55:28,758 - INFO - tqdm - perplexity: 43.5701, batch_loss: 3.4451, loss: 3.7744 ||:  78%|#######8  | 405/516 [33:41<09:51,  5.33s/it]
2025-02-04 02:55:43,241 - INFO - tqdm - perplexity: 43.4691, batch_loss: 3.4602, loss: 3.7721 ||:  79%|#######9  | 408/516 [33:56<09:23,  5.22s/it]
2025-02-04 02:55:57,144 - INFO - tqdm - perplexity: 43.3967, batch_loss: 3.4998, loss: 3.7704 ||:  80%|#######9  | 411/516 [34:10<08:34,  4.90s/it]
2025-02-04 02:56:07,546 - INFO - tqdm - perplexity: 43.3357, batch_loss: 3.5243, loss: 3.7690 ||:  80%|########  | 413/516 [34:20<08:42,  5.08s/it]
2025-02-04 02:56:24,046 - INFO - tqdm - perplexity: 43.2705, batch_loss: 3.4798, loss: 3.7675 ||:  81%|########  | 416/516 [34:37<09:09,  5.50s/it]
2025-02-04 02:56:38,043 - INFO - tqdm - perplexity: 43.1894, batch_loss: 3.4667, loss: 3.7656 ||:  81%|########1 | 419/516 [34:51<08:02,  4.97s/it]
2025-02-04 02:56:51,868 - INFO - tqdm - perplexity: 43.1202, batch_loss: 3.5086, loss: 3.7640 ||:  82%|########1 | 422/516 [35:04<07:33,  4.82s/it]
2025-02-04 02:57:03,610 - INFO - tqdm - perplexity: 43.0544, batch_loss: 3.4983, loss: 3.7625 ||:  82%|########2 | 424/516 [35:16<08:14,  5.37s/it]
2025-02-04 02:57:16,924 - INFO - tqdm - perplexity: 42.9511, batch_loss: 3.6075, loss: 3.7601 ||:  83%|########2 | 427/516 [35:29<07:02,  4.74s/it]
2025-02-04 02:57:29,333 - INFO - tqdm - perplexity: 42.9009, batch_loss: 3.6148, loss: 3.7589 ||:  83%|########3 | 430/516 [35:42<06:12,  4.33s/it]
2025-02-04 02:57:39,341 - INFO - tqdm - perplexity: 42.8598, batch_loss: 3.5506, loss: 3.7579 ||:  84%|########3 | 432/516 [35:52<06:39,  4.76s/it]
2025-02-04 02:57:53,157 - INFO - tqdm - perplexity: 42.8034, batch_loss: 3.6606, loss: 3.7566 ||:  84%|########4 | 435/516 [36:06<06:17,  4.66s/it]
2025-02-04 02:58:07,584 - INFO - tqdm - perplexity: 42.7532, batch_loss: 3.4479, loss: 3.7554 ||:  85%|########4 | 438/516 [36:20<06:09,  4.74s/it]
2025-02-04 02:58:18,743 - INFO - tqdm - perplexity: 42.7296, batch_loss: 3.7958, loss: 3.7549 ||:  85%|########5 | 440/516 [36:31<06:40,  5.27s/it]
2025-02-04 02:58:32,389 - INFO - tqdm - perplexity: 42.6683, batch_loss: 3.8520, loss: 3.7535 ||:  86%|########5 | 443/516 [36:45<05:47,  4.76s/it]
2025-02-04 02:58:42,489 - INFO - tqdm - perplexity: 42.5956, batch_loss: 3.4742, loss: 3.7518 ||:  86%|########6 | 445/516 [36:55<05:50,  4.94s/it]
2025-02-04 02:58:56,884 - INFO - tqdm - perplexity: 42.4965, batch_loss: 3.4636, loss: 3.7494 ||:  87%|########6 | 448/516 [37:09<05:26,  4.80s/it]
2025-02-04 02:59:10,776 - INFO - tqdm - perplexity: 42.4292, batch_loss: 3.3643, loss: 3.7478 ||:  87%|########7 | 451/516 [37:23<05:07,  4.73s/it]
2025-02-04 02:59:24,633 - INFO - tqdm - perplexity: 42.3839, batch_loss: 3.5397, loss: 3.7468 ||:  88%|########7 | 454/516 [37:37<04:50,  4.68s/it]
2025-02-04 02:59:38,901 - INFO - tqdm - perplexity: 42.3204, batch_loss: 3.4358, loss: 3.7453 ||:  89%|########8 | 457/516 [37:51<04:41,  4.77s/it]
2025-02-04 02:59:48,914 - INFO - tqdm - perplexity: 42.2609, batch_loss: 3.5059, loss: 3.7439 ||:  89%|########8 | 459/516 [38:01<04:40,  4.91s/it]
2025-02-04 03:00:03,046 - INFO - tqdm - perplexity: 42.1693, batch_loss: 3.3083, loss: 3.7417 ||:  90%|########9 | 462/516 [38:16<04:16,  4.76s/it]
2025-02-04 03:00:13,091 - INFO - tqdm - perplexity: 42.1336, batch_loss: 3.6266, loss: 3.7408 ||:  90%|########9 | 464/516 [38:26<04:17,  4.95s/it]
2025-02-04 03:00:26,747 - INFO - tqdm - perplexity: 42.0620, batch_loss: 3.4853, loss: 3.7391 ||:  91%|######### | 467/516 [38:39<03:51,  4.73s/it]
2025-02-04 03:00:37,094 - INFO - tqdm - perplexity: 41.9906, batch_loss: 3.4366, loss: 3.7374 ||:  91%|######### | 469/516 [38:50<03:56,  5.03s/it]
2025-02-04 03:00:48,300 - INFO - tqdm - perplexity: 41.9339, batch_loss: 3.3584, loss: 3.7361 ||:  91%|#########1| 471/516 [39:01<04:03,  5.40s/it]
2025-02-04 03:01:02,604 - INFO - tqdm - perplexity: 41.9035, batch_loss: 3.2427, loss: 3.7354 ||:  92%|#########1| 474/516 [39:15<03:31,  5.03s/it]
2025-02-04 03:01:12,672 - INFO - tqdm - perplexity: 41.8527, batch_loss: 3.4960, loss: 3.7342 ||:  92%|#########2| 476/516 [39:25<03:20,  5.01s/it]
2025-02-04 03:01:28,916 - INFO - tqdm - perplexity: 41.8103, batch_loss: 3.6148, loss: 3.7331 ||:  93%|#########2| 479/516 [39:41<03:21,  5.44s/it]
2025-02-04 03:01:43,513 - INFO - tqdm - perplexity: 41.7297, batch_loss: 3.5291, loss: 3.7312 ||:  93%|#########3| 482/516 [39:56<02:52,  5.09s/it]
2025-02-04 03:01:59,088 - INFO - tqdm - perplexity: 41.6413, batch_loss: 3.2877, loss: 3.7291 ||:  94%|#########3| 485/516 [40:12<02:43,  5.28s/it]
2025-02-04 03:02:12,729 - INFO - tqdm - perplexity: 41.5644, batch_loss: 3.3479, loss: 3.7272 ||:  95%|#########4| 488/516 [40:25<02:20,  5.00s/it]
2025-02-04 03:02:24,837 - INFO - tqdm - perplexity: 41.4534, batch_loss: 3.3346, loss: 3.7246 ||:  95%|#########5| 491/516 [40:37<01:47,  4.30s/it]
2025-02-04 03:02:37,773 - INFO - tqdm - perplexity: 41.3931, batch_loss: 3.2343, loss: 3.7231 ||:  96%|#########5| 493/516 [40:50<02:03,  5.36s/it]
2025-02-04 03:02:51,844 - INFO - tqdm - perplexity: 41.3466, batch_loss: 3.5469, loss: 3.7220 ||:  96%|#########6| 496/516 [41:04<01:37,  4.89s/it]
2025-02-04 03:03:06,102 - INFO - tqdm - perplexity: 41.2738, batch_loss: 3.3748, loss: 3.7202 ||:  97%|#########6| 499/516 [41:19<01:21,  4.82s/it]
2025-02-04 03:03:17,860 - INFO - tqdm - perplexity: 41.2356, batch_loss: 3.3214, loss: 3.7193 ||:  97%|#########7| 501/516 [41:30<01:19,  5.33s/it]
2025-02-04 03:03:28,911 - INFO - tqdm - perplexity: 41.1868, batch_loss: 3.5415, loss: 3.7181 ||:  97%|#########7| 503/516 [41:41<01:10,  5.39s/it]
2025-02-04 03:03:41,734 - INFO - tqdm - perplexity: 41.1067, batch_loss: 3.4364, loss: 3.7162 ||:  98%|#########8| 506/516 [41:54<00:47,  4.71s/it]
2025-02-04 03:03:57,527 - INFO - tqdm - perplexity: 41.0468, batch_loss: 3.4176, loss: 3.7147 ||:  99%|#########8| 509/516 [42:10<00:36,  5.20s/it]
2025-02-04 03:04:07,799 - INFO - tqdm - perplexity: 40.9997, batch_loss: 3.4466, loss: 3.7136 ||:  99%|#########9| 511/516 [42:20<00:25,  5.20s/it]
2025-02-04 03:04:17,830 - INFO - tqdm - perplexity: 40.9699, batch_loss: 3.3215, loss: 3.7128 ||:  99%|#########9| 513/516 [42:30<00:15,  5.08s/it]
2025-02-04 03:04:22,347 - INFO - tqdm - perplexity: 40.9460, batch_loss: 3.4131, loss: 3.7123 ||: 100%|#########9| 514/516 [42:35<00:09,  4.91s/it]
2025-02-04 03:04:27,571 - INFO - tqdm - perplexity: 40.9067, batch_loss: 3.2179, loss: 3.7113 ||: 100%|#########9| 515/516 [42:40<00:05,  5.01s/it]
2025-02-04 03:04:28,783 - INFO - tqdm - perplexity: 40.8932, batch_loss: 3.5413, loss: 3.7110 ||: 100%|##########| 516/516 [42:41<00:00,  3.87s/it]
2025-02-04 03:04:28,784 - INFO - tqdm - perplexity: 40.8932, batch_loss: 3.5413, loss: 3.7110 ||: 100%|##########| 516/516 [42:41<00:00,  4.96s/it]
2025-02-04 03:04:28,786 - INFO - allennlp.training.gradient_descent_trainer - Validating
2025-02-04 03:04:28,786 - INFO - tqdm - 0%|          | 0/129 [00:00<?, ?it/s]
2025-02-04 03:04:38,881 - INFO - tqdm - perplexity: 38.7036, batch_loss: 3.7348, loss: 3.6559 ||:   6%|6         | 8/129 [00:10<02:41,  1.34s/it]
2025-02-04 03:04:49,239 - INFO - tqdm - perplexity: 38.9206, batch_loss: 3.5320, loss: 3.6615 ||:  12%|#2        | 16/129 [00:20<02:27,  1.31s/it]
2025-02-04 03:04:59,955 - INFO - tqdm - perplexity: 38.7711, batch_loss: 3.5935, loss: 3.6577 ||:  19%|#8        | 24/129 [00:31<02:19,  1.33s/it]
2025-02-04 03:05:10,655 - INFO - tqdm - perplexity: 39.9558, batch_loss: 3.6491, loss: 3.6878 ||:  26%|##5       | 33/129 [00:41<01:58,  1.23s/it]
2025-02-04 03:05:21,070 - INFO - tqdm - perplexity: 40.1228, batch_loss: 3.4945, loss: 3.6919 ||:  32%|###1      | 41/129 [00:52<01:49,  1.25s/it]
2025-02-04 03:05:31,589 - INFO - tqdm - perplexity: 39.6473, batch_loss: 3.2982, loss: 3.6800 ||:  38%|###7      | 49/129 [01:02<01:46,  1.33s/it]
2025-02-04 03:05:41,909 - INFO - tqdm - perplexity: 40.2914, batch_loss: 3.7520, loss: 3.6961 ||:  44%|####4     | 57/129 [01:13<01:30,  1.26s/it]
2025-02-04 03:05:52,014 - INFO - tqdm - perplexity: 40.9239, batch_loss: 3.7971, loss: 3.7117 ||:  50%|#####     | 65/129 [01:23<01:16,  1.19s/it]
2025-02-04 03:06:02,751 - INFO - tqdm - perplexity: 40.6018, batch_loss: 3.6274, loss: 3.7038 ||:  57%|#####6    | 73/129 [01:33<01:13,  1.31s/it]
2025-02-04 03:06:13,197 - INFO - tqdm - perplexity: 40.6440, batch_loss: 4.0684, loss: 3.7049 ||:  63%|######2   | 81/129 [01:44<01:02,  1.30s/it]
2025-02-04 03:06:24,483 - INFO - tqdm - perplexity: 40.4164, batch_loss: 3.6011, loss: 3.6992 ||:  70%|######9   | 90/129 [01:55<00:50,  1.28s/it]
2025-02-04 03:06:35,469 - INFO - tqdm - perplexity: 40.2513, batch_loss: 3.6204, loss: 3.6951 ||:  77%|#######6  | 99/129 [02:06<00:35,  1.17s/it]
2025-02-04 03:06:46,082 - INFO - tqdm - perplexity: 40.1839, batch_loss: 3.9514, loss: 3.6935 ||:  83%|########2 | 107/129 [02:17<00:27,  1.26s/it]
2025-02-04 03:06:56,759 - INFO - tqdm - perplexity: 40.2180, batch_loss: 3.9426, loss: 3.6943 ||:  90%|########9 | 116/129 [02:27<00:15,  1.21s/it]
2025-02-04 03:07:06,832 - INFO - tqdm - perplexity: 39.9878, batch_loss: 3.6342, loss: 3.6886 ||:  96%|#########6| 124/129 [02:38<00:06,  1.28s/it]
2025-02-04 03:07:13,623 - INFO - tqdm - perplexity: 39.8954, batch_loss: 4.1632, loss: 3.6863 ||: 100%|##########| 129/129 [02:44<00:00,  1.34s/it]
2025-02-04 03:07:13,624 - INFO - tqdm - perplexity: 39.8954, batch_loss: 4.1632, loss: 3.6863 ||: 100%|##########| 129/129 [02:44<00:00,  1.28s/it]
2025-02-04 03:07:13,625 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2025-02-04 03:07:13,626 - INFO - allennlp.training.callbacks.console_logger - loss               |     3.711  |     3.686
2025-02-04 03:07:13,626 - INFO - allennlp.training.callbacks.console_logger - perplexity         |    40.893  |    39.895
2025-02-04 03:07:13,626 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |     0.000  |       N/A
2025-02-04 03:07:14,245 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:45:27.285896
2025-02-04 03:07:14,245 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 21:54:33
2025-02-04 03:07:14,245 - INFO - allennlp.training.gradient_descent_trainer - Epoch 3/31
2025-02-04 03:07:14,246 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 0B
2025-02-04 03:07:14,246 - INFO - allennlp.training.gradient_descent_trainer - Training
2025-02-04 03:07:14,246 - INFO - tqdm - 0%|          | 0/516 [00:00<?, ?it/s]
2025-02-04 03:07:28,378 - INFO - tqdm - perplexity: 26.3542, batch_loss: 3.4717, loss: 3.2716 ||:   1%|          | 3/516 [00:14<39:59,  4.68s/it]
2025-02-04 03:07:42,374 - INFO - tqdm - perplexity: 24.9479, batch_loss: 3.2567, loss: 3.2168 ||:   1%|1         | 6/516 [00:28<39:44,  4.68s/it]
2025-02-04 03:07:54,009 - INFO - tqdm - perplexity: 27.2118, batch_loss: 3.4054, loss: 3.3037 ||:   2%|1         | 9/516 [00:39<34:38,  4.10s/it]
2025-02-04 03:08:07,782 - INFO - tqdm - perplexity: 26.9044, batch_loss: 3.4030, loss: 3.2923 ||:   2%|2         | 12/516 [00:53<36:58,  4.40s/it]
2025-02-04 03:08:21,930 - INFO - tqdm - perplexity: 27.1324, batch_loss: 3.1612, loss: 3.3007 ||:   3%|2         | 15/516 [01:07<39:36,  4.74s/it]
2025-02-04 03:08:36,501 - INFO - tqdm - perplexity: 27.4717, batch_loss: 3.3554, loss: 3.3132 ||:   3%|3         | 18/516 [01:22<41:16,  4.97s/it]
2025-02-04 03:08:47,787 - INFO - tqdm - perplexity: 27.2499, batch_loss: 3.2703, loss: 3.3051 ||:   4%|3         | 20/516 [01:33<44:19,  5.36s/it]
2025-02-04 03:08:58,926 - INFO - tqdm - perplexity: 26.6568, batch_loss: 3.0944, loss: 3.2830 ||:   4%|4         | 22/516 [01:44<44:28,  5.40s/it]
2025-02-04 03:09:16,249 - INFO - tqdm - perplexity: 26.4714, batch_loss: 3.1395, loss: 3.2761 ||:   5%|4         | 25/516 [02:02<48:34,  5.94s/it]
2025-02-04 03:09:27,629 - INFO - tqdm - perplexity: 26.3575, batch_loss: 3.0017, loss: 3.2718 ||:   5%|5         | 27/516 [02:13<48:26,  5.94s/it]
2025-02-04 03:09:40,414 - INFO - tqdm - perplexity: 26.2427, batch_loss: 3.1377, loss: 3.2674 ||:   6%|5         | 30/516 [02:26<39:09,  4.83s/it]
2025-02-04 03:09:54,498 - INFO - tqdm - perplexity: 25.9982, batch_loss: 3.0682, loss: 3.2580 ||:   6%|6         | 33/516 [02:40<37:52,  4.70s/it]
2025-02-04 03:10:08,251 - INFO - tqdm - perplexity: 26.0468, batch_loss: 3.2138, loss: 3.2599 ||:   7%|6         | 36/516 [02:54<37:02,  4.63s/it]
2025-02-04 03:10:18,908 - INFO - tqdm - perplexity: 26.1276, batch_loss: 3.4469, loss: 3.2630 ||:   7%|7         | 38/516 [03:04<38:41,  4.86s/it]
2025-02-04 03:10:35,305 - INFO - tqdm - perplexity: 25.8511, batch_loss: 2.9829, loss: 3.2524 ||:   8%|7         | 41/516 [03:21<42:38,  5.39s/it]
2025-02-04 03:10:45,892 - INFO - tqdm - perplexity: 26.1183, batch_loss: 3.6937, loss: 3.2626 ||:   8%|8         | 43/516 [03:31<42:57,  5.45s/it]
2025-02-04 03:10:56,403 - INFO - tqdm - perplexity: 26.0597, batch_loss: 3.3395, loss: 3.2604 ||:   9%|8         | 45/516 [03:42<41:13,  5.25s/it]
2025-02-04 03:11:11,860 - INFO - tqdm - perplexity: 26.1303, batch_loss: 3.4076, loss: 3.2631 ||:   9%|9         | 48/516 [03:57<40:54,  5.24s/it]
2025-02-04 03:11:25,906 - INFO - tqdm - perplexity: 26.0254, batch_loss: 3.4869, loss: 3.2591 ||:  10%|9         | 51/516 [04:11<37:27,  4.83s/it]
2025-02-04 03:11:36,671 - INFO - tqdm - perplexity: 25.9846, batch_loss: 3.1390, loss: 3.2575 ||:  10%|#         | 53/516 [04:22<40:25,  5.24s/it]
2025-02-04 03:11:50,057 - INFO - tqdm - perplexity: 26.0452, batch_loss: 3.2043, loss: 3.2598 ||:  11%|#         | 56/516 [04:35<36:39,  4.78s/it]
2025-02-04 03:12:00,114 - INFO - tqdm - perplexity: 26.3236, batch_loss: 3.5606, loss: 3.2705 ||:  11%|#1        | 58/516 [04:45<38:00,  4.98s/it]
2025-02-04 03:12:15,597 - INFO - tqdm - perplexity: 26.2399, batch_loss: 3.1835, loss: 3.2673 ||:  12%|#1        | 61/516 [05:01<39:43,  5.24s/it]
2025-02-04 03:12:26,299 - INFO - tqdm - perplexity: 26.3055, batch_loss: 3.3966, loss: 3.2698 ||:  12%|#2        | 63/516 [05:12<40:25,  5.36s/it]
2025-02-04 03:12:39,530 - INFO - tqdm - perplexity: 26.2840, batch_loss: 3.3043, loss: 3.2690 ||:  13%|#2        | 66/516 [05:25<35:22,  4.72s/it]
2025-02-04 03:12:49,591 - INFO - tqdm - perplexity: 26.3447, batch_loss: 3.0506, loss: 3.2713 ||:  13%|#3        | 68/516 [05:35<36:46,  4.93s/it]
2025-02-04 03:13:00,957 - INFO - tqdm - perplexity: 26.4470, batch_loss: 3.4540, loss: 3.2751 ||:  14%|#3        | 70/516 [05:46<40:02,  5.39s/it]
2025-02-04 03:13:11,275 - INFO - tqdm - perplexity: 26.3684, batch_loss: 3.2915, loss: 3.2722 ||:  14%|#3        | 72/516 [05:57<38:35,  5.22s/it]
2025-02-04 03:13:25,335 - INFO - tqdm - perplexity: 26.4858, batch_loss: 3.3480, loss: 3.2766 ||:  15%|#4        | 75/516 [06:11<35:02,  4.77s/it]
2025-02-04 03:13:36,497 - INFO - tqdm - perplexity: 26.4128, batch_loss: 3.0981, loss: 3.2738 ||:  15%|#4        | 77/516 [06:22<37:45,  5.16s/it]
2025-02-04 03:13:50,653 - INFO - tqdm - perplexity: 26.3252, batch_loss: 3.2814, loss: 3.2705 ||:  16%|#5        | 80/516 [06:36<34:36,  4.76s/it]
2025-02-04 03:14:01,584 - INFO - tqdm - perplexity: 26.2875, batch_loss: 3.2589, loss: 3.2691 ||:  16%|#5        | 82/516 [06:47<36:22,  5.03s/it]
2025-02-04 03:14:11,790 - INFO - tqdm - perplexity: 26.2289, batch_loss: 3.2145, loss: 3.2669 ||:  16%|#6        | 84/516 [06:57<36:04,  5.01s/it]
2025-02-04 03:14:26,226 - INFO - tqdm - perplexity: 26.1108, batch_loss: 3.1223, loss: 3.2623 ||:  17%|#6        | 87/516 [07:11<34:23,  4.81s/it]
2025-02-04 03:14:37,290 - INFO - tqdm - perplexity: 26.0310, batch_loss: 3.2481, loss: 3.2593 ||:  17%|#7        | 89/516 [07:23<35:58,  5.05s/it]
2025-02-04 03:14:51,331 - INFO - tqdm - perplexity: 26.0561, batch_loss: 3.1809, loss: 3.2603 ||:  18%|#7        | 92/516 [07:37<33:59,  4.81s/it]
2025-02-04 03:15:05,994 - INFO - tqdm - perplexity: 26.0181, batch_loss: 3.1195, loss: 3.2588 ||:  18%|#8        | 95/516 [07:51<34:35,  4.93s/it]
2025-02-04 03:15:19,860 - INFO - tqdm - perplexity: 26.0933, batch_loss: 3.2187, loss: 3.2617 ||:  19%|#8        | 98/516 [08:05<33:45,  4.84s/it]
2025-02-04 03:15:31,518 - INFO - tqdm - perplexity: 26.1681, batch_loss: 3.4585, loss: 3.2645 ||:  19%|#9        | 100/516 [08:17<36:47,  5.31s/it]
2025-02-04 03:15:44,349 - INFO - tqdm - perplexity: 26.1059, batch_loss: 3.2300, loss: 3.2622 ||:  20%|#9        | 103/516 [08:30<32:10,  4.67s/it]
2025-02-04 03:15:58,293 - INFO - tqdm - perplexity: 26.1315, batch_loss: 3.2727, loss: 3.2631 ||:  21%|##        | 106/516 [08:44<31:18,  4.58s/it]
2025-02-04 03:16:12,220 - INFO - tqdm - perplexity: 25.9673, batch_loss: 3.0790, loss: 3.2568 ||:  21%|##1       | 109/516 [08:57<31:05,  4.58s/it]
2025-02-04 03:16:24,389 - INFO - tqdm - perplexity: 26.0040, batch_loss: 3.3259, loss: 3.2582 ||:  22%|##1       | 112/516 [09:10<28:27,  4.23s/it]
2025-02-04 03:16:35,030 - INFO - tqdm - perplexity: 25.9341, batch_loss: 3.1373, loss: 3.2556 ||:  22%|##2       | 114/516 [09:20<31:55,  4.76s/it]
2025-02-04 03:16:49,891 - INFO - tqdm - perplexity: 25.9295, batch_loss: 3.3101, loss: 3.2554 ||:  23%|##2       | 117/516 [09:35<32:38,  4.91s/it]
2025-02-04 03:17:01,444 - INFO - tqdm - perplexity: 25.9231, batch_loss: 3.3732, loss: 3.2551 ||:  23%|##3       | 119/516 [09:47<35:58,  5.44s/it]
2025-02-04 03:17:11,590 - INFO - tqdm - perplexity: 25.8197, batch_loss: 2.9982, loss: 3.2511 ||:  23%|##3       | 121/516 [09:57<34:33,  5.25s/it]
2025-02-04 03:17:21,602 - INFO - tqdm - perplexity: 25.7077, batch_loss: 2.9475, loss: 3.2468 ||:  24%|##3       | 123/516 [10:07<33:34,  5.13s/it]
2025-02-04 03:17:35,193 - INFO - tqdm - perplexity: 25.6692, batch_loss: 3.2705, loss: 3.2453 ||:  24%|##4       | 126/516 [10:20<30:43,  4.73s/it]
2025-02-04 03:17:49,749 - INFO - tqdm - perplexity: 25.7188, batch_loss: 3.2486, loss: 3.2472 ||:  25%|##5       | 129/516 [10:35<31:18,  4.85s/it]
2025-02-04 03:18:03,139 - INFO - tqdm - perplexity: 25.7312, batch_loss: 3.3994, loss: 3.2477 ||:  26%|##5       | 132/516 [10:48<29:08,  4.55s/it]
2025-02-04 03:18:16,072 - INFO - tqdm - perplexity: 25.6804, batch_loss: 3.1326, loss: 3.2457 ||:  26%|##6       | 135/516 [11:01<28:04,  4.42s/it]
2025-02-04 03:18:30,171 - INFO - tqdm - perplexity: 25.5726, batch_loss: 3.0850, loss: 3.2415 ||:  27%|##6       | 138/516 [11:15<29:20,  4.66s/it]
2025-02-04 03:18:45,957 - INFO - tqdm - perplexity: 25.5307, batch_loss: 3.0691, loss: 3.2399 ||:  27%|##7       | 141/516 [11:31<31:58,  5.12s/it]
2025-02-04 03:18:58,357 - INFO - tqdm - perplexity: 25.5007, batch_loss: 3.3044, loss: 3.2387 ||:  28%|##7       | 144/516 [11:44<27:28,  4.43s/it]
2025-02-04 03:19:14,376 - INFO - tqdm - perplexity: 25.4597, batch_loss: 3.2553, loss: 3.2371 ||:  28%|##8       | 147/516 [12:00<32:05,  5.22s/it]
2025-02-04 03:19:27,490 - INFO - tqdm - perplexity: 25.4374, batch_loss: 3.2177, loss: 3.2362 ||:  29%|##9       | 150/516 [12:13<28:24,  4.66s/it]
2025-02-04 03:19:42,904 - INFO - tqdm - perplexity: 25.3943, batch_loss: 3.2962, loss: 3.2345 ||:  30%|##9       | 153/516 [12:28<30:13,  5.00s/it]
2025-02-04 03:19:57,399 - INFO - tqdm - perplexity: 25.2842, batch_loss: 2.9940, loss: 3.2302 ||:  30%|###       | 156/516 [12:43<29:24,  4.90s/it]
2025-02-04 03:20:11,093 - INFO - tqdm - perplexity: 25.2714, batch_loss: 3.2379, loss: 3.2297 ||:  31%|###       | 159/516 [12:56<27:08,  4.56s/it]
2025-02-04 03:20:23,122 - INFO - tqdm - perplexity: 25.1921, batch_loss: 3.0011, loss: 3.2265 ||:  31%|###1      | 161/516 [13:08<31:43,  5.36s/it]
2025-02-04 03:20:38,608 - INFO - tqdm - perplexity: 25.1901, batch_loss: 3.3522, loss: 3.2265 ||:  32%|###1      | 164/516 [13:24<31:44,  5.41s/it]
2025-02-04 03:20:51,881 - INFO - tqdm - perplexity: 25.1980, batch_loss: 3.1159, loss: 3.2268 ||:  32%|###2      | 167/516 [13:37<27:46,  4.77s/it]
2025-02-04 03:21:02,718 - INFO - tqdm - perplexity: 25.1125, batch_loss: 2.9954, loss: 3.2234 ||:  33%|###2      | 169/516 [13:48<29:10,  5.04s/it]
2025-02-04 03:21:16,215 - INFO - tqdm - perplexity: 25.1433, batch_loss: 3.2394, loss: 3.2246 ||:  33%|###3      | 172/516 [14:01<26:59,  4.71s/it]
2025-02-04 03:21:27,378 - INFO - tqdm - perplexity: 25.1015, batch_loss: 2.9087, loss: 3.2229 ||:  34%|###3      | 174/516 [14:13<29:17,  5.14s/it]
2025-02-04 03:21:41,174 - INFO - tqdm - perplexity: 25.0610, batch_loss: 3.1096, loss: 3.2213 ||:  34%|###4      | 177/516 [14:26<27:03,  4.79s/it]
2025-02-04 03:21:52,523 - INFO - tqdm - perplexity: 25.0421, batch_loss: 2.9571, loss: 3.2206 ||:  35%|###4      | 179/516 [14:38<29:22,  5.23s/it]
2025-02-04 03:22:06,420 - INFO - tqdm - perplexity: 24.9873, batch_loss: 3.0666, loss: 3.2184 ||:  35%|###5      | 182/516 [14:52<26:43,  4.80s/it]
2025-02-04 03:22:19,861 - INFO - tqdm - perplexity: 24.9620, batch_loss: 3.1100, loss: 3.2174 ||:  36%|###5      | 185/516 [15:05<25:11,  4.57s/it]
2025-02-04 03:22:36,587 - INFO - tqdm - perplexity: 24.8969, batch_loss: 2.8383, loss: 3.2147 ||:  36%|###6      | 187/516 [15:22<37:36,  6.86s/it]
2025-02-04 03:22:48,233 - INFO - tqdm - perplexity: 24.9040, batch_loss: 3.1419, loss: 3.2150 ||:  37%|###6      | 189/516 [15:33<33:57,  6.23s/it]
2025-02-04 03:23:02,800 - INFO - tqdm - perplexity: 24.8199, batch_loss: 3.0209, loss: 3.2116 ||:  37%|###7      | 192/516 [15:48<29:11,  5.41s/it]
2025-02-04 03:23:13,097 - INFO - tqdm - perplexity: 24.8111, batch_loss: 3.1772, loss: 3.2113 ||:  38%|###7      | 194/516 [15:58<28:25,  5.30s/it]
2025-02-04 03:23:27,644 - INFO - tqdm - perplexity: 24.7627, batch_loss: 3.1140, loss: 3.2093 ||:  38%|###8      | 197/516 [16:13<26:22,  4.96s/it]
2025-02-04 03:23:41,018 - INFO - tqdm - perplexity: 24.7803, batch_loss: 3.4020, loss: 3.2100 ||:  39%|###8      | 200/516 [16:26<24:22,  4.63s/it]
2025-02-04 03:23:57,268 - INFO - tqdm - perplexity: 24.7230, batch_loss: 3.0100, loss: 3.2077 ||:  39%|###9      | 203/516 [16:43<27:19,  5.24s/it]
2025-02-04 03:24:10,878 - INFO - tqdm - perplexity: 24.6809, batch_loss: 3.1521, loss: 3.2060 ||:  40%|###9      | 206/516 [16:56<25:00,  4.84s/it]
2025-02-04 03:24:21,044 - INFO - tqdm - perplexity: 24.6772, batch_loss: 3.2352, loss: 3.2059 ||:  40%|####      | 208/516 [17:06<25:17,  4.93s/it]
2025-02-04 03:24:31,687 - INFO - tqdm - perplexity: 24.6875, batch_loss: 3.2933, loss: 3.2063 ||:  41%|####      | 210/516 [17:17<25:57,  5.09s/it]
2025-02-04 03:24:42,909 - INFO - tqdm - perplexity: 24.6279, batch_loss: 3.1032, loss: 3.2039 ||:  41%|####1     | 212/516 [17:28<26:53,  5.31s/it]
2025-02-04 03:24:56,316 - INFO - tqdm - perplexity: 24.5929, batch_loss: 3.0130, loss: 3.2025 ||:  42%|####1     | 215/516 [17:42<23:39,  4.72s/it]
2025-02-04 03:25:07,262 - INFO - tqdm - perplexity: 24.4986, batch_loss: 2.6568, loss: 3.1986 ||:  42%|####2     | 217/516 [17:53<25:28,  5.11s/it]
2025-02-04 03:25:17,854 - INFO - tqdm - perplexity: 24.4759, batch_loss: 3.2245, loss: 3.1977 ||:  42%|####2     | 219/516 [18:03<25:59,  5.25s/it]
2025-02-04 03:25:33,475 - INFO - tqdm - perplexity: 24.4337, batch_loss: 2.9267, loss: 3.1960 ||:  43%|####3     | 222/516 [18:19<26:15,  5.36s/it]
2025-02-04 03:25:43,599 - INFO - tqdm - perplexity: 24.4329, batch_loss: 3.1579, loss: 3.1959 ||:  43%|####3     | 224/516 [18:29<25:05,  5.16s/it]
2025-02-04 03:25:59,109 - INFO - tqdm - perplexity: 24.3963, batch_loss: 3.0314, loss: 3.1944 ||:  44%|####3     | 227/516 [18:44<25:01,  5.19s/it]
2025-02-04 03:26:13,115 - INFO - tqdm - perplexity: 24.3756, batch_loss: 3.2083, loss: 3.1936 ||:  45%|####4     | 230/516 [18:58<22:42,  4.76s/it]
2025-02-04 03:26:25,133 - INFO - tqdm - perplexity: 24.3592, batch_loss: 3.0319, loss: 3.1929 ||:  45%|####5     | 233/516 [19:10<20:17,  4.30s/it]
2025-02-04 03:26:35,375 - INFO - tqdm - perplexity: 24.3565, batch_loss: 3.0135, loss: 3.1928 ||:  46%|####5     | 235/516 [19:21<22:12,  4.74s/it]
2025-02-04 03:26:49,858 - INFO - tqdm - perplexity: 24.3417, batch_loss: 3.0810, loss: 3.1922 ||:  46%|####6     | 238/516 [19:35<22:26,  4.84s/it]
2025-02-04 03:27:04,243 - INFO - tqdm - perplexity: 24.3220, batch_loss: 2.9535, loss: 3.1914 ||:  47%|####6     | 241/516 [19:49<22:04,  4.82s/it]
2025-02-04 03:27:17,761 - INFO - tqdm - perplexity: 24.2861, batch_loss: 2.9956, loss: 3.1899 ||:  47%|####7     | 244/516 [20:03<21:30,  4.75s/it]
2025-02-04 03:27:27,821 - INFO - tqdm - perplexity: 24.2742, batch_loss: 2.9488, loss: 3.1894 ||:  48%|####7     | 246/516 [20:13<21:58,  4.88s/it]
2025-02-04 03:27:39,910 - INFO - tqdm - perplexity: 24.2527, batch_loss: 3.0846, loss: 3.1885 ||:  48%|####8     | 248/516 [20:25<23:46,  5.32s/it]
2025-02-04 03:27:50,799 - INFO - tqdm - perplexity: 24.2054, batch_loss: 3.0757, loss: 3.1866 ||:  48%|####8     | 250/516 [20:36<23:53,  5.39s/it]
2025-02-04 03:28:01,393 - INFO - tqdm - perplexity: 24.1799, batch_loss: 2.9949, loss: 3.1855 ||:  49%|####8     | 252/516 [20:47<23:24,  5.32s/it]
2025-02-04 03:28:14,349 - INFO - tqdm - perplexity: 24.1475, batch_loss: 3.0750, loss: 3.1842 ||:  49%|####9     | 255/516 [21:00<20:10,  4.64s/it]
2025-02-04 03:28:29,494 - INFO - tqdm - perplexity: 24.0879, batch_loss: 2.8614, loss: 3.1817 ||:  50%|#####     | 258/516 [21:15<21:15,  4.94s/it]
2025-02-04 03:28:43,911 - INFO - tqdm - perplexity: 24.0770, batch_loss: 3.1475, loss: 3.1813 ||:  51%|#####     | 261/516 [21:29<21:32,  5.07s/it]
2025-02-04 03:28:56,068 - INFO - tqdm - perplexity: 24.0691, batch_loss: 3.1069, loss: 3.1809 ||:  51%|#####1    | 264/516 [21:41<18:44,  4.46s/it]
2025-02-04 03:29:10,156 - INFO - tqdm - perplexity: 24.0826, batch_loss: 3.2716, loss: 3.1815 ||:  52%|#####1    | 267/516 [21:55<19:02,  4.59s/it]
2025-02-04 03:29:22,931 - INFO - tqdm - perplexity: 24.0676, batch_loss: 3.2926, loss: 3.1809 ||:  52%|#####2    | 270/516 [22:08<17:43,  4.32s/it]
2025-02-04 03:29:33,571 - INFO - tqdm - perplexity: 24.0218, batch_loss: 2.9434, loss: 3.1790 ||:  53%|#####2    | 272/516 [22:19<19:27,  4.78s/it]
2025-02-04 03:29:44,086 - INFO - tqdm - perplexity: 23.9765, batch_loss: 2.9187, loss: 3.1771 ||:  53%|#####3    | 274/516 [22:29<20:10,  5.00s/it]
2025-02-04 03:29:58,787 - INFO - tqdm - perplexity: 23.9446, batch_loss: 2.8891, loss: 3.1757 ||:  54%|#####3    | 277/516 [22:44<19:44,  4.96s/it]
2025-02-04 03:30:10,089 - INFO - tqdm - perplexity: 23.9014, batch_loss: 2.9880, loss: 3.1739 ||:  54%|#####4    | 279/516 [22:55<20:41,  5.24s/it]
2025-02-04 03:30:20,418 - INFO - tqdm - perplexity: 23.8694, batch_loss: 3.0074, loss: 3.1726 ||:  54%|#####4    | 281/516 [23:06<20:15,  5.17s/it]
2025-02-04 03:30:33,923 - INFO - tqdm - perplexity: 23.8364, batch_loss: 3.0510, loss: 3.1712 ||:  55%|#####5    | 284/516 [23:19<18:24,  4.76s/it]
2025-02-04 03:30:46,658 - INFO - tqdm - perplexity: 23.8449, batch_loss: 2.9811, loss: 3.1716 ||:  56%|#####5    | 287/516 [23:32<17:03,  4.47s/it]
2025-02-04 03:31:00,902 - INFO - tqdm - perplexity: 23.7903, batch_loss: 2.9733, loss: 3.1693 ||:  56%|#####6    | 290/516 [23:46<17:18,  4.59s/it]
2025-02-04 03:31:14,144 - INFO - tqdm - perplexity: 23.7856, batch_loss: 3.4134, loss: 3.1691 ||:  57%|#####6    | 293/516 [23:59<16:28,  4.43s/it]
2025-02-04 03:31:27,611 - INFO - tqdm - perplexity: 23.7720, batch_loss: 3.3569, loss: 3.1685 ||:  57%|#####7    | 296/516 [24:13<16:32,  4.51s/it]
2025-02-04 03:31:38,098 - INFO - tqdm - perplexity: 23.7683, batch_loss: 2.9723, loss: 3.1684 ||:  58%|#####7    | 298/516 [24:23<18:11,  5.00s/it]
2025-02-04 03:31:51,880 - INFO - tqdm - perplexity: 23.7382, batch_loss: 2.9663, loss: 3.1671 ||:  58%|#####8    | 301/516 [24:37<16:29,  4.60s/it]
2025-02-04 03:32:06,309 - INFO - tqdm - perplexity: 23.7264, batch_loss: 3.1628, loss: 3.1666 ||:  59%|#####8    | 304/516 [24:52<16:56,  4.79s/it]
2025-02-04 03:32:20,568 - INFO - tqdm - perplexity: 23.6712, batch_loss: 2.9239, loss: 3.1643 ||:  59%|#####9    | 307/516 [25:06<16:49,  4.83s/it]
2025-02-04 03:32:31,421 - INFO - tqdm - perplexity: 23.6290, batch_loss: 3.0099, loss: 3.1625 ||:  60%|#####9    | 309/516 [25:17<17:38,  5.12s/it]
2025-02-04 03:32:41,517 - INFO - tqdm - perplexity: 23.5943, batch_loss: 2.9220, loss: 3.1610 ||:  60%|######    | 311/516 [25:27<17:19,  5.07s/it]
2025-02-04 03:32:52,559 - INFO - tqdm - perplexity: 23.5589, batch_loss: 2.9562, loss: 3.1595 ||:  61%|######    | 313/516 [25:38<17:43,  5.24s/it]
2025-02-04 03:33:04,652 - INFO - tqdm - perplexity: 23.5485, batch_loss: 3.3055, loss: 3.1591 ||:  61%|######1   | 316/516 [25:50<14:39,  4.40s/it]
2025-02-04 03:33:19,163 - INFO - tqdm - perplexity: 23.5245, batch_loss: 3.1413, loss: 3.1580 ||:  62%|######1   | 319/516 [26:04<15:44,  4.79s/it]
2025-02-04 03:33:32,734 - INFO - tqdm - perplexity: 23.4807, batch_loss: 2.9919, loss: 3.1562 ||:  62%|######2   | 322/516 [26:18<14:43,  4.56s/it]
2025-02-04 03:33:48,405 - INFO - tqdm - perplexity: 23.4346, batch_loss: 2.6673, loss: 3.1542 ||:  63%|######2   | 325/516 [26:34<16:27,  5.17s/it]
2025-02-04 03:34:01,452 - INFO - tqdm - perplexity: 23.4100, batch_loss: 2.8473, loss: 3.1532 ||:  64%|######3   | 328/516 [26:47<14:34,  4.65s/it]
2025-02-04 03:34:14,315 - INFO - tqdm - perplexity: 23.3820, batch_loss: 2.9928, loss: 3.1520 ||:  64%|######3   | 330/516 [27:00<17:12,  5.55s/it]
2025-02-04 03:34:24,434 - INFO - tqdm - perplexity: 23.3331, batch_loss: 2.6529, loss: 3.1499 ||:  64%|######4   | 332/516 [27:10<16:32,  5.39s/it]
2025-02-04 03:34:38,991 - INFO - tqdm - perplexity: 23.3059, batch_loss: 2.9959, loss: 3.1487 ||:  65%|######4   | 335/516 [27:24<15:28,  5.13s/it]
2025-02-04 03:34:50,387 - INFO - tqdm - perplexity: 23.2819, batch_loss: 3.1584, loss: 3.1477 ||:  65%|######5   | 337/516 [27:36<16:01,  5.37s/it]
2025-02-04 03:35:04,789 - INFO - tqdm - perplexity: 23.2484, batch_loss: 3.1370, loss: 3.1462 ||:  66%|######5   | 340/516 [27:50<15:20,  5.23s/it]
2025-02-04 03:35:15,513 - INFO - tqdm - perplexity: 23.2182, batch_loss: 2.9870, loss: 3.1449 ||:  66%|######6   | 342/516 [28:01<15:13,  5.25s/it]
2025-02-04 03:35:30,332 - INFO - tqdm - perplexity: 23.1969, batch_loss: 3.1249, loss: 3.1440 ||:  67%|######6   | 345/516 [28:16<14:25,  5.06s/it]
2025-02-04 03:35:40,447 - INFO - tqdm - perplexity: 23.1730, batch_loss: 3.0244, loss: 3.1430 ||:  67%|######7   | 347/516 [28:26<14:19,  5.09s/it]
2025-02-04 03:35:54,032 - INFO - tqdm - perplexity: 23.1655, batch_loss: 2.9010, loss: 3.1427 ||:  68%|######7   | 350/516 [28:39<13:08,  4.75s/it]
2025-02-04 03:36:06,007 - INFO - tqdm - perplexity: 23.1616, batch_loss: 3.2776, loss: 3.1425 ||:  68%|######8   | 353/516 [28:51<11:31,  4.24s/it]
2025-02-04 03:36:17,656 - INFO - tqdm - perplexity: 23.1177, batch_loss: 2.7998, loss: 3.1406 ||:  69%|######8   | 355/516 [29:03<13:24,  5.00s/it]
2025-02-04 03:36:34,431 - INFO - tqdm - perplexity: 23.0742, batch_loss: 2.7162, loss: 3.1387 ||:  69%|######9   | 358/516 [29:20<14:33,  5.53s/it]
2025-02-04 03:36:46,879 - INFO - tqdm - perplexity: 23.0585, batch_loss: 2.9047, loss: 3.1380 ||:  70%|######9   | 360/516 [29:32<15:38,  6.02s/it]
2025-02-04 03:37:02,061 - INFO - tqdm - perplexity: 23.0459, batch_loss: 3.0075, loss: 3.1375 ||:  70%|#######   | 363/516 [29:47<13:53,  5.45s/it]
2025-02-04 03:37:14,057 - INFO - tqdm - perplexity: 23.0019, batch_loss: 2.7032, loss: 3.1356 ||:  71%|#######   | 365/516 [29:59<14:25,  5.73s/it]
2025-02-04 03:37:27,577 - INFO - tqdm - perplexity: 22.9757, batch_loss: 2.9868, loss: 3.1344 ||:  71%|#######1  | 368/516 [30:13<12:07,  4.91s/it]
2025-02-04 03:37:40,355 - INFO - tqdm - perplexity: 22.9333, batch_loss: 3.0734, loss: 3.1326 ||:  72%|#######1  | 371/516 [30:26<10:26,  4.32s/it]
2025-02-04 03:37:50,659 - INFO - tqdm - perplexity: 22.8886, batch_loss: 2.7492, loss: 3.1306 ||:  72%|#######2  | 373/516 [30:36<11:21,  4.76s/it]
2025-02-04 03:38:02,179 - INFO - tqdm - perplexity: 22.8751, batch_loss: 3.0092, loss: 3.1300 ||:  73%|#######2  | 375/516 [30:47<12:26,  5.30s/it]
2025-02-04 03:38:15,432 - INFO - tqdm - perplexity: 22.8500, batch_loss: 2.9319, loss: 3.1290 ||:  73%|#######3  | 378/516 [31:01<10:47,  4.69s/it]
2025-02-04 03:38:28,736 - INFO - tqdm - perplexity: 22.8435, batch_loss: 2.8390, loss: 3.1287 ||:  74%|#######3  | 381/516 [31:14<10:14,  4.55s/it]
2025-02-04 03:38:43,670 - INFO - tqdm - perplexity: 22.8152, batch_loss: 2.9425, loss: 3.1274 ||:  74%|#######4  | 384/516 [31:29<10:43,  4.87s/it]
2025-02-04 03:38:58,363 - INFO - tqdm - perplexity: 22.7732, batch_loss: 2.8851, loss: 3.1256 ||:  75%|#######5  | 387/516 [31:44<10:48,  5.02s/it]
2025-02-04 03:39:13,018 - INFO - tqdm - perplexity: 22.7566, batch_loss: 3.0863, loss: 3.1249 ||:  76%|#######5  | 390/516 [31:58<10:29,  4.99s/it]
2025-02-04 03:39:26,446 - INFO - tqdm - perplexity: 22.7208, batch_loss: 2.8869, loss: 3.1233 ||:  76%|#######6  | 393/516 [32:12<09:37,  4.70s/it]
2025-02-04 03:39:41,718 - INFO - tqdm - perplexity: 22.6876, batch_loss: 2.9045, loss: 3.1218 ||:  77%|#######6  | 396/516 [32:27<10:13,  5.11s/it]
2025-02-04 03:39:54,599 - INFO - tqdm - perplexity: 22.6579, batch_loss: 2.9945, loss: 3.1205 ||:  77%|#######7  | 399/516 [32:40<09:03,  4.64s/it]
2025-02-04 03:40:04,991 - INFO - tqdm - perplexity: 22.6328, batch_loss: 2.9161, loss: 3.1194 ||:  78%|#######7  | 401/516 [32:50<09:29,  4.95s/it]
2025-02-04 03:40:17,608 - INFO - tqdm - perplexity: 22.5778, batch_loss: 2.5255, loss: 3.1170 ||:  78%|#######8  | 403/516 [33:03<10:38,  5.65s/it]
2025-02-04 03:40:28,161 - INFO - tqdm - perplexity: 22.5459, batch_loss: 2.8565, loss: 3.1156 ||:  78%|#######8  | 405/516 [33:13<10:12,  5.52s/it]
2025-02-04 03:40:41,743 - INFO - tqdm - perplexity: 22.4892, batch_loss: 2.7754, loss: 3.1130 ||:  79%|#######9  | 408/516 [33:27<08:38,  4.80s/it]
2025-02-04 03:40:57,069 - INFO - tqdm - perplexity: 22.4566, batch_loss: 2.7637, loss: 3.1116 ||:  80%|#######9  | 411/516 [33:42<08:49,  5.05s/it]
2025-02-04 03:41:11,371 - INFO - tqdm - perplexity: 22.4401, batch_loss: 2.8419, loss: 3.1108 ||:  80%|########  | 414/516 [33:57<08:19,  4.90s/it]
2025-02-04 03:41:21,610 - INFO - tqdm - perplexity: 22.4285, batch_loss: 3.1548, loss: 3.1103 ||:  81%|########  | 416/516 [34:07<08:25,  5.06s/it]
2025-02-04 03:41:32,809 - INFO - tqdm - perplexity: 22.4008, batch_loss: 2.7877, loss: 3.1091 ||:  81%|########1 | 418/516 [34:18<08:45,  5.37s/it]
2025-02-04 03:41:48,800 - INFO - tqdm - perplexity: 22.3816, batch_loss: 2.8446, loss: 3.1082 ||:  82%|########1 | 421/516 [34:34<08:44,  5.52s/it]
2025-02-04 03:42:01,375 - INFO - tqdm - perplexity: 22.3507, batch_loss: 2.7590, loss: 3.1069 ||:  82%|########1 | 423/516 [34:47<09:04,  5.86s/it]
2025-02-04 03:42:17,534 - INFO - tqdm - perplexity: 22.3019, batch_loss: 2.5597, loss: 3.1047 ||:  83%|########2 | 426/516 [35:03<08:34,  5.71s/it]
2025-02-04 03:42:27,753 - INFO - tqdm - perplexity: 22.2695, batch_loss: 2.6919, loss: 3.1032 ||:  83%|########2 | 428/516 [35:13<08:03,  5.50s/it]
2025-02-04 03:42:38,117 - INFO - tqdm - perplexity: 22.2525, batch_loss: 3.0495, loss: 3.1025 ||:  83%|########3 | 430/516 [35:23<07:38,  5.33s/it]
2025-02-04 03:42:52,572 - INFO - tqdm - perplexity: 22.2437, batch_loss: 3.2025, loss: 3.1021 ||:  84%|########3 | 433/516 [35:38<06:59,  5.05s/it]
2025-02-04 03:43:04,218 - INFO - tqdm - perplexity: 22.2313, batch_loss: 2.8761, loss: 3.1015 ||:  84%|########4 | 435/516 [35:49<07:12,  5.34s/it]
2025-02-04 03:43:16,616 - INFO - tqdm - perplexity: 22.2003, batch_loss: 2.7388, loss: 3.1001 ||:  85%|########4 | 437/516 [36:02<07:38,  5.80s/it]
2025-02-04 03:43:30,151 - INFO - tqdm - perplexity: 22.1631, batch_loss: 2.9449, loss: 3.0984 ||:  85%|########5 | 440/516 [36:15<06:13,  4.92s/it]
2025-02-04 03:43:44,948 - INFO - tqdm - perplexity: 22.1243, batch_loss: 2.9539, loss: 3.0967 ||:  86%|########5 | 443/516 [36:30<06:16,  5.16s/it]
2025-02-04 03:43:59,219 - INFO - tqdm - perplexity: 22.1032, batch_loss: 3.1246, loss: 3.0957 ||:  86%|########6 | 446/516 [36:44<05:40,  4.86s/it]
2025-02-04 03:44:13,339 - INFO - tqdm - perplexity: 22.0842, batch_loss: 2.9528, loss: 3.0949 ||:  87%|########7 | 449/516 [36:59<05:21,  4.80s/it]
2025-02-04 03:44:27,908 - INFO - tqdm - perplexity: 22.0660, batch_loss: 2.8793, loss: 3.0940 ||:  88%|########7 | 452/516 [37:13<05:08,  4.82s/it]
2025-02-04 03:44:42,982 - INFO - tqdm - perplexity: 22.0299, batch_loss: 2.6992, loss: 3.0924 ||:  88%|########8 | 455/516 [37:28<05:03,  4.97s/it]
2025-02-04 03:44:54,181 - INFO - tqdm - perplexity: 21.9801, batch_loss: 2.7157, loss: 3.0901 ||:  89%|########8 | 457/516 [37:39<05:06,  5.19s/it]
2025-02-04 03:45:06,632 - INFO - tqdm - perplexity: 21.9607, batch_loss: 2.7774, loss: 3.0893 ||:  89%|########9 | 460/516 [37:52<04:18,  4.62s/it]
2025-02-04 03:45:22,108 - INFO - tqdm - perplexity: 21.9435, batch_loss: 2.9156, loss: 3.0885 ||:  90%|########9 | 463/516 [38:07<04:24,  4.99s/it]
2025-02-04 03:45:35,798 - INFO - tqdm - perplexity: 21.9201, batch_loss: 2.7977, loss: 3.0874 ||:  90%|######### | 466/516 [38:21<03:50,  4.61s/it]
2025-02-04 03:45:50,956 - INFO - tqdm - perplexity: 21.8963, batch_loss: 3.0057, loss: 3.0863 ||:  91%|######### | 469/516 [38:36<03:53,  4.96s/it]
2025-02-04 03:46:03,720 - INFO - tqdm - perplexity: 21.8865, batch_loss: 3.2939, loss: 3.0859 ||:  91%|#########1| 472/516 [38:49<03:19,  4.54s/it]
2025-02-04 03:46:17,848 - INFO - tqdm - perplexity: 21.8761, batch_loss: 2.9316, loss: 3.0854 ||:  92%|#########2| 475/516 [39:03<03:13,  4.71s/it]
2025-02-04 03:46:30,782 - INFO - tqdm - perplexity: 21.8553, batch_loss: 3.1482, loss: 3.0844 ||:  93%|#########2| 478/516 [39:16<02:46,  4.39s/it]
2025-02-04 03:46:41,943 - INFO - tqdm - perplexity: 21.8138, batch_loss: 2.7344, loss: 3.0825 ||:  93%|#########3| 480/516 [39:27<02:55,  4.88s/it]
2025-02-04 03:46:55,887 - INFO - tqdm - perplexity: 21.7898, batch_loss: 2.8330, loss: 3.0814 ||:  94%|#########3| 483/516 [39:41<02:36,  4.75s/it]
2025-02-04 03:47:09,460 - INFO - tqdm - perplexity: 21.7820, batch_loss: 2.8521, loss: 3.0811 ||:  94%|#########4| 486/516 [39:55<02:15,  4.52s/it]
2025-02-04 03:47:22,006 - INFO - tqdm - perplexity: 21.7516, batch_loss: 2.7165, loss: 3.0797 ||:  95%|#########4| 489/516 [40:07<01:56,  4.31s/it]
2025-02-04 03:47:36,872 - INFO - tqdm - perplexity: 21.7140, batch_loss: 2.6387, loss: 3.0780 ||:  95%|#########5| 492/516 [40:22<01:54,  4.77s/it]
2025-02-04 03:47:50,133 - INFO - tqdm - perplexity: 21.7032, batch_loss: 2.9411, loss: 3.0775 ||:  96%|#########5| 495/516 [40:35<01:33,  4.43s/it]
2025-02-04 03:48:02,917 - INFO - tqdm - perplexity: 21.6934, batch_loss: 3.0521, loss: 3.0770 ||:  97%|#########6| 498/516 [40:48<01:17,  4.32s/it]
2025-02-04 03:48:15,172 - INFO - tqdm - perplexity: 21.6634, batch_loss: 2.7305, loss: 3.0756 ||:  97%|#########7| 501/516 [41:00<01:02,  4.17s/it]
2025-02-04 03:48:28,883 - INFO - tqdm - perplexity: 21.6234, batch_loss: 2.5530, loss: 3.0738 ||:  97%|#########7| 503/516 [41:14<01:11,  5.50s/it]
2025-02-04 03:48:41,179 - INFO - tqdm - perplexity: 21.5995, batch_loss: 2.8967, loss: 3.0727 ||:  98%|#########7| 505/516 [41:26<01:05,  5.98s/it]
2025-02-04 03:48:51,704 - INFO - tqdm - perplexity: 21.5696, batch_loss: 2.8474, loss: 3.0713 ||:  98%|#########8| 507/516 [41:37<00:50,  5.66s/it]
2025-02-04 03:49:06,221 - INFO - tqdm - perplexity: 21.5481, batch_loss: 2.7573, loss: 3.0703 ||:  99%|#########8| 510/516 [41:51<00:31,  5.23s/it]
2025-02-04 03:49:19,473 - INFO - tqdm - perplexity: 21.5398, batch_loss: 2.9691, loss: 3.0699 ||:  99%|#########9| 513/516 [42:05<00:13,  4.65s/it]
2025-02-04 03:49:24,324 - INFO - tqdm - perplexity: 21.5162, batch_loss: 2.5058, loss: 3.0688 ||: 100%|#########9| 514/516 [42:10<00:09,  4.71s/it]
2025-02-04 03:49:28,362 - INFO - tqdm - perplexity: 21.4974, batch_loss: 2.6188, loss: 3.0679 ||: 100%|#########9| 515/516 [42:14<00:04,  4.51s/it]
2025-02-04 03:49:28,905 - INFO - tqdm - perplexity: 21.4763, batch_loss: 2.5614, loss: 3.0670 ||: 100%|##########| 516/516 [42:14<00:00,  3.32s/it]
2025-02-04 03:49:28,906 - INFO - tqdm - perplexity: 21.4763, batch_loss: 2.5614, loss: 3.0670 ||: 100%|##########| 516/516 [42:14<00:00,  4.91s/it]
2025-02-04 03:49:28,907 - INFO - allennlp.training.gradient_descent_trainer - Validating
2025-02-04 03:49:28,907 - INFO - tqdm - 0%|          | 0/129 [00:00<?, ?it/s]
2025-02-04 03:49:39,722 - INFO - tqdm - perplexity: 28.9511, batch_loss: 3.9761, loss: 3.3656 ||:   6%|6         | 8/129 [00:10<02:56,  1.46s/it]
2025-02-04 03:49:50,775 - INFO - tqdm - perplexity: 27.4404, batch_loss: 3.2950, loss: 3.3120 ||:  13%|#3        | 17/129 [00:21<02:23,  1.28s/it]
2025-02-04 03:50:01,767 - INFO - tqdm - perplexity: 26.9413, batch_loss: 3.4778, loss: 3.2937 ||:  20%|##        | 26/129 [00:32<02:04,  1.21s/it]
2025-02-04 03:50:11,793 - INFO - tqdm - perplexity: 26.2153, batch_loss: 3.0226, loss: 3.2663 ||:  26%|##6       | 34/129 [00:42<02:05,  1.32s/it]
2025-02-04 03:50:23,131 - INFO - tqdm - perplexity: 26.1064, batch_loss: 2.9588, loss: 3.2622 ||:  33%|###3      | 43/129 [00:54<01:53,  1.32s/it]
2025-02-04 03:50:33,479 - INFO - tqdm - perplexity: 26.7112, batch_loss: 4.2583, loss: 3.2851 ||:  40%|###9      | 51/129 [01:04<01:38,  1.26s/it]
2025-02-04 03:50:44,258 - INFO - tqdm - perplexity: 25.9660, batch_loss: 3.4474, loss: 3.2568 ||:  47%|####6     | 60/129 [01:15<01:20,  1.17s/it]
2025-02-04 03:50:55,227 - INFO - tqdm - perplexity: 25.7687, batch_loss: 2.8694, loss: 3.2492 ||:  53%|#####3    | 69/129 [01:26<01:16,  1.27s/it]
2025-02-04 03:51:05,520 - INFO - tqdm - perplexity: 25.5820, batch_loss: 2.8204, loss: 3.2419 ||:  60%|#####9    | 77/129 [01:36<01:05,  1.27s/it]
2025-02-04 03:51:16,293 - INFO - tqdm - perplexity: 26.0171, batch_loss: 3.7353, loss: 3.2588 ||:  67%|######6   | 86/129 [01:47<00:50,  1.18s/it]
2025-02-04 03:51:26,802 - INFO - tqdm - perplexity: 26.0636, batch_loss: 3.1873, loss: 3.2605 ||:  73%|#######2  | 94/129 [01:57<00:44,  1.26s/it]
2025-02-04 03:51:38,127 - INFO - tqdm - perplexity: 25.9966, batch_loss: 2.9786, loss: 3.2580 ||:  80%|#######9  | 103/129 [02:09<00:32,  1.27s/it]
2025-02-04 03:51:48,666 - INFO - tqdm - perplexity: 26.1004, batch_loss: 3.4044, loss: 3.2620 ||:  86%|########6 | 111/129 [02:19<00:24,  1.36s/it]
2025-02-04 03:51:58,667 - INFO - tqdm - perplexity: 26.0286, batch_loss: 3.3596, loss: 3.2592 ||:  92%|#########2| 119/129 [02:29<00:12,  1.27s/it]
2025-02-04 03:52:09,013 - INFO - tqdm - perplexity: 25.9610, batch_loss: 3.2581, loss: 3.2566 ||:  98%|#########8| 127/129 [02:40<00:02,  1.29s/it]
2025-02-04 03:52:11,371 - INFO - tqdm - perplexity: 25.8028, batch_loss: 2.7307, loss: 3.2505 ||: 100%|##########| 129/129 [02:42<00:00,  1.24s/it]
2025-02-04 03:52:11,371 - INFO - tqdm - perplexity: 25.8028, batch_loss: 2.7307, loss: 3.2505 ||: 100%|##########| 129/129 [02:42<00:00,  1.26s/it]
2025-02-04 03:52:11,373 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2025-02-04 03:52:11,373 - INFO - allennlp.training.callbacks.console_logger - loss               |     3.067  |     3.250
2025-02-04 03:52:11,374 - INFO - allennlp.training.callbacks.console_logger - perplexity         |    21.476  |    25.803
2025-02-04 03:52:11,374 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |     0.000  |       N/A
2025-02-04 03:52:11,986 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:44:57.740584
2025-02-04 03:52:11,987 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 21:06:39
2025-02-04 03:52:11,987 - INFO - allennlp.training.gradient_descent_trainer - Epoch 4/31
2025-02-04 03:52:11,987 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 0B
2025-02-04 03:52:11,988 - INFO - allennlp.training.gradient_descent_trainer - Training
2025-02-04 03:52:11,988 - INFO - tqdm - 0%|          | 0/516 [00:00<?, ?it/s]
2025-02-04 03:52:23,954 - INFO - tqdm - perplexity: 17.3834, batch_loss: 2.7262, loss: 2.8555 ||:   0%|          | 2/516 [00:11<48:44,  5.69s/it]
2025-02-04 03:52:38,151 - INFO - tqdm - perplexity: 16.3823, batch_loss: 2.7085, loss: 2.7962 ||:   1%|          | 5/516 [00:26<42:08,  4.95s/it]
2025-02-04 03:52:48,358 - INFO - tqdm - perplexity: 15.5850, batch_loss: 2.6002, loss: 2.7463 ||:   1%|1         | 7/516 [00:36<43:44,  5.16s/it]
2025-02-04 03:53:03,258 - INFO - tqdm - perplexity: 15.6538, batch_loss: 2.7491, loss: 2.7507 ||:   2%|1         | 10/516 [00:51<42:37,  5.05s/it]
2025-02-04 03:53:16,747 - INFO - tqdm - perplexity: 15.7083, batch_loss: 2.7535, loss: 2.7542 ||:   3%|2         | 13/516 [01:04<38:49,  4.63s/it]
2025-02-04 03:53:31,061 - INFO - tqdm - perplexity: 15.0015, batch_loss: 2.3513, loss: 2.7082 ||:   3%|2         | 15/516 [01:19<49:32,  5.93s/it]
2025-02-04 03:53:42,054 - INFO - tqdm - perplexity: 15.2096, batch_loss: 2.9192, loss: 2.7219 ||:   3%|3         | 17/516 [01:30<47:07,  5.67s/it]
2025-02-04 03:53:56,331 - INFO - tqdm - perplexity: 15.2993, batch_loss: 2.5894, loss: 2.7278 ||:   4%|3         | 20/516 [01:44<42:23,  5.13s/it]
2025-02-04 03:54:09,769 - INFO - tqdm - perplexity: 15.3462, batch_loss: 2.4992, loss: 2.7309 ||:   4%|4         | 23/516 [01:57<38:07,  4.64s/it]
2025-02-04 03:54:23,217 - INFO - tqdm - perplexity: 15.2509, batch_loss: 2.5949, loss: 2.7246 ||:   5%|5         | 26/516 [02:11<36:37,  4.49s/it]
2025-02-04 03:54:33,935 - INFO - tqdm - perplexity: 15.0467, batch_loss: 2.6142, loss: 2.7112 ||:   5%|5         | 28/516 [02:21<39:10,  4.82s/it]
2025-02-04 03:54:46,715 - INFO - tqdm - perplexity: 14.8065, batch_loss: 2.3126, loss: 2.6951 ||:   6%|5         | 30/516 [02:34<45:11,  5.58s/it]
2025-02-04 03:54:59,532 - INFO - tqdm - perplexity: 14.6140, batch_loss: 2.6240, loss: 2.6820 ||:   6%|6         | 33/516 [02:47<38:02,  4.73s/it]
2025-02-04 03:55:13,985 - INFO - tqdm - perplexity: 14.5299, batch_loss: 2.6302, loss: 2.6762 ||:   7%|6         | 36/516 [03:01<38:11,  4.77s/it]
2025-02-04 03:55:28,223 - INFO - tqdm - perplexity: 14.5522, batch_loss: 2.6037, loss: 2.6777 ||:   8%|7         | 39/516 [03:16<37:38,  4.74s/it]
2025-02-04 03:55:39,335 - INFO - tqdm - perplexity: 14.5551, batch_loss: 2.8886, loss: 2.6779 ||:   8%|7         | 41/516 [03:27<40:07,  5.07s/it]
2025-02-04 03:55:52,341 - INFO - tqdm - perplexity: 14.5905, batch_loss: 2.6645, loss: 2.6804 ||:   9%|8         | 44/516 [03:40<36:11,  4.60s/it]
2025-02-04 03:56:04,917 - INFO - tqdm - perplexity: 14.5067, batch_loss: 2.5953, loss: 2.6746 ||:   9%|9         | 47/516 [03:52<33:38,  4.30s/it]
2025-02-04 03:56:18,018 - INFO - tqdm - perplexity: 14.5037, batch_loss: 2.7264, loss: 2.6744 ||:  10%|9         | 50/516 [04:06<33:31,  4.32s/it]
2025-02-04 03:56:28,219 - INFO - tqdm - perplexity: 14.4570, batch_loss: 2.5795, loss: 2.6712 ||:  10%|#         | 52/516 [04:16<36:56,  4.78s/it]
2025-02-04 03:56:41,807 - INFO - tqdm - perplexity: 14.4872, batch_loss: 2.5310, loss: 2.6733 ||:  11%|#         | 55/516 [04:29<35:47,  4.66s/it]
2025-02-04 03:56:53,111 - INFO - tqdm - perplexity: 14.5173, batch_loss: 2.7520, loss: 2.6753 ||:  11%|#1        | 57/516 [04:41<40:12,  5.26s/it]
2025-02-04 03:57:06,645 - INFO - tqdm - perplexity: 14.5398, batch_loss: 2.6568, loss: 2.6769 ||:  12%|#1        | 60/516 [04:54<36:56,  4.86s/it]
2025-02-04 03:57:19,538 - INFO - tqdm - perplexity: 14.5724, batch_loss: 2.7504, loss: 2.6791 ||:  12%|#2        | 63/516 [05:07<33:44,  4.47s/it]
2025-02-04 03:57:32,361 - INFO - tqdm - perplexity: 14.5792, batch_loss: 2.7661, loss: 2.6796 ||:  13%|#2        | 66/516 [05:20<32:58,  4.40s/it]
2025-02-04 03:57:46,866 - INFO - tqdm - perplexity: 14.4737, batch_loss: 2.6079, loss: 2.6723 ||:  13%|#3        | 69/516 [05:34<34:41,  4.66s/it]
2025-02-04 03:57:57,430 - INFO - tqdm - perplexity: 14.4543, batch_loss: 2.7332, loss: 2.6710 ||:  14%|#3        | 71/516 [05:45<36:24,  4.91s/it]
2025-02-04 03:58:08,181 - INFO - tqdm - perplexity: 14.4465, batch_loss: 2.8018, loss: 2.6705 ||:  14%|#4        | 73/516 [05:56<37:14,  5.04s/it]
2025-02-04 03:58:19,653 - INFO - tqdm - perplexity: 14.4264, batch_loss: 2.5107, loss: 2.6691 ||:  15%|#4        | 75/516 [06:07<40:24,  5.50s/it]
2025-02-04 03:58:34,516 - INFO - tqdm - perplexity: 14.4638, batch_loss: 2.6756, loss: 2.6717 ||:  15%|#5        | 78/516 [06:22<37:54,  5.19s/it]
2025-02-04 03:58:44,770 - INFO - tqdm - perplexity: 14.4213, batch_loss: 2.7129, loss: 2.6687 ||:  16%|#5        | 80/516 [06:32<37:11,  5.12s/it]
2025-02-04 03:58:55,452 - INFO - tqdm - perplexity: 14.3944, batch_loss: 2.5963, loss: 2.6668 ||:  16%|#5        | 82/516 [06:43<38:17,  5.29s/it]
2025-02-04 03:59:06,241 - INFO - tqdm - perplexity: 14.3871, batch_loss: 2.6103, loss: 2.6663 ||:  16%|#6        | 84/516 [06:54<38:48,  5.39s/it]
2025-02-04 03:59:20,366 - INFO - tqdm - perplexity: 14.3730, batch_loss: 2.5743, loss: 2.6654 ||:  17%|#6        | 87/516 [07:08<34:51,  4.87s/it]
2025-02-04 03:59:34,086 - INFO - tqdm - perplexity: 14.3335, batch_loss: 2.6192, loss: 2.6626 ||:  17%|#7        | 90/516 [07:22<33:20,  4.70s/it]
2025-02-04 03:59:47,818 - INFO - tqdm - perplexity: 14.2990, batch_loss: 2.5445, loss: 2.6602 ||:  18%|#8        | 93/516 [07:35<32:17,  4.58s/it]
2025-02-04 04:00:01,042 - INFO - tqdm - perplexity: 14.2819, batch_loss: 2.8415, loss: 2.6590 ||:  19%|#8        | 96/516 [07:49<31:00,  4.43s/it]
2025-02-04 04:00:16,141 - INFO - tqdm - perplexity: 14.2888, batch_loss: 2.6592, loss: 2.6595 ||:  19%|#9        | 99/516 [08:04<33:48,  4.86s/it]
2025-02-04 04:00:26,651 - INFO - tqdm - perplexity: 14.3055, batch_loss: 2.6893, loss: 2.6606 ||:  20%|#9        | 101/516 [08:14<34:13,  4.95s/it]
2025-02-04 04:00:41,090 - INFO - tqdm - perplexity: 14.2853, batch_loss: 2.7535, loss: 2.6592 ||:  20%|##        | 104/516 [08:29<33:39,  4.90s/it]
2025-02-04 04:00:51,443 - INFO - tqdm - perplexity: 14.2617, batch_loss: 2.5889, loss: 2.6576 ||:  21%|##        | 106/516 [08:39<34:40,  5.07s/it]
2025-02-04 04:01:08,962 - INFO - tqdm - perplexity: 14.2479, batch_loss: 2.3814, loss: 2.6566 ||:  21%|##1       | 109/516 [08:56<39:28,  5.82s/it]
2025-02-04 04:01:20,093 - INFO - tqdm - perplexity: 14.2280, batch_loss: 2.7602, loss: 2.6552 ||:  22%|##1       | 111/516 [09:08<37:54,  5.62s/it]
2025-02-04 04:01:30,643 - INFO - tqdm - perplexity: 14.1951, batch_loss: 2.6283, loss: 2.6529 ||:  22%|##1       | 113/516 [09:18<35:47,  5.33s/it]
2025-02-04 04:01:45,285 - INFO - tqdm - perplexity: 14.2184, batch_loss: 2.6428, loss: 2.6545 ||:  22%|##2       | 116/516 [09:33<33:24,  5.01s/it]
2025-02-04 04:01:57,156 - INFO - tqdm - perplexity: 14.2293, batch_loss: 2.6857, loss: 2.6553 ||:  23%|##2       | 118/516 [09:45<36:54,  5.56s/it]
2025-02-04 04:02:10,913 - INFO - tqdm - perplexity: 14.2471, batch_loss: 2.6985, loss: 2.6566 ||:  23%|##3       | 121/516 [09:58<33:20,  5.06s/it]
2025-02-04 04:02:23,341 - INFO - tqdm - perplexity: 14.2437, batch_loss: 2.6152, loss: 2.6563 ||:  24%|##4       | 124/516 [10:11<29:16,  4.48s/it]
2025-02-04 04:02:34,417 - INFO - tqdm - perplexity: 14.1855, batch_loss: 2.4425, loss: 2.6522 ||:  24%|##4       | 126/516 [10:22<32:06,  4.94s/it]
2025-02-04 04:02:44,793 - INFO - tqdm - perplexity: 14.1980, batch_loss: 2.7464, loss: 2.6531 ||:  25%|##4       | 128/516 [10:32<33:20,  5.16s/it]
2025-02-04 04:02:59,631 - INFO - tqdm - perplexity: 14.2075, batch_loss: 2.5241, loss: 2.6538 ||:  25%|##5       | 131/516 [10:47<33:15,  5.18s/it]
2025-02-04 04:03:12,865 - INFO - tqdm - perplexity: 14.1935, batch_loss: 2.4658, loss: 2.6528 ||:  26%|##5       | 134/516 [11:00<30:05,  4.73s/it]
2025-02-04 04:03:26,804 - INFO - tqdm - perplexity: 14.1955, batch_loss: 2.6624, loss: 2.6529 ||:  27%|##6       | 137/516 [11:14<30:19,  4.80s/it]
2025-02-04 04:03:39,075 - INFO - tqdm - perplexity: 14.1592, batch_loss: 2.4440, loss: 2.6504 ||:  27%|##7       | 140/516 [11:27<27:11,  4.34s/it]
2025-02-04 04:03:51,498 - INFO - tqdm - perplexity: 14.1758, batch_loss: 2.7274, loss: 2.6515 ||:  28%|##7       | 143/516 [11:39<26:23,  4.24s/it]
2025-02-04 04:04:04,179 - INFO - tqdm - perplexity: 14.1719, batch_loss: 2.7084, loss: 2.6513 ||:  28%|##8       | 146/516 [11:52<25:30,  4.14s/it]
2025-02-04 04:04:17,967 - INFO - tqdm - perplexity: 14.1627, batch_loss: 2.4830, loss: 2.6506 ||:  29%|##8       | 149/516 [12:05<27:29,  4.50s/it]
2025-02-04 04:04:31,980 - INFO - tqdm - perplexity: 14.1462, batch_loss: 2.7444, loss: 2.6494 ||:  29%|##9       | 152/516 [12:19<28:06,  4.63s/it]
2025-02-04 04:04:46,233 - INFO - tqdm - perplexity: 14.1357, batch_loss: 2.7055, loss: 2.6487 ||:  30%|###       | 155/516 [12:34<28:30,  4.74s/it]
2025-02-04 04:05:00,091 - INFO - tqdm - perplexity: 14.1135, batch_loss: 2.7377, loss: 2.6471 ||:  31%|###       | 158/516 [12:48<27:55,  4.68s/it]
2025-02-04 04:05:15,896 - INFO - tqdm - perplexity: 14.1025, batch_loss: 2.5667, loss: 2.6464 ||:  31%|###1      | 161/516 [13:03<31:06,  5.26s/it]
2025-02-04 04:05:29,460 - INFO - tqdm - perplexity: 14.0903, batch_loss: 2.6717, loss: 2.6455 ||:  32%|###1      | 164/516 [13:17<27:57,  4.77s/it]
2025-02-04 04:05:43,411 - INFO - tqdm - perplexity: 14.0704, batch_loss: 2.6073, loss: 2.6441 ||:  32%|###2      | 167/516 [13:31<27:03,  4.65s/it]
2025-02-04 04:05:58,210 - INFO - tqdm - perplexity: 14.0397, batch_loss: 2.6462, loss: 2.6419 ||:  33%|###2      | 170/516 [13:46<27:58,  4.85s/it]
2025-02-04 04:06:08,540 - INFO - tqdm - perplexity: 14.0435, batch_loss: 2.6530, loss: 2.6422 ||:  33%|###3      | 172/516 [13:56<28:26,  4.96s/it]
2025-02-04 04:06:20,038 - INFO - tqdm - perplexity: 14.0305, batch_loss: 2.5503, loss: 2.6412 ||:  34%|###3      | 174/516 [14:08<30:19,  5.32s/it]
2025-02-04 04:06:33,350 - INFO - tqdm - perplexity: 14.0382, batch_loss: 2.7854, loss: 2.6418 ||:  34%|###4      | 177/516 [14:21<26:57,  4.77s/it]
2025-02-04 04:06:46,583 - INFO - tqdm - perplexity: 14.0207, batch_loss: 2.5109, loss: 2.6405 ||:  35%|###4      | 180/516 [14:34<25:19,  4.52s/it]
2025-02-04 04:06:57,883 - INFO - tqdm - perplexity: 13.9815, batch_loss: 2.4356, loss: 2.6377 ||:  35%|###5      | 182/516 [14:45<28:03,  5.04s/it]
2025-02-04 04:07:11,086 - INFO - tqdm - perplexity: 13.9836, batch_loss: 2.5207, loss: 2.6379 ||:  36%|###5      | 185/516 [14:59<25:22,  4.60s/it]
2025-02-04 04:07:23,980 - INFO - tqdm - perplexity: 13.9269, batch_loss: 2.3678, loss: 2.6338 ||:  36%|###6      | 188/516 [15:11<23:47,  4.35s/it]
2025-02-04 04:07:35,356 - INFO - tqdm - perplexity: 13.9097, batch_loss: 2.4987, loss: 2.6326 ||:  37%|###6      | 190/516 [15:23<27:05,  4.99s/it]
2025-02-04 04:07:49,555 - INFO - tqdm - perplexity: 13.9020, batch_loss: 2.6404, loss: 2.6320 ||:  37%|###7      | 193/516 [15:37<25:43,  4.78s/it]
2025-02-04 04:08:04,860 - INFO - tqdm - perplexity: 13.8779, batch_loss: 2.2993, loss: 2.6303 ||:  38%|###7      | 196/516 [15:52<27:28,  5.15s/it]
2025-02-04 04:08:17,107 - INFO - tqdm - perplexity: 13.8698, batch_loss: 2.6581, loss: 2.6297 ||:  38%|###8      | 198/516 [16:05<29:04,  5.49s/it]
2025-02-04 04:08:31,780 - INFO - tqdm - perplexity: 13.8406, batch_loss: 2.5017, loss: 2.6276 ||:  39%|###8      | 201/516 [16:19<27:10,  5.18s/it]
2025-02-04 04:08:45,222 - INFO - tqdm - perplexity: 13.8297, batch_loss: 2.6949, loss: 2.6268 ||:  40%|###9      | 204/516 [16:33<24:45,  4.76s/it]
2025-02-04 04:08:56,069 - INFO - tqdm - perplexity: 13.8095, batch_loss: 2.3664, loss: 2.6254 ||:  40%|###9      | 206/516 [16:44<26:41,  5.17s/it]
2025-02-04 04:09:07,247 - INFO - tqdm - perplexity: 13.7930, batch_loss: 2.4263, loss: 2.6242 ||:  40%|####      | 208/516 [16:55<27:57,  5.45s/it]
2025-02-04 04:09:21,158 - INFO - tqdm - perplexity: 13.7927, batch_loss: 2.7025, loss: 2.6241 ||:  41%|####      | 211/516 [17:09<24:47,  4.88s/it]
2025-02-04 04:09:35,168 - INFO - tqdm - perplexity: 13.7890, batch_loss: 2.7381, loss: 2.6239 ||:  41%|####1     | 214/516 [17:23<24:34,  4.88s/it]
2025-02-04 04:09:49,919 - INFO - tqdm - perplexity: 13.8001, batch_loss: 2.6187, loss: 2.6247 ||:  42%|####2     | 217/516 [17:37<24:53,  4.99s/it]
2025-02-04 04:10:04,499 - INFO - tqdm - perplexity: 13.7796, batch_loss: 2.4563, loss: 2.6232 ||:  43%|####2     | 220/516 [17:52<24:25,  4.95s/it]
2025-02-04 04:10:14,504 - INFO - tqdm - perplexity: 13.7502, batch_loss: 2.2622, loss: 2.6211 ||:  43%|####3     | 222/516 [18:02<24:39,  5.03s/it]
2025-02-04 04:10:27,868 - INFO - tqdm - perplexity: 13.7631, batch_loss: 2.6368, loss: 2.6220 ||:  44%|####3     | 225/516 [18:15<22:21,  4.61s/it]
2025-02-04 04:10:40,383 - INFO - tqdm - perplexity: 13.7650, batch_loss: 2.5670, loss: 2.6221 ||:  44%|####4     | 228/516 [18:28<21:12,  4.42s/it]
2025-02-04 04:10:55,441 - INFO - tqdm - perplexity: 13.7541, batch_loss: 2.4279, loss: 2.6213 ||:  45%|####4     | 231/516 [18:43<23:29,  4.95s/it]
2025-02-04 04:11:06,357 - INFO - tqdm - perplexity: 13.7479, batch_loss: 2.6340, loss: 2.6209 ||:  45%|####5     | 233/516 [18:54<24:17,  5.15s/it]
2025-02-04 04:11:20,299 - INFO - tqdm - perplexity: 13.7681, batch_loss: 2.5826, loss: 2.6224 ||:  46%|####5     | 236/516 [19:08<22:48,  4.89s/it]
2025-02-04 04:11:34,584 - INFO - tqdm - perplexity: 13.7575, batch_loss: 2.5327, loss: 2.6216 ||:  46%|####6     | 239/516 [19:22<21:56,  4.75s/it]
2025-02-04 04:11:44,975 - INFO - tqdm - perplexity: 13.7499, batch_loss: 2.4889, loss: 2.6210 ||:  47%|####6     | 241/516 [19:32<22:54,  5.00s/it]
2025-02-04 04:11:55,125 - INFO - tqdm - perplexity: 13.7368, batch_loss: 2.6602, loss: 2.6201 ||:  47%|####7     | 243/516 [19:43<22:44,  5.00s/it]
2025-02-04 04:12:05,591 - INFO - tqdm - perplexity: 13.7133, batch_loss: 2.4183, loss: 2.6184 ||:  47%|####7     | 245/516 [19:53<22:40,  5.02s/it]
2025-02-04 04:12:19,726 - INFO - tqdm - perplexity: 13.6911, batch_loss: 2.5623, loss: 2.6167 ||:  48%|####8     | 248/516 [20:07<21:22,  4.79s/it]
2025-02-04 04:12:34,028 - INFO - tqdm - perplexity: 13.6885, batch_loss: 2.5624, loss: 2.6166 ||:  49%|####8     | 251/516 [20:22<21:29,  4.87s/it]
2025-02-04 04:12:44,347 - INFO - tqdm - perplexity: 13.6834, batch_loss: 2.5956, loss: 2.6162 ||:  49%|####9     | 253/516 [20:32<22:00,  5.02s/it]
2025-02-04 04:12:57,379 - INFO - tqdm - perplexity: 13.6538, batch_loss: 2.3236, loss: 2.6140 ||:  49%|####9     | 255/516 [20:45<25:03,  5.76s/it]
2025-02-04 04:13:09,238 - INFO - tqdm - perplexity: 13.6228, batch_loss: 2.2374, loss: 2.6117 ||:  50%|####9     | 257/516 [20:57<25:43,  5.96s/it]
2025-02-04 04:13:19,792 - INFO - tqdm - perplexity: 13.6204, batch_loss: 2.6363, loss: 2.6116 ||:  50%|#####     | 259/516 [21:07<23:27,  5.48s/it]
2025-02-04 04:13:35,211 - INFO - tqdm - perplexity: 13.6019, batch_loss: 2.1916, loss: 2.6102 ||:  51%|#####     | 261/516 [21:23<29:36,  6.97s/it]
2025-02-04 04:13:47,569 - INFO - tqdm - perplexity: 13.6116, batch_loss: 2.8208, loss: 2.6109 ||:  51%|#####     | 263/516 [21:35<27:03,  6.42s/it]
2025-02-04 04:14:01,905 - INFO - tqdm - perplexity: 13.6097, batch_loss: 2.6892, loss: 2.6108 ||:  52%|#####1    | 266/516 [21:49<22:11,  5.33s/it]
2025-02-04 04:14:15,647 - INFO - tqdm - perplexity: 13.6052, batch_loss: 2.4301, loss: 2.6105 ||:  52%|#####2    | 269/516 [22:03<20:11,  4.91s/it]
2025-02-04 04:14:28,803 - INFO - tqdm - perplexity: 13.6076, batch_loss: 2.6916, loss: 2.6106 ||:  53%|#####2    | 272/516 [22:16<18:26,  4.53s/it]
2025-02-04 04:14:38,858 - INFO - tqdm - perplexity: 13.6099, batch_loss: 2.5751, loss: 2.6108 ||:  53%|#####3    | 274/516 [22:26<19:15,  4.77s/it]
2025-02-04 04:14:54,020 - INFO - tqdm - perplexity: 13.5906, batch_loss: 2.8401, loss: 2.6094 ||:  54%|#####3    | 277/516 [22:42<20:23,  5.12s/it]
2025-02-04 04:15:05,819 - INFO - tqdm - perplexity: 13.5819, batch_loss: 2.5392, loss: 2.6087 ||:  54%|#####4    | 279/516 [22:53<22:07,  5.60s/it]
2025-02-04 04:15:17,258 - INFO - tqdm - perplexity: 13.5611, batch_loss: 2.2810, loss: 2.6072 ||:  54%|#####4    | 281/516 [23:05<22:26,  5.73s/it]
2025-02-04 04:15:29,681 - INFO - tqdm - perplexity: 13.5462, batch_loss: 2.3384, loss: 2.6061 ||:  55%|#####4    | 283/516 [23:17<23:41,  6.10s/it]
2025-02-04 04:15:40,603 - INFO - tqdm - perplexity: 13.5216, batch_loss: 2.1704, loss: 2.6043 ||:  55%|#####5    | 285/516 [23:28<22:35,  5.87s/it]
2025-02-04 04:15:53,398 - INFO - tqdm - perplexity: 13.5053, batch_loss: 2.4911, loss: 2.6031 ||:  56%|#####5    | 287/516 [23:41<23:39,  6.20s/it]
2025-02-04 04:16:06,752 - INFO - tqdm - perplexity: 13.4838, batch_loss: 2.3538, loss: 2.6015 ||:  56%|#####6    | 290/516 [23:54<19:05,  5.07s/it]
2025-02-04 04:16:19,035 - INFO - tqdm - perplexity: 13.4812, batch_loss: 2.7860, loss: 2.6013 ||:  57%|#####6    | 293/516 [24:07<16:12,  4.36s/it]
2025-02-04 04:16:32,692 - INFO - tqdm - perplexity: 13.4721, batch_loss: 2.4422, loss: 2.6006 ||:  57%|#####7    | 295/516 [24:20<21:06,  5.73s/it]
2025-02-04 04:16:45,663 - INFO - tqdm - perplexity: 13.4545, batch_loss: 2.3887, loss: 2.5993 ||:  58%|#####7    | 298/516 [24:33<17:24,  4.79s/it]
2025-02-04 04:16:59,618 - INFO - tqdm - perplexity: 13.4514, batch_loss: 2.6239, loss: 2.5991 ||:  58%|#####8    | 301/516 [24:47<16:25,  4.58s/it]
2025-02-04 04:17:10,654 - INFO - tqdm - perplexity: 13.4199, batch_loss: 2.2454, loss: 2.5967 ||:  59%|#####8    | 303/516 [24:58<17:54,  5.05s/it]
2025-02-04 04:17:24,857 - INFO - tqdm - perplexity: 13.3903, batch_loss: 2.3804, loss: 2.5945 ||:  59%|#####9    | 306/516 [25:12<17:05,  4.88s/it]
2025-02-04 04:17:38,843 - INFO - tqdm - perplexity: 13.3793, batch_loss: 2.5489, loss: 2.5937 ||:  60%|#####9    | 309/516 [25:26<16:39,  4.83s/it]
2025-02-04 04:17:53,455 - INFO - tqdm - perplexity: 13.3623, batch_loss: 2.5323, loss: 2.5924 ||:  60%|######    | 312/516 [25:41<16:33,  4.87s/it]
2025-02-04 04:18:09,616 - INFO - tqdm - perplexity: 13.3584, batch_loss: 2.8065, loss: 2.5921 ||:  61%|######1   | 315/516 [25:57<18:03,  5.39s/it]
2025-02-04 04:18:20,250 - INFO - tqdm - perplexity: 13.3500, batch_loss: 2.3441, loss: 2.5915 ||:  61%|######1   | 317/516 [26:08<18:02,  5.44s/it]
2025-02-04 04:18:35,323 - INFO - tqdm - perplexity: 13.3295, batch_loss: 2.2573, loss: 2.5900 ||:  62%|######2   | 320/516 [26:23<17:09,  5.25s/it]
2025-02-04 04:18:45,437 - INFO - tqdm - perplexity: 13.3295, batch_loss: 2.7340, loss: 2.5900 ||:  62%|######2   | 322/516 [26:33<16:28,  5.10s/it]
2025-02-04 04:18:55,668 - INFO - tqdm - perplexity: 13.3199, batch_loss: 2.3243, loss: 2.5893 ||:  63%|######2   | 324/516 [26:43<16:22,  5.12s/it]
2025-02-04 04:19:10,794 - INFO - tqdm - perplexity: 13.3099, batch_loss: 2.5235, loss: 2.5885 ||:  63%|######3   | 327/516 [26:58<16:35,  5.27s/it]
2025-02-04 04:19:26,535 - INFO - tqdm - perplexity: 13.2998, batch_loss: 2.4151, loss: 2.5878 ||:  64%|######3   | 330/516 [27:14<16:30,  5.33s/it]
2025-02-04 04:19:42,911 - INFO - tqdm - perplexity: 13.2834, batch_loss: 2.4887, loss: 2.5865 ||:  65%|######4   | 333/516 [27:30<17:06,  5.61s/it]
2025-02-04 04:19:54,307 - INFO - tqdm - perplexity: 13.2773, batch_loss: 2.6467, loss: 2.5861 ||:  65%|######4   | 335/516 [27:42<17:06,  5.67s/it]
2025-02-04 04:20:08,535 - INFO - tqdm - perplexity: 13.2636, batch_loss: 2.2825, loss: 2.5850 ||:  66%|######5   | 338/516 [27:56<14:53,  5.02s/it]
2025-02-04 04:20:18,890 - INFO - tqdm - perplexity: 13.2474, batch_loss: 2.3663, loss: 2.5838 ||:  66%|######5   | 340/516 [28:06<15:00,  5.12s/it]
2025-02-04 04:20:29,376 - INFO - tqdm - perplexity: 13.2299, batch_loss: 2.5109, loss: 2.5825 ||:  66%|######6   | 342/516 [28:17<15:05,  5.20s/it]
2025-02-04 04:20:39,379 - INFO - tqdm - perplexity: 13.2167, batch_loss: 2.4109, loss: 2.5815 ||:  67%|######6   | 344/516 [28:27<14:18,  4.99s/it]
2025-02-04 04:20:49,691 - INFO - tqdm - perplexity: 13.2029, batch_loss: 2.4873, loss: 2.5804 ||:  67%|######7   | 346/516 [28:37<14:17,  5.04s/it]
2025-02-04 04:21:03,176 - INFO - tqdm - perplexity: 13.1861, batch_loss: 2.4710, loss: 2.5792 ||:  68%|######7   | 349/516 [28:51<12:54,  4.64s/it]
2025-02-04 04:21:16,647 - INFO - tqdm - perplexity: 13.1642, batch_loss: 2.5501, loss: 2.5775 ||:  68%|######8   | 352/516 [29:04<12:10,  4.46s/it]
2025-02-04 04:21:30,837 - INFO - tqdm - perplexity: 13.1601, batch_loss: 2.7828, loss: 2.5772 ||:  69%|######8   | 355/516 [29:18<12:32,  4.67s/it]
2025-02-04 04:21:41,800 - INFO - tqdm - perplexity: 13.1410, batch_loss: 2.3458, loss: 2.5757 ||:  69%|######9   | 357/516 [29:29<13:18,  5.02s/it]
2025-02-04 04:21:54,759 - INFO - tqdm - perplexity: 13.1098, batch_loss: 2.1603, loss: 2.5734 ||:  70%|######9   | 360/516 [29:42<11:54,  4.58s/it]
2025-02-04 04:22:06,906 - INFO - tqdm - perplexity: 13.0913, batch_loss: 2.4192, loss: 2.5719 ||:  70%|#######   | 362/516 [29:54<13:21,  5.21s/it]
2025-02-04 04:22:21,282 - INFO - tqdm - perplexity: 13.0787, batch_loss: 2.3915, loss: 2.5710 ||:  71%|#######   | 365/516 [30:09<12:23,  4.92s/it]
2025-02-04 04:22:36,552 - INFO - tqdm - perplexity: 13.0751, batch_loss: 2.5121, loss: 2.5707 ||:  71%|#######1  | 368/516 [30:24<12:28,  5.06s/it]
2025-02-04 04:22:49,498 - INFO - tqdm - perplexity: 13.0659, batch_loss: 2.5353, loss: 2.5700 ||:  72%|#######1  | 371/516 [30:37<11:12,  4.64s/it]
2025-02-04 04:22:59,914 - INFO - tqdm - perplexity: 13.0593, batch_loss: 2.4757, loss: 2.5695 ||:  72%|#######2  | 373/516 [30:47<11:40,  4.90s/it]
2025-02-04 04:23:13,306 - INFO - tqdm - perplexity: 13.0593, batch_loss: 2.7038, loss: 2.5695 ||:  73%|#######2  | 376/516 [31:01<10:48,  4.63s/it]
2025-02-04 04:23:28,543 - INFO - tqdm - perplexity: 13.0462, batch_loss: 2.4337, loss: 2.5685 ||:  73%|#######3  | 379/516 [31:16<11:15,  4.93s/it]
2025-02-04 04:23:42,827 - INFO - tqdm - perplexity: 13.0206, batch_loss: 2.2437, loss: 2.5665 ||:  74%|#######4  | 382/516 [31:30<10:56,  4.90s/it]
2025-02-04 04:23:57,675 - INFO - tqdm - perplexity: 13.0090, batch_loss: 2.6824, loss: 2.5656 ||:  75%|#######4  | 385/516 [31:45<10:42,  4.90s/it]
2025-02-04 04:24:12,236 - INFO - tqdm - perplexity: 12.9951, batch_loss: 2.4233, loss: 2.5646 ||:  75%|#######5  | 388/516 [32:00<10:23,  4.87s/it]
2025-02-04 04:24:22,903 - INFO - tqdm - perplexity: 12.9906, batch_loss: 2.6691, loss: 2.5642 ||:  76%|#######5  | 390/516 [32:10<10:32,  5.02s/it]
2025-02-04 04:24:36,786 - INFO - tqdm - perplexity: 12.9689, batch_loss: 2.1053, loss: 2.5626 ||:  76%|#######6  | 393/516 [32:24<09:42,  4.74s/it]
2025-02-04 04:24:47,852 - INFO - tqdm - perplexity: 12.9595, batch_loss: 2.3262, loss: 2.5618 ||:  77%|#######6  | 395/516 [32:35<10:17,  5.10s/it]
2025-02-04 04:25:02,123 - INFO - tqdm - perplexity: 12.9426, batch_loss: 2.5897, loss: 2.5605 ||:  77%|#######7  | 398/516 [32:50<09:34,  4.87s/it]
2025-02-04 04:25:13,042 - INFO - tqdm - perplexity: 12.9283, batch_loss: 2.2735, loss: 2.5594 ||:  78%|#######7  | 400/516 [33:01<09:56,  5.14s/it]
2025-02-04 04:25:25,730 - INFO - tqdm - perplexity: 12.9203, batch_loss: 2.5443, loss: 2.5588 ||:  78%|#######8  | 403/516 [33:13<08:24,  4.46s/it]
2025-02-04 04:25:36,736 - INFO - tqdm - perplexity: 12.9091, batch_loss: 2.3331, loss: 2.5579 ||:  78%|#######8  | 405/516 [33:24<09:03,  4.89s/it]
2025-02-04 04:25:48,322 - INFO - tqdm - perplexity: 12.9058, batch_loss: 2.5731, loss: 2.5577 ||:  79%|#######8  | 407/516 [33:36<09:51,  5.42s/it]
2025-02-04 04:25:58,720 - INFO - tqdm - perplexity: 12.8919, batch_loss: 2.3818, loss: 2.5566 ||:  79%|#######9  | 409/516 [33:46<09:18,  5.22s/it]
2025-02-04 04:26:09,233 - INFO - tqdm - perplexity: 12.8773, batch_loss: 2.3130, loss: 2.5555 ||:  80%|#######9  | 411/516 [33:57<09:02,  5.17s/it]
2025-02-04 04:26:20,704 - INFO - tqdm - perplexity: 12.8747, batch_loss: 2.4994, loss: 2.5553 ||:  80%|########  | 413/516 [34:08<09:18,  5.43s/it]
2025-02-04 04:26:31,673 - INFO - tqdm - perplexity: 12.8566, batch_loss: 2.3851, loss: 2.5539 ||:  80%|########  | 415/516 [34:19<09:12,  5.47s/it]
2025-02-04 04:26:45,495 - INFO - tqdm - perplexity: 12.8448, batch_loss: 2.4311, loss: 2.5529 ||:  81%|########1 | 418/516 [34:33<07:58,  4.88s/it]
2025-02-04 04:26:59,187 - INFO - tqdm - perplexity: 12.8237, batch_loss: 2.3505, loss: 2.5513 ||:  82%|########1 | 421/516 [34:47<07:15,  4.58s/it]
2025-02-04 04:27:09,273 - INFO - tqdm - perplexity: 12.8189, batch_loss: 2.5400, loss: 2.5509 ||:  82%|########1 | 423/516 [34:57<07:28,  4.82s/it]
2025-02-04 04:27:19,577 - INFO - tqdm - perplexity: 12.8136, batch_loss: 2.4343, loss: 2.5505 ||:  82%|########2 | 425/516 [35:07<07:31,  4.96s/it]
2025-02-04 04:27:33,936 - INFO - tqdm - perplexity: 12.8069, batch_loss: 2.6185, loss: 2.5500 ||:  83%|########2 | 428/516 [35:21<07:05,  4.84s/it]
2025-02-04 04:27:46,078 - INFO - tqdm - perplexity: 12.7834, batch_loss: 2.2384, loss: 2.5481 ||:  83%|########3 | 430/516 [35:34<07:46,  5.43s/it]
2025-02-04 04:27:57,493 - INFO - tqdm - perplexity: 12.7736, batch_loss: 2.3318, loss: 2.5474 ||:  84%|########3 | 432/516 [35:45<07:51,  5.62s/it]
2025-02-04 04:28:12,013 - INFO - tqdm - perplexity: 12.7545, batch_loss: 2.3184, loss: 2.5459 ||:  84%|########4 | 435/516 [36:00<06:52,  5.09s/it]
2025-02-04 04:28:27,484 - INFO - tqdm - perplexity: 12.7343, batch_loss: 2.5143, loss: 2.5443 ||:  85%|########4 | 438/516 [36:15<06:42,  5.16s/it]
2025-02-04 04:28:42,204 - INFO - tqdm - perplexity: 12.7210, batch_loss: 2.2731, loss: 2.5433 ||:  85%|########5 | 441/516 [36:30<06:18,  5.05s/it]
2025-02-04 04:28:56,928 - INFO - tqdm - perplexity: 12.7083, batch_loss: 2.4725, loss: 2.5423 ||:  86%|########6 | 444/516 [36:44<05:58,  4.98s/it]
2025-02-04 04:29:07,090 - INFO - tqdm - perplexity: 12.7097, batch_loss: 2.5808, loss: 2.5424 ||:  86%|########6 | 446/516 [36:55<05:50,  5.00s/it]
2025-02-04 04:29:20,068 - INFO - tqdm - perplexity: 12.6999, batch_loss: 2.6116, loss: 2.5416 ||:  87%|########7 | 449/516 [37:08<05:02,  4.52s/it]
2025-02-04 04:29:35,685 - INFO - tqdm - perplexity: 12.6860, batch_loss: 2.5957, loss: 2.5405 ||:  88%|########7 | 452/516 [37:23<05:21,  5.02s/it]
2025-02-04 04:29:49,276 - INFO - tqdm - perplexity: 12.6825, batch_loss: 2.3165, loss: 2.5402 ||:  88%|########8 | 455/516 [37:37<04:53,  4.80s/it]
2025-02-04 04:30:01,384 - INFO - tqdm - perplexity: 12.6730, batch_loss: 2.2680, loss: 2.5395 ||:  89%|########8 | 457/516 [37:49<05:18,  5.40s/it]
2025-02-04 04:30:16,316 - INFO - tqdm - perplexity: 12.6550, batch_loss: 2.1269, loss: 2.5381 ||:  89%|########9 | 460/516 [38:04<04:53,  5.24s/it]
2025-02-04 04:30:31,272 - INFO - tqdm - perplexity: 12.6426, batch_loss: 2.3613, loss: 2.5371 ||:  90%|########9 | 463/516 [38:19<04:31,  5.13s/it]
2025-02-04 04:30:46,182 - INFO - tqdm - perplexity: 12.6285, batch_loss: 2.1563, loss: 2.5360 ||:  90%|######### | 466/516 [38:34<04:12,  5.04s/it]
2025-02-04 04:30:57,596 - INFO - tqdm - perplexity: 12.6094, batch_loss: 2.1825, loss: 2.5344 ||:  91%|######### | 468/516 [38:45<04:19,  5.40s/it]
2025-02-04 04:31:08,892 - INFO - tqdm - perplexity: 12.6034, batch_loss: 2.4686, loss: 2.5340 ||:  91%|#########1| 470/516 [38:56<04:11,  5.47s/it]
2025-02-04 04:31:24,363 - INFO - tqdm - perplexity: 12.5781, batch_loss: 2.0617, loss: 2.5320 ||:  92%|#########1| 473/516 [39:12<03:52,  5.41s/it]
2025-02-04 04:31:36,208 - INFO - tqdm - perplexity: 12.5800, batch_loss: 2.3904, loss: 2.5321 ||:  92%|#########2| 475/516 [39:24<03:47,  5.54s/it]
2025-02-04 04:31:50,035 - INFO - tqdm - perplexity: 12.5664, batch_loss: 2.2726, loss: 2.5310 ||:  93%|#########2| 478/516 [39:38<03:12,  5.06s/it]
2025-02-04 04:32:00,588 - INFO - tqdm - perplexity: 12.5545, batch_loss: 2.2047, loss: 2.5301 ||:  93%|#########3| 480/516 [39:48<03:08,  5.24s/it]
2025-02-04 04:32:16,365 - INFO - tqdm - perplexity: 12.5429, batch_loss: 2.3196, loss: 2.5292 ||:  94%|#########3| 483/516 [40:04<02:59,  5.45s/it]
2025-02-04 04:32:27,044 - INFO - tqdm - perplexity: 12.5358, batch_loss: 2.3632, loss: 2.5286 ||:  94%|#########3| 485/516 [40:15<02:48,  5.43s/it]
2025-02-04 04:32:41,383 - INFO - tqdm - perplexity: 12.5219, batch_loss: 2.3694, loss: 2.5275 ||:  95%|#########4| 488/516 [40:29<02:19,  4.99s/it]
2025-02-04 04:32:53,938 - INFO - tqdm - perplexity: 12.5090, batch_loss: 2.2695, loss: 2.5265 ||:  95%|#########4| 490/516 [40:41<02:26,  5.64s/it]
2025-02-04 04:33:08,293 - INFO - tqdm - perplexity: 12.5003, batch_loss: 2.3005, loss: 2.5258 ||:  96%|#########5| 493/516 [40:56<01:57,  5.10s/it]
2025-02-04 04:33:22,512 - INFO - tqdm - perplexity: 12.4967, batch_loss: 2.4379, loss: 2.5255 ||:  96%|#########6| 496/516 [41:10<01:36,  4.82s/it]
2025-02-04 04:33:34,463 - INFO - tqdm - perplexity: 12.4865, batch_loss: 2.4410, loss: 2.5246 ||:  97%|#########6| 498/516 [41:22<01:37,  5.40s/it]
2025-02-04 04:33:50,166 - INFO - tqdm - perplexity: 12.4722, batch_loss: 2.2602, loss: 2.5235 ||:  97%|#########7| 501/516 [41:38<01:21,  5.41s/it]
2025-02-04 04:34:00,997 - INFO - tqdm - perplexity: 12.4705, batch_loss: 2.2558, loss: 2.5234 ||:  97%|#########7| 503/516 [41:49<01:12,  5.55s/it]
2025-02-04 04:34:16,994 - INFO - tqdm - perplexity: 12.4678, batch_loss: 2.2278, loss: 2.5232 ||:  98%|#########8| 506/516 [42:05<00:54,  5.50s/it]
2025-02-04 04:34:27,472 - INFO - tqdm - perplexity: 12.4594, batch_loss: 2.3827, loss: 2.5225 ||:  98%|#########8| 508/516 [42:15<00:42,  5.32s/it]
2025-02-04 04:34:39,016 - INFO - tqdm - perplexity: 12.4533, batch_loss: 2.4356, loss: 2.5220 ||:  99%|#########8| 510/516 [42:27<00:33,  5.53s/it]
2025-02-04 04:34:51,773 - INFO - tqdm - perplexity: 12.4399, batch_loss: 2.1475, loss: 2.5209 ||:  99%|#########9| 512/516 [42:39<00:23,  6.00s/it]
2025-02-04 04:34:59,602 - INFO - tqdm - perplexity: 12.4292, batch_loss: 2.3040, loss: 2.5201 ||: 100%|#########9| 514/516 [42:47<00:09,  4.90s/it]
2025-02-04 04:35:05,866 - INFO - tqdm - perplexity: 12.4243, batch_loss: 2.3162, loss: 2.5197 ||: 100%|#########9| 515/516 [42:53<00:05,  5.31s/it]
2025-02-04 04:35:06,821 - INFO - tqdm - perplexity: 12.4193, batch_loss: 2.3111, loss: 2.5193 ||: 100%|##########| 516/516 [42:54<00:00,  4.00s/it]
2025-02-04 04:35:06,822 - INFO - tqdm - perplexity: 12.4193, batch_loss: 2.3111, loss: 2.5193 ||: 100%|##########| 516/516 [42:54<00:00,  4.99s/it]
2025-02-04 04:35:06,830 - INFO - allennlp.training.gradient_descent_trainer - Validating
2025-02-04 04:35:06,831 - INFO - tqdm - 0%|          | 0/129 [00:00<?, ?it/s]
2025-02-04 04:35:17,810 - INFO - tqdm - perplexity: 17.6371, batch_loss: 2.6005, loss: 2.8700 ||:   7%|6         | 9/129 [00:10<02:26,  1.22s/it]
2025-02-04 04:35:28,146 - INFO - tqdm - perplexity: 18.5433, batch_loss: 2.9478, loss: 2.9201 ||:  13%|#3        | 17/129 [00:21<02:21,  1.26s/it]
2025-02-04 04:35:38,514 - INFO - tqdm - perplexity: 18.9776, batch_loss: 3.0625, loss: 2.9433 ||:  19%|#9        | 25/129 [00:31<02:16,  1.31s/it]
2025-02-04 04:35:49,817 - INFO - tqdm - perplexity: 18.5309, batch_loss: 2.9178, loss: 2.9194 ||:  26%|##6       | 34/129 [00:42<01:58,  1.25s/it]
2025-02-04 04:36:00,260 - INFO - tqdm - perplexity: 17.8223, batch_loss: 2.5239, loss: 2.8805 ||:  33%|###2      | 42/129 [00:53<02:02,  1.40s/it]
2025-02-04 04:36:10,280 - INFO - tqdm - perplexity: 17.6341, batch_loss: 2.9141, loss: 2.8698 ||:  39%|###8      | 50/129 [01:03<01:40,  1.27s/it]
2025-02-04 04:36:20,446 - INFO - tqdm - perplexity: 17.8908, batch_loss: 2.6986, loss: 2.8843 ||:  45%|####4     | 58/129 [01:13<01:30,  1.28s/it]
2025-02-04 04:36:30,826 - INFO - tqdm - perplexity: 17.6049, batch_loss: 2.9820, loss: 2.8682 ||:  51%|#####1    | 66/129 [01:23<01:18,  1.24s/it]
2025-02-04 04:36:41,862 - INFO - tqdm - perplexity: 17.9395, batch_loss: 2.8710, loss: 2.8870 ||:  58%|#####8    | 75/129 [01:35<01:07,  1.26s/it]
2025-02-04 04:36:52,685 - INFO - tqdm - perplexity: 17.9785, batch_loss: 2.7107, loss: 2.8892 ||:  65%|######5   | 84/129 [01:45<00:56,  1.26s/it]
2025-02-04 04:37:02,999 - INFO - tqdm - perplexity: 17.8213, batch_loss: 2.6715, loss: 2.8804 ||:  71%|#######1  | 92/129 [01:56<00:47,  1.29s/it]
2025-02-04 04:37:13,494 - INFO - tqdm - perplexity: 17.9177, batch_loss: 2.9415, loss: 2.8858 ||:  78%|#######7  | 100/129 [02:06<00:36,  1.25s/it]
2025-02-04 04:37:23,726 - INFO - tqdm - perplexity: 17.9394, batch_loss: 2.6659, loss: 2.8870 ||:  84%|########3 | 108/129 [02:16<00:26,  1.26s/it]
2025-02-04 04:37:33,742 - INFO - tqdm - perplexity: 17.8758, batch_loss: 2.7983, loss: 2.8834 ||:  90%|########9 | 116/129 [02:26<00:15,  1.21s/it]
2025-02-04 04:37:45,068 - INFO - tqdm - perplexity: 17.8846, batch_loss: 2.8556, loss: 2.8839 ||:  96%|#########6| 124/129 [02:38<00:07,  1.42s/it]
2025-02-04 04:37:51,650 - INFO - tqdm - perplexity: 17.8864, batch_loss: 2.8692, loss: 2.8840 ||: 100%|##########| 129/129 [02:44<00:00,  1.28s/it]
2025-02-04 04:37:51,650 - INFO - tqdm - perplexity: 17.8864, batch_loss: 2.8692, loss: 2.8840 ||: 100%|##########| 129/129 [02:44<00:00,  1.28s/it]
2025-02-04 04:37:51,652 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2025-02-04 04:37:51,652 - INFO - allennlp.training.callbacks.console_logger - loss               |     2.519  |     2.884
2025-02-04 04:37:51,652 - INFO - allennlp.training.callbacks.console_logger - perplexity         |    12.419  |    17.886
2025-02-04 04:37:51,653 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |     0.000  |       N/A
2025-02-04 04:37:52,297 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:45:40.310477
2025-02-04 04:37:52,297 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 20:23:45
2025-02-04 04:37:52,298 - INFO - allennlp.training.gradient_descent_trainer - Epoch 5/31
2025-02-04 04:37:52,298 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 0B
2025-02-04 04:37:52,298 - INFO - allennlp.training.gradient_descent_trainer - Training
2025-02-04 04:37:52,298 - INFO - tqdm - 0%|          | 0/516 [00:00<?, ?it/s]
2025-02-04 04:38:02,433 - INFO - tqdm - perplexity: 10.1906, batch_loss: 2.3723, loss: 2.3215 ||:   0%|          | 2/516 [00:10<44:23,  5.18s/it]
2025-02-04 04:38:16,391 - INFO - tqdm - perplexity: 9.6344, batch_loss: 2.0836, loss: 2.2653 ||:   1%|          | 5/516 [00:24<40:00,  4.70s/it]
2025-02-04 04:38:29,339 - INFO - tqdm - perplexity: 9.7463, batch_loss: 2.3821, loss: 2.2769 ||:   2%|1         | 8/516 [00:37<37:43,  4.46s/it]
2025-02-04 04:38:44,899 - INFO - tqdm - perplexity: 9.4944, batch_loss: 1.9104, loss: 2.2507 ||:   2%|2         | 11/516 [00:52<42:16,  5.02s/it]
2025-02-04 04:38:58,897 - INFO - tqdm - perplexity: 9.4070, batch_loss: 2.2532, loss: 2.2415 ||:   3%|2         | 14/516 [01:06<40:04,  4.79s/it]
2025-02-04 04:39:12,828 - INFO - tqdm - perplexity: 9.2347, batch_loss: 2.0803, loss: 2.2230 ||:   3%|3         | 17/516 [01:20<39:05,  4.70s/it]
2025-02-04 04:39:23,617 - INFO - tqdm - perplexity: 9.1557, batch_loss: 2.1430, loss: 2.2144 ||:   4%|3         | 19/516 [01:31<41:54,  5.06s/it]
2025-02-04 04:39:34,079 - INFO - tqdm - perplexity: 9.1847, batch_loss: 2.3750, loss: 2.2175 ||:   4%|4         | 21/516 [01:41<42:44,  5.18s/it]
2025-02-04 04:39:48,118 - INFO - tqdm - perplexity: 9.1407, batch_loss: 2.2414, loss: 2.2127 ||:   5%|4         | 24/516 [01:55<39:44,  4.85s/it]
2025-02-04 04:40:01,445 - INFO - tqdm - perplexity: 9.1390, batch_loss: 2.2664, loss: 2.2126 ||:   5%|5         | 27/516 [02:09<37:43,  4.63s/it]
2025-02-04 04:40:16,703 - INFO - tqdm - perplexity: 9.0174, batch_loss: 2.1206, loss: 2.1992 ||:   6%|5         | 30/516 [02:24<40:56,  5.05s/it]
2025-02-04 04:40:30,735 - INFO - tqdm - perplexity: 8.9756, batch_loss: 1.9952, loss: 2.1945 ||:   6%|6         | 33/516 [02:38<37:38,  4.68s/it]
2025-02-04 04:40:43,910 - INFO - tqdm - perplexity: 8.9726, batch_loss: 2.1946, loss: 2.1942 ||:   7%|6         | 36/516 [02:51<35:20,  4.42s/it]
2025-02-04 04:40:59,220 - INFO - tqdm - perplexity: 8.9225, batch_loss: 2.1700, loss: 2.1886 ||:   8%|7         | 39/516 [03:06<38:59,  4.91s/it]
2025-02-04 04:41:11,107 - INFO - tqdm - perplexity: 8.9477, batch_loss: 2.1626, loss: 2.1914 ||:   8%|8         | 42/516 [03:18<33:20,  4.22s/it]
2025-02-04 04:41:21,859 - INFO - tqdm - perplexity: 8.9259, batch_loss: 2.0857, loss: 2.1890 ||:   9%|8         | 44/516 [03:29<38:40,  4.92s/it]
2025-02-04 04:41:35,752 - INFO - tqdm - perplexity: 8.9283, batch_loss: 2.1211, loss: 2.1892 ||:   9%|9         | 47/516 [03:43<37:23,  4.78s/it]
2025-02-04 04:41:48,579 - INFO - tqdm - perplexity: 8.9086, batch_loss: 2.1978, loss: 2.1870 ||:  10%|9         | 50/516 [03:56<34:31,  4.44s/it]
2025-02-04 04:42:02,742 - INFO - tqdm - perplexity: 8.8298, batch_loss: 2.1484, loss: 2.1781 ||:  10%|#         | 53/516 [04:10<35:09,  4.56s/it]
2025-02-04 04:42:13,184 - INFO - tqdm - perplexity: 8.8235, batch_loss: 2.0391, loss: 2.1774 ||:  11%|#         | 55/516 [04:20<38:07,  4.96s/it]
2025-02-04 04:42:26,653 - INFO - tqdm - perplexity: 8.8583, batch_loss: 2.4612, loss: 2.1814 ||:  11%|#1        | 58/516 [04:34<35:41,  4.68s/it]
2025-02-04 04:42:39,223 - INFO - tqdm - perplexity: 8.8155, batch_loss: 2.1537, loss: 2.1765 ||:  12%|#1        | 60/516 [04:46<41:45,  5.50s/it]
2025-02-04 04:42:49,490 - INFO - tqdm - perplexity: 8.8133, batch_loss: 2.0166, loss: 2.1763 ||:  12%|#2        | 62/516 [04:57<40:00,  5.29s/it]
2025-02-04 04:43:01,047 - INFO - tqdm - perplexity: 8.8355, batch_loss: 2.1213, loss: 2.1788 ||:  12%|#2        | 64/516 [05:08<42:01,  5.58s/it]
2025-02-04 04:43:15,212 - INFO - tqdm - perplexity: 8.8449, batch_loss: 2.0480, loss: 2.1798 ||:  13%|#2        | 67/516 [05:22<37:48,  5.05s/it]
2025-02-04 04:43:29,905 - INFO - tqdm - perplexity: 8.8078, batch_loss: 2.0337, loss: 2.1756 ||:  14%|#3        | 70/516 [05:37<36:46,  4.95s/it]
2025-02-04 04:43:41,099 - INFO - tqdm - perplexity: 8.7617, batch_loss: 1.9244, loss: 2.1704 ||:  14%|#3        | 72/516 [05:48<38:37,  5.22s/it]
2025-02-04 04:43:54,531 - INFO - tqdm - perplexity: 8.7024, batch_loss: 1.8280, loss: 2.1636 ||:  15%|#4        | 75/516 [06:02<34:36,  4.71s/it]
2025-02-04 04:44:07,187 - INFO - tqdm - perplexity: 8.6961, batch_loss: 2.2011, loss: 2.1629 ||:  15%|#5        | 78/516 [06:14<32:00,  4.38s/it]
2025-02-04 04:44:18,760 - INFO - tqdm - perplexity: 8.6844, batch_loss: 2.0258, loss: 2.1615 ||:  16%|#5        | 80/516 [06:26<37:46,  5.20s/it]
2025-02-04 04:44:30,463 - INFO - tqdm - perplexity: 8.6869, batch_loss: 2.2253, loss: 2.1618 ||:  16%|#5        | 82/516 [06:38<40:35,  5.61s/it]
2025-02-04 04:44:40,942 - INFO - tqdm - perplexity: 8.6802, batch_loss: 2.0271, loss: 2.1610 ||:  16%|#6        | 84/516 [06:48<39:22,  5.47s/it]
2025-02-04 04:44:55,760 - INFO - tqdm - perplexity: 8.6344, batch_loss: 2.0070, loss: 2.1558 ||:  17%|#6        | 87/516 [07:03<37:02,  5.18s/it]
2025-02-04 04:45:10,307 - INFO - tqdm - perplexity: 8.6199, batch_loss: 2.0641, loss: 2.1541 ||:  17%|#7        | 90/516 [07:18<35:47,  5.04s/it]
2025-02-04 04:45:21,543 - INFO - tqdm - perplexity: 8.6182, batch_loss: 2.0402, loss: 2.1539 ||:  18%|#7        | 92/516 [07:29<37:56,  5.37s/it]
2025-02-04 04:45:33,168 - INFO - tqdm - perplexity: 8.5823, batch_loss: 2.0196, loss: 2.1497 ||:  18%|#8        | 94/516 [07:40<38:15,  5.44s/it]
2025-02-04 04:45:48,384 - INFO - tqdm - perplexity: 8.5743, batch_loss: 2.0898, loss: 2.1488 ||:  19%|#8        | 97/516 [07:56<37:12,  5.33s/it]
2025-02-04 04:46:02,710 - INFO - tqdm - perplexity: 8.5221, batch_loss: 1.9712, loss: 2.1427 ||:  19%|#9        | 100/516 [08:10<34:53,  5.03s/it]
2025-02-04 04:46:16,841 - INFO - tqdm - perplexity: 8.5428, batch_loss: 2.1524, loss: 2.1451 ||:  20%|#9        | 103/516 [08:24<32:58,  4.79s/it]
2025-02-04 04:46:30,373 - INFO - tqdm - perplexity: 8.5203, batch_loss: 2.0337, loss: 2.1425 ||:  21%|##        | 106/516 [08:38<31:34,  4.62s/it]
2025-02-04 04:46:43,370 - INFO - tqdm - perplexity: 8.5458, batch_loss: 2.2231, loss: 2.1454 ||:  21%|##1       | 109/516 [08:51<29:55,  4.41s/it]
2025-02-04 04:46:57,955 - INFO - tqdm - perplexity: 8.5495, batch_loss: 2.0505, loss: 2.1459 ||:  22%|##1       | 112/516 [09:05<31:38,  4.70s/it]
2025-02-04 04:47:11,626 - INFO - tqdm - perplexity: 8.5504, batch_loss: 2.1489, loss: 2.1460 ||:  22%|##2       | 115/516 [09:19<30:32,  4.57s/it]
2025-02-04 04:47:24,787 - INFO - tqdm - perplexity: 8.5585, batch_loss: 2.1036, loss: 2.1469 ||:  23%|##2       | 118/516 [09:32<29:33,  4.46s/it]
2025-02-04 04:47:37,528 - INFO - tqdm - perplexity: 8.5515, batch_loss: 2.2465, loss: 2.1461 ||:  23%|##3       | 121/516 [09:45<27:59,  4.25s/it]
2025-02-04 04:47:52,886 - INFO - tqdm - perplexity: 8.5609, batch_loss: 2.1393, loss: 2.1472 ||:  24%|##4       | 124/516 [10:00<31:48,  4.87s/it]
2025-02-04 04:48:05,578 - INFO - tqdm - perplexity: 8.5766, batch_loss: 2.2018, loss: 2.1490 ||:  25%|##4       | 127/516 [10:13<28:56,  4.47s/it]
2025-02-04 04:48:18,779 - INFO - tqdm - perplexity: 8.5394, batch_loss: 1.6666, loss: 2.1447 ||:  25%|##5       | 129/516 [10:26<36:14,  5.62s/it]
2025-02-04 04:48:32,354 - INFO - tqdm - perplexity: 8.5282, batch_loss: 2.0170, loss: 2.1434 ||:  26%|##5       | 132/516 [10:40<32:11,  5.03s/it]
2025-02-04 04:48:44,613 - INFO - tqdm - perplexity: 8.5259, batch_loss: 2.2283, loss: 2.1431 ||:  26%|##6       | 135/516 [10:52<27:41,  4.36s/it]
2025-02-04 04:48:57,491 - INFO - tqdm - perplexity: 8.5222, batch_loss: 2.0515, loss: 2.1427 ||:  27%|##6       | 138/516 [11:05<27:36,  4.38s/it]
2025-02-04 04:49:08,446 - INFO - tqdm - perplexity: 8.5352, batch_loss: 2.3396, loss: 2.1442 ||:  27%|##7       | 140/516 [11:16<31:20,  5.00s/it]
2025-02-04 04:49:22,019 - INFO - tqdm - perplexity: 8.5259, batch_loss: 1.9781, loss: 2.1431 ||:  28%|##7       | 143/516 [11:29<28:43,  4.62s/it]
2025-02-04 04:49:32,759 - INFO - tqdm - perplexity: 8.5290, batch_loss: 2.1078, loss: 2.1435 ||:  28%|##8       | 145/516 [11:40<30:06,  4.87s/it]
2025-02-04 04:49:47,490 - INFO - tqdm - perplexity: 8.5537, batch_loss: 2.4372, loss: 2.1464 ||:  29%|##8       | 148/516 [11:55<30:07,  4.91s/it]
2025-02-04 04:49:57,746 - INFO - tqdm - perplexity: 8.5612, batch_loss: 2.1368, loss: 2.1472 ||:  29%|##9       | 150/516 [12:05<30:47,  5.05s/it]
2025-02-04 04:50:13,295 - INFO - tqdm - perplexity: 8.5633, batch_loss: 2.0296, loss: 2.1475 ||:  30%|##9       | 153/516 [12:20<31:25,  5.19s/it]
2025-02-04 04:50:23,682 - INFO - tqdm - perplexity: 8.5519, batch_loss: 2.1218, loss: 2.1462 ||:  30%|###       | 155/516 [12:31<30:51,  5.13s/it]
2025-02-04 04:50:36,222 - INFO - tqdm - perplexity: 8.5706, batch_loss: 2.2451, loss: 2.1483 ||:  30%|###       | 157/516 [12:43<33:45,  5.64s/it]
2025-02-04 04:50:46,380 - INFO - tqdm - perplexity: 8.5546, batch_loss: 2.1346, loss: 2.1465 ||:  31%|###       | 159/516 [12:54<31:20,  5.27s/it]
2025-02-04 04:50:57,322 - INFO - tqdm - perplexity: 8.5470, batch_loss: 2.0067, loss: 2.1456 ||:  31%|###1      | 161/516 [13:05<32:08,  5.43s/it]
2025-02-04 04:51:11,487 - INFO - tqdm - perplexity: 8.5340, batch_loss: 1.9445, loss: 2.1441 ||:  32%|###1      | 164/516 [13:19<29:45,  5.07s/it]
2025-02-04 04:51:26,889 - INFO - tqdm - perplexity: 8.5230, batch_loss: 1.9999, loss: 2.1428 ||:  32%|###2      | 167/516 [13:34<30:34,  5.26s/it]
2025-02-04 04:51:37,735 - INFO - tqdm - perplexity: 8.5194, batch_loss: 2.0597, loss: 2.1423 ||:  33%|###2      | 169/516 [13:45<31:10,  5.39s/it]
2025-02-04 04:51:48,242 - INFO - tqdm - perplexity: 8.5173, batch_loss: 2.2934, loss: 2.1421 ||:  33%|###3      | 171/516 [13:55<30:00,  5.22s/it]
2025-02-04 04:52:00,159 - INFO - tqdm - perplexity: 8.5138, batch_loss: 2.0719, loss: 2.1417 ||:  34%|###3      | 173/516 [14:07<31:47,  5.56s/it]
2025-02-04 04:52:10,965 - INFO - tqdm - perplexity: 8.5131, batch_loss: 1.9476, loss: 2.1416 ||:  34%|###3      | 175/516 [14:18<31:13,  5.50s/it]
2025-02-04 04:52:23,954 - INFO - tqdm - perplexity: 8.5123, batch_loss: 2.0203, loss: 2.1415 ||:  34%|###4      | 178/516 [14:31<26:30,  4.71s/it]
2025-02-04 04:52:34,110 - INFO - tqdm - perplexity: 8.5068, batch_loss: 2.1659, loss: 2.1409 ||:  35%|###4      | 180/516 [14:41<27:47,  4.96s/it]
2025-02-04 04:52:45,819 - INFO - tqdm - perplexity: 8.4822, batch_loss: 1.8296, loss: 2.1380 ||:  35%|###5      | 182/516 [14:53<30:49,  5.54s/it]
2025-02-04 04:53:00,302 - INFO - tqdm - perplexity: 8.4741, batch_loss: 2.0925, loss: 2.1370 ||:  36%|###5      | 185/516 [15:08<28:17,  5.13s/it]
2025-02-04 04:53:11,419 - INFO - tqdm - perplexity: 8.4634, batch_loss: 2.2383, loss: 2.1358 ||:  36%|###6      | 187/516 [15:19<28:40,  5.23s/it]
2025-02-04 04:53:25,206 - INFO - tqdm - perplexity: 8.4562, batch_loss: 1.9320, loss: 2.1349 ||:  37%|###6      | 190/516 [15:32<26:17,  4.84s/it]
2025-02-04 04:53:40,450 - INFO - tqdm - perplexity: 8.4521, batch_loss: 2.1290, loss: 2.1344 ||:  37%|###7      | 193/516 [15:48<27:14,  5.06s/it]
2025-02-04 04:53:51,824 - INFO - tqdm - perplexity: 8.4527, batch_loss: 2.2886, loss: 2.1345 ||:  38%|###7      | 195/516 [15:59<28:08,  5.26s/it]
2025-02-04 04:54:03,608 - INFO - tqdm - perplexity: 8.4291, batch_loss: 1.6371, loss: 2.1317 ||:  38%|###8      | 197/516 [16:11<29:50,  5.61s/it]
2025-02-04 04:54:14,299 - INFO - tqdm - perplexity: 8.4378, batch_loss: 2.2343, loss: 2.1327 ||:  39%|###8      | 199/516 [16:21<28:41,  5.43s/it]
2025-02-04 04:54:29,325 - INFO - tqdm - perplexity: 8.4393, batch_loss: 2.0409, loss: 2.1329 ||:  39%|###9      | 202/516 [16:37<27:02,  5.17s/it]
2025-02-04 04:54:46,408 - INFO - tqdm - perplexity: 8.4320, batch_loss: 1.8801, loss: 2.1320 ||:  40%|###9      | 204/516 [16:54<37:13,  7.16s/it]
2025-02-04 04:54:59,464 - INFO - tqdm - perplexity: 8.4441, batch_loss: 2.1866, loss: 2.1335 ||:  40%|####      | 207/516 [17:07<27:43,  5.38s/it]
2025-02-04 04:55:09,716 - INFO - tqdm - perplexity: 8.4402, batch_loss: 2.0682, loss: 2.1330 ||:  41%|####      | 209/516 [17:17<27:06,  5.30s/it]
2025-02-04 04:55:24,805 - INFO - tqdm - perplexity: 8.4418, batch_loss: 2.0387, loss: 2.1332 ||:  41%|####1     | 212/516 [17:32<26:33,  5.24s/it]
2025-02-04 04:55:39,535 - INFO - tqdm - perplexity: 8.4405, batch_loss: 2.0311, loss: 2.1330 ||:  42%|####1     | 215/516 [17:47<25:41,  5.12s/it]
2025-02-04 04:55:53,557 - INFO - tqdm - perplexity: 8.4358, batch_loss: 2.0184, loss: 2.1325 ||:  42%|####2     | 218/516 [18:01<24:21,  4.90s/it]
2025-02-04 04:56:05,291 - INFO - tqdm - perplexity: 8.4179, batch_loss: 1.6819, loss: 2.1304 ||:  43%|####2     | 220/516 [18:12<26:52,  5.45s/it]
2025-02-04 04:56:19,731 - INFO - tqdm - perplexity: 8.4163, batch_loss: 2.0828, loss: 2.1302 ||:  43%|####3     | 223/516 [18:27<24:35,  5.04s/it]
2025-02-04 04:56:30,863 - INFO - tqdm - perplexity: 8.4102, batch_loss: 2.1547, loss: 2.1294 ||:  44%|####3     | 225/516 [18:38<25:20,  5.23s/it]
2025-02-04 04:56:44,624 - INFO - tqdm - perplexity: 8.4098, batch_loss: 2.0497, loss: 2.1294 ||:  44%|####4     | 228/516 [18:52<22:51,  4.76s/it]
2025-02-04 04:56:55,918 - INFO - tqdm - perplexity: 8.4028, batch_loss: 2.0715, loss: 2.1286 ||:  45%|####4     | 230/516 [19:03<24:29,  5.14s/it]
2025-02-04 04:57:10,174 - INFO - tqdm - perplexity: 8.4023, batch_loss: 1.9977, loss: 2.1285 ||:  45%|####5     | 233/516 [19:17<23:16,  4.93s/it]
2025-02-04 04:57:22,742 - INFO - tqdm - perplexity: 8.4019, batch_loss: 1.8638, loss: 2.1285 ||:  46%|####5     | 236/516 [19:30<20:47,  4.46s/it]
2025-02-04 04:57:36,255 - INFO - tqdm - perplexity: 8.3934, batch_loss: 1.9252, loss: 2.1274 ||:  46%|####6     | 239/516 [19:43<21:03,  4.56s/it]
2025-02-04 04:57:50,336 - INFO - tqdm - perplexity: 8.3927, batch_loss: 1.9848, loss: 2.1274 ||:  47%|####6     | 242/516 [19:58<21:48,  4.78s/it]
2025-02-04 04:58:01,753 - INFO - tqdm - perplexity: 8.3913, batch_loss: 2.3240, loss: 2.1272 ||:  47%|####7     | 244/516 [20:09<23:19,  5.14s/it]
2025-02-04 04:58:15,350 - INFO - tqdm - perplexity: 8.3866, batch_loss: 2.1255, loss: 2.1266 ||:  48%|####7     | 247/516 [20:23<20:34,  4.59s/it]
2025-02-04 04:58:30,885 - INFO - tqdm - perplexity: 8.3788, batch_loss: 1.7697, loss: 2.1257 ||:  48%|####8     | 250/516 [20:38<22:40,  5.11s/it]
2025-02-04 04:58:41,269 - INFO - tqdm - perplexity: 8.3722, batch_loss: 1.9754, loss: 2.1249 ||:  49%|####8     | 252/516 [20:48<23:21,  5.31s/it]
2025-02-04 04:58:55,216 - INFO - tqdm - perplexity: 8.3629, batch_loss: 1.9942, loss: 2.1238 ||:  49%|####9     | 255/516 [21:02<21:30,  4.94s/it]
2025-02-04 04:59:09,008 - INFO - tqdm - perplexity: 8.3580, batch_loss: 2.1593, loss: 2.1232 ||:  50%|#####     | 258/516 [21:16<19:55,  4.63s/it]
2025-02-04 04:59:21,859 - INFO - tqdm - perplexity: 8.3622, batch_loss: 2.3162, loss: 2.1237 ||:  51%|#####     | 261/516 [21:29<18:28,  4.35s/it]
2025-02-04 04:59:34,111 - INFO - tqdm - perplexity: 8.3636, batch_loss: 1.8913, loss: 2.1239 ||:  51%|#####     | 263/516 [21:41<21:47,  5.17s/it]
2025-02-04 04:59:48,288 - INFO - tqdm - perplexity: 8.3606, batch_loss: 2.1643, loss: 2.1235 ||:  52%|#####1    | 266/516 [21:55<20:14,  4.86s/it]
2025-02-04 05:00:02,665 - INFO - tqdm - perplexity: 8.3503, batch_loss: 1.9484, loss: 2.1223 ||:  52%|#####2    | 269/516 [22:10<19:53,  4.83s/it]
2025-02-04 05:00:13,255 - INFO - tqdm - perplexity: 8.3417, batch_loss: 2.0275, loss: 2.1213 ||:  53%|#####2    | 271/516 [22:20<20:43,  5.07s/it]
2025-02-04 05:00:29,339 - INFO - tqdm - perplexity: 8.3313, batch_loss: 1.9863, loss: 2.1200 ||:  53%|#####3    | 274/516 [22:37<21:49,  5.41s/it]
2025-02-04 05:00:40,797 - INFO - tqdm - perplexity: 8.3189, batch_loss: 2.0440, loss: 2.1185 ||:  53%|#####3    | 276/516 [22:48<22:07,  5.53s/it]
2025-02-04 05:00:50,990 - INFO - tqdm - perplexity: 8.3221, batch_loss: 2.1612, loss: 2.1189 ||:  54%|#####3    | 278/516 [22:58<21:02,  5.31s/it]
2025-02-04 05:01:05,978 - INFO - tqdm - perplexity: 8.3124, batch_loss: 2.0357, loss: 2.1177 ||:  54%|#####4    | 281/516 [23:13<20:35,  5.26s/it]
2025-02-04 05:01:20,519 - INFO - tqdm - perplexity: 8.3130, batch_loss: 2.1265, loss: 2.1178 ||:  55%|#####5    | 284/516 [23:28<19:33,  5.06s/it]
2025-02-04 05:01:30,693 - INFO - tqdm - perplexity: 8.3108, batch_loss: 2.1932, loss: 2.1176 ||:  55%|#####5    | 286/516 [23:38<19:12,  5.01s/it]
2025-02-04 05:01:43,825 - INFO - tqdm - perplexity: 8.3023, batch_loss: 2.0782, loss: 2.1165 ||:  56%|#####6    | 289/516 [23:51<17:16,  4.57s/it]
2025-02-04 05:01:58,230 - INFO - tqdm - perplexity: 8.3043, batch_loss: 2.1859, loss: 2.1168 ||:  57%|#####6    | 292/516 [24:05<17:45,  4.76s/it]
2025-02-04 05:02:10,119 - INFO - tqdm - perplexity: 8.2945, batch_loss: 1.9419, loss: 2.1156 ||:  57%|#####6    | 294/516 [24:17<19:38,  5.31s/it]
2025-02-04 05:02:20,892 - INFO - tqdm - perplexity: 8.2948, batch_loss: 2.0411, loss: 2.1156 ||:  57%|#####7    | 296/516 [24:28<19:55,  5.43s/it]
2025-02-04 05:02:34,638 - INFO - tqdm - perplexity: 8.2856, batch_loss: 1.9896, loss: 2.1145 ||:  58%|#####7    | 299/516 [24:42<17:34,  4.86s/it]
2025-02-04 05:02:45,190 - INFO - tqdm - perplexity: 8.2741, batch_loss: 1.9208, loss: 2.1131 ||:  58%|#####8    | 301/516 [24:52<18:07,  5.06s/it]
2025-02-04 05:02:59,010 - INFO - tqdm - perplexity: 8.2771, batch_loss: 2.1753, loss: 2.1135 ||:  59%|#####8    | 304/516 [25:06<17:02,  4.82s/it]
2025-02-04 05:03:13,149 - INFO - tqdm - perplexity: 8.2760, batch_loss: 2.0653, loss: 2.1134 ||:  59%|#####9    | 307/516 [25:20<16:45,  4.81s/it]
2025-02-04 05:03:25,975 - INFO - tqdm - perplexity: 8.2719, batch_loss: 2.0668, loss: 2.1129 ||:  60%|######    | 310/516 [25:33<15:16,  4.45s/it]
2025-02-04 05:03:36,523 - INFO - tqdm - perplexity: 8.2620, batch_loss: 1.9696, loss: 2.1117 ||:  60%|######    | 312/516 [25:44<16:13,  4.77s/it]
2025-02-04 05:03:47,771 - INFO - tqdm - perplexity: 8.2612, batch_loss: 2.0357, loss: 2.1116 ||:  61%|######    | 314/516 [25:55<17:49,  5.29s/it]
2025-02-04 05:03:58,265 - INFO - tqdm - perplexity: 8.2518, batch_loss: 1.8921, loss: 2.1104 ||:  61%|######1   | 316/516 [26:05<17:55,  5.38s/it]
2025-02-04 05:04:08,447 - INFO - tqdm - perplexity: 8.2445, batch_loss: 2.0787, loss: 2.1095 ||:  62%|######1   | 318/516 [26:16<17:17,  5.24s/it]
2025-02-04 05:04:21,616 - INFO - tqdm - perplexity: 8.2478, batch_loss: 2.1513, loss: 2.1099 ||:  62%|######2   | 321/516 [26:29<15:14,  4.69s/it]
2025-02-04 05:04:33,104 - INFO - tqdm - perplexity: 8.2449, batch_loss: 1.8246, loss: 2.1096 ||:  63%|######2   | 323/516 [26:40<17:06,  5.32s/it]
2025-02-04 05:04:46,542 - INFO - tqdm - perplexity: 8.2425, batch_loss: 2.0915, loss: 2.1093 ||:  63%|######3   | 326/516 [26:54<15:06,  4.77s/it]
2025-02-04 05:05:00,275 - INFO - tqdm - perplexity: 8.2184, batch_loss: 1.5224, loss: 2.1064 ||:  64%|######3   | 328/516 [27:07<18:21,  5.86s/it]
2025-02-04 05:05:12,351 - INFO - tqdm - perplexity: 8.2117, batch_loss: 2.0514, loss: 2.1056 ||:  64%|######3   | 330/516 [27:20<18:26,  5.95s/it]
2025-02-04 05:05:26,740 - INFO - tqdm - perplexity: 8.2104, batch_loss: 2.1671, loss: 2.1054 ||:  65%|######4   | 333/516 [27:34<15:51,  5.20s/it]
2025-02-04 05:05:38,619 - INFO - tqdm - perplexity: 8.2179, batch_loss: 2.3143, loss: 2.1063 ||:  65%|######4   | 335/516 [27:46<17:09,  5.69s/it]
2025-02-04 05:05:53,283 - INFO - tqdm - perplexity: 8.2199, batch_loss: 2.1222, loss: 2.1066 ||:  66%|######5   | 338/516 [28:00<15:38,  5.27s/it]
2025-02-04 05:06:04,281 - INFO - tqdm - perplexity: 8.2131, batch_loss: 1.9172, loss: 2.1057 ||:  66%|######5   | 340/516 [28:11<15:55,  5.43s/it]
2025-02-04 05:06:14,625 - INFO - tqdm - perplexity: 8.2156, batch_loss: 2.0212, loss: 2.1060 ||:  66%|######6   | 342/516 [28:22<15:32,  5.36s/it]
2025-02-04 05:06:25,804 - INFO - tqdm - perplexity: 8.2124, batch_loss: 1.8166, loss: 2.1056 ||:  67%|######6   | 344/516 [28:33<15:32,  5.42s/it]
2025-02-04 05:06:39,851 - INFO - tqdm - perplexity: 8.2060, batch_loss: 1.7927, loss: 2.1049 ||:  67%|######7   | 347/516 [28:47<14:10,  5.03s/it]
2025-02-04 05:06:53,465 - INFO - tqdm - perplexity: 8.1976, batch_loss: 1.7993, loss: 2.1038 ||:  68%|######7   | 350/516 [29:01<12:53,  4.66s/it]
2025-02-04 05:07:05,001 - INFO - tqdm - perplexity: 8.1847, batch_loss: 1.9050, loss: 2.1023 ||:  68%|######8   | 352/516 [29:12<14:03,  5.15s/it]
2025-02-04 05:07:18,987 - INFO - tqdm - perplexity: 8.1742, batch_loss: 1.9725, loss: 2.1010 ||:  69%|######8   | 355/516 [29:26<13:01,  4.85s/it]
2025-02-04 05:07:35,883 - INFO - tqdm - perplexity: 8.1602, batch_loss: 1.7974, loss: 2.0993 ||:  69%|######9   | 358/516 [29:43<14:33,  5.53s/it]
2025-02-04 05:07:49,256 - INFO - tqdm - perplexity: 8.1607, batch_loss: 2.2859, loss: 2.0993 ||:  70%|######9   | 361/516 [29:56<12:26,  4.82s/it]
2025-02-04 05:08:00,050 - INFO - tqdm - perplexity: 8.1531, batch_loss: 1.7731, loss: 2.0984 ||:  70%|#######   | 363/516 [30:07<13:11,  5.17s/it]
2025-02-04 05:08:13,229 - INFO - tqdm - perplexity: 8.1455, batch_loss: 2.0223, loss: 2.0975 ||:  71%|#######   | 366/516 [30:20<11:38,  4.66s/it]
2025-02-04 05:08:24,200 - INFO - tqdm - perplexity: 8.1351, batch_loss: 1.9674, loss: 2.0962 ||:  71%|#######1  | 368/516 [30:31<12:25,  5.04s/it]
2025-02-04 05:08:35,517 - INFO - tqdm - perplexity: 8.1332, batch_loss: 1.9846, loss: 2.0960 ||:  72%|#######1  | 370/516 [30:43<13:04,  5.38s/it]
2025-02-04 05:08:45,998 - INFO - tqdm - perplexity: 8.1276, batch_loss: 1.9057, loss: 2.0953 ||:  72%|#######2  | 372/516 [30:53<12:38,  5.27s/it]
2025-02-04 05:08:57,202 - INFO - tqdm - perplexity: 8.1202, batch_loss: 2.0650, loss: 2.0944 ||:  72%|#######2  | 374/516 [31:04<12:41,  5.36s/it]
2025-02-04 05:09:07,675 - INFO - tqdm - perplexity: 8.1205, batch_loss: 2.1565, loss: 2.0944 ||:  73%|#######2  | 376/516 [31:15<12:30,  5.36s/it]
2025-02-04 05:09:22,319 - INFO - tqdm - perplexity: 8.1234, batch_loss: 2.3512, loss: 2.0948 ||:  73%|#######3  | 379/516 [31:30<11:35,  5.08s/it]
2025-02-04 05:09:36,124 - INFO - tqdm - perplexity: 8.1153, batch_loss: 2.0619, loss: 2.0938 ||:  74%|#######4  | 382/516 [31:43<10:19,  4.62s/it]
2025-02-04 05:09:47,304 - INFO - tqdm - perplexity: 8.1123, batch_loss: 1.9284, loss: 2.0934 ||:  74%|#######4  | 384/516 [31:55<11:21,  5.17s/it]
2025-02-04 05:09:57,463 - INFO - tqdm - perplexity: 8.1129, batch_loss: 1.9733, loss: 2.0935 ||:  75%|#######4  | 386/516 [32:05<10:52,  5.02s/it]
2025-02-04 05:10:08,317 - INFO - tqdm - perplexity: 8.1107, batch_loss: 1.9667, loss: 2.0932 ||:  75%|#######5  | 388/516 [32:16<11:11,  5.25s/it]
2025-02-04 05:10:25,761 - INFO - tqdm - perplexity: 8.1080, batch_loss: 1.9937, loss: 2.0929 ||:  76%|#######5  | 391/516 [32:33<12:14,  5.88s/it]
2025-02-04 05:10:38,989 - INFO - tqdm - perplexity: 8.1021, batch_loss: 1.9598, loss: 2.0921 ||:  76%|#######6  | 394/516 [32:46<09:57,  4.90s/it]
2025-02-04 05:10:53,553 - INFO - tqdm - perplexity: 8.0886, batch_loss: 1.9706, loss: 2.0905 ||:  77%|#######6  | 397/516 [33:01<09:44,  4.91s/it]
2025-02-04 05:11:05,538 - INFO - tqdm - perplexity: 8.0811, batch_loss: 2.0328, loss: 2.0895 ||:  77%|#######7  | 399/516 [33:13<10:39,  5.46s/it]
2025-02-04 05:11:19,174 - INFO - tqdm - perplexity: 8.0755, batch_loss: 2.0790, loss: 2.0888 ||:  78%|#######7  | 402/516 [33:26<09:07,  4.81s/it]
2025-02-04 05:11:32,576 - INFO - tqdm - perplexity: 8.0729, batch_loss: 2.0254, loss: 2.0885 ||:  78%|#######8  | 405/516 [33:40<08:32,  4.62s/it]
2025-02-04 05:11:46,602 - INFO - tqdm - perplexity: 8.0708, batch_loss: 1.9975, loss: 2.0882 ||:  79%|#######9  | 408/516 [33:54<08:25,  4.68s/it]
2025-02-04 05:11:56,692 - INFO - tqdm - perplexity: 8.0756, batch_loss: 2.1956, loss: 2.0889 ||:  79%|#######9  | 410/516 [34:04<08:33,  4.84s/it]
2025-02-04 05:12:13,366 - INFO - tqdm - perplexity: 8.0668, batch_loss: 1.8649, loss: 2.0878 ||:  80%|########  | 413/516 [34:21<09:25,  5.49s/it]
2025-02-04 05:12:28,846 - INFO - tqdm - perplexity: 8.0638, batch_loss: 1.7586, loss: 2.0874 ||:  81%|########  | 416/516 [34:36<09:00,  5.41s/it]
2025-02-04 05:12:44,519 - INFO - tqdm - perplexity: 8.0551, batch_loss: 1.7601, loss: 2.0863 ||:  81%|########1 | 419/516 [34:52<08:47,  5.44s/it]
2025-02-04 05:12:58,547 - INFO - tqdm - perplexity: 8.0460, batch_loss: 1.8110, loss: 2.0852 ||:  82%|########1 | 422/516 [35:06<07:44,  4.94s/it]
2025-02-04 05:13:12,266 - INFO - tqdm - perplexity: 8.0374, batch_loss: 1.8533, loss: 2.0841 ||:  82%|########2 | 425/516 [35:19<07:11,  4.74s/it]
2025-02-04 05:13:26,238 - INFO - tqdm - perplexity: 8.0350, batch_loss: 2.0390, loss: 2.0838 ||:  83%|########2 | 428/516 [35:33<06:49,  4.65s/it]
2025-02-04 05:13:39,203 - INFO - tqdm - perplexity: 8.0265, batch_loss: 2.0040, loss: 2.0828 ||:  84%|########3 | 431/516 [35:46<06:16,  4.43s/it]
2025-02-04 05:13:54,291 - INFO - tqdm - perplexity: 8.0221, batch_loss: 1.8736, loss: 2.0822 ||:  84%|########4 | 434/516 [36:01<06:40,  4.88s/it]
2025-02-04 05:14:06,677 - INFO - tqdm - perplexity: 8.0131, batch_loss: 1.8982, loss: 2.0811 ||:  84%|########4 | 436/516 [36:14<07:23,  5.55s/it]
2025-02-04 05:14:20,108 - INFO - tqdm - perplexity: 8.0083, batch_loss: 1.9142, loss: 2.0805 ||:  85%|########5 | 439/516 [36:27<06:09,  4.80s/it]
2025-02-04 05:14:31,011 - INFO - tqdm - perplexity: 8.0092, batch_loss: 2.0967, loss: 2.0806 ||:  85%|########5 | 441/516 [36:38<06:23,  5.11s/it]
2025-02-04 05:14:42,793 - INFO - tqdm - perplexity: 8.0062, batch_loss: 2.1667, loss: 2.0802 ||:  86%|########6 | 444/516 [36:50<05:09,  4.30s/it]
2025-02-04 05:14:53,009 - INFO - tqdm - perplexity: 8.0004, batch_loss: 1.9936, loss: 2.0795 ||:  86%|########6 | 446/516 [37:00<05:27,  4.67s/it]
2025-02-04 05:15:03,033 - INFO - tqdm - perplexity: 7.9954, batch_loss: 1.9255, loss: 2.0789 ||:  87%|########6 | 448/516 [37:10<05:33,  4.91s/it]
2025-02-04 05:15:13,332 - INFO - tqdm - perplexity: 7.9874, batch_loss: 1.9787, loss: 2.0779 ||:  87%|########7 | 450/516 [37:21<05:28,  4.98s/it]
2025-02-04 05:15:26,742 - INFO - tqdm - perplexity: 7.9817, batch_loss: 2.0631, loss: 2.0772 ||:  88%|########7 | 453/516 [37:34<04:54,  4.68s/it]
2025-02-04 05:15:36,756 - INFO - tqdm - perplexity: 7.9820, batch_loss: 2.0597, loss: 2.0772 ||:  88%|########8 | 455/516 [37:44<04:50,  4.76s/it]
2025-02-04 05:15:47,878 - INFO - tqdm - perplexity: 7.9784, batch_loss: 1.9254, loss: 2.0767 ||:  89%|########8 | 457/516 [37:55<05:06,  5.20s/it]
2025-02-04 05:16:00,267 - INFO - tqdm - perplexity: 7.9742, batch_loss: 1.9112, loss: 2.0762 ||:  89%|########9 | 460/516 [38:07<04:11,  4.50s/it]
2025-02-04 05:16:13,391 - INFO - tqdm - perplexity: 7.9680, batch_loss: 1.9714, loss: 2.0754 ||:  90%|########9 | 463/516 [38:21<03:51,  4.37s/it]
2025-02-04 05:16:23,528 - INFO - tqdm - perplexity: 7.9668, batch_loss: 1.9643, loss: 2.0753 ||:  90%|######### | 465/516 [38:31<03:59,  4.69s/it]
2025-02-04 05:16:35,350 - INFO - tqdm - perplexity: 7.9693, batch_loss: 2.1758, loss: 2.0756 ||:  91%|######### | 468/516 [38:43<03:23,  4.23s/it]
2025-02-04 05:16:48,975 - INFO - tqdm - perplexity: 7.9670, batch_loss: 2.1929, loss: 2.0753 ||:  91%|#########1| 470/516 [38:56<04:15,  5.56s/it]
2025-02-04 05:17:03,248 - INFO - tqdm - perplexity: 7.9593, batch_loss: 1.9974, loss: 2.0743 ||:  92%|#########1| 473/516 [39:10<03:35,  5.02s/it]
2025-02-04 05:17:17,770 - INFO - tqdm - perplexity: 7.9571, batch_loss: 2.0872, loss: 2.0741 ||:  92%|#########2| 476/516 [39:25<03:17,  4.95s/it]
2025-02-04 05:17:30,127 - INFO - tqdm - perplexity: 7.9478, batch_loss: 1.8112, loss: 2.0729 ||:  93%|#########2| 478/516 [39:37<03:30,  5.53s/it]
2025-02-04 05:17:41,869 - INFO - tqdm - perplexity: 7.9515, batch_loss: 2.1164, loss: 2.0734 ||:  93%|#########3| 480/516 [39:49<03:24,  5.67s/it]
2025-02-04 05:17:55,880 - INFO - tqdm - perplexity: 7.9455, batch_loss: 1.8691, loss: 2.0726 ||:  94%|#########3| 483/516 [40:03<02:49,  5.14s/it]
2025-02-04 05:18:11,095 - INFO - tqdm - perplexity: 7.9398, batch_loss: 1.9358, loss: 2.0719 ||:  94%|#########4| 486/516 [40:18<02:34,  5.16s/it]
2025-02-04 05:18:24,334 - INFO - tqdm - perplexity: 7.9387, batch_loss: 1.9447, loss: 2.0718 ||:  95%|#########4| 489/516 [40:32<02:04,  4.62s/it]
2025-02-04 05:18:37,949 - INFO - tqdm - perplexity: 7.9339, batch_loss: 1.9508, loss: 2.0711 ||:  95%|#########5| 492/516 [40:45<01:50,  4.59s/it]
2025-02-04 05:18:51,841 - INFO - tqdm - perplexity: 7.9322, batch_loss: 2.0639, loss: 2.0709 ||:  96%|#########5| 495/516 [40:59<01:35,  4.55s/it]
2025-02-04 05:19:06,812 - INFO - tqdm - perplexity: 7.9288, batch_loss: 2.0351, loss: 2.0705 ||:  97%|#########6| 498/516 [41:14<01:26,  4.81s/it]
2025-02-04 05:19:19,946 - INFO - tqdm - perplexity: 7.9242, batch_loss: 1.9505, loss: 2.0699 ||:  97%|#########6| 500/516 [41:27<01:31,  5.69s/it]
2025-02-04 05:19:31,030 - INFO - tqdm - perplexity: 7.9176, batch_loss: 1.7217, loss: 2.0691 ||:  97%|#########7| 502/516 [41:38<01:20,  5.72s/it]
2025-02-04 05:19:44,975 - INFO - tqdm - perplexity: 7.9164, batch_loss: 2.1612, loss: 2.0689 ||:  98%|#########7| 505/516 [41:52<00:55,  5.02s/it]
2025-02-04 05:19:59,368 - INFO - tqdm - perplexity: 7.9048, batch_loss: 1.8364, loss: 2.0675 ||:  98%|#########8| 508/516 [42:07<00:38,  4.84s/it]
2025-02-04 05:20:13,556 - INFO - tqdm - perplexity: 7.8974, batch_loss: 1.8987, loss: 2.0665 ||:  99%|#########9| 511/516 [42:21<00:24,  4.83s/it]
2025-02-04 05:20:23,692 - INFO - tqdm - perplexity: 7.8928, batch_loss: 1.9067, loss: 2.0660 ||:  99%|#########9| 513/516 [42:31<00:15,  5.05s/it]
2025-02-04 05:20:28,923 - INFO - tqdm - perplexity: 7.8915, batch_loss: 1.9784, loss: 2.0658 ||: 100%|#########9| 514/516 [42:36<00:10,  5.10s/it]
2025-02-04 05:20:32,569 - INFO - tqdm - perplexity: 7.8910, batch_loss: 2.0339, loss: 2.0657 ||: 100%|#########9| 515/516 [42:40<00:04,  4.66s/it]
2025-02-04 05:20:33,621 - INFO - tqdm - perplexity: 7.8824, batch_loss: 1.5021, loss: 2.0646 ||: 100%|##########| 516/516 [42:41<00:00,  3.58s/it]
2025-02-04 05:20:33,622 - INFO - tqdm - perplexity: 7.8824, batch_loss: 1.5021, loss: 2.0646 ||: 100%|##########| 516/516 [42:41<00:00,  4.96s/it]
2025-02-04 05:20:33,629 - INFO - allennlp.training.gradient_descent_trainer - Validating
2025-02-04 05:20:33,630 - INFO - tqdm - 0%|          | 0/129 [00:00<?, ?it/s]
2025-02-04 05:20:44,127 - INFO - tqdm - perplexity: 12.4055, batch_loss: 2.6939, loss: 2.5181 ||:   6%|6         | 8/129 [00:10<02:40,  1.33s/it]
2025-02-04 05:20:54,253 - INFO - tqdm - perplexity: 13.3895, batch_loss: 2.8333, loss: 2.5945 ||:  12%|#2        | 16/129 [00:20<02:20,  1.24s/it]
2025-02-04 05:21:04,897 - INFO - tqdm - perplexity: 13.3765, batch_loss: 2.5794, loss: 2.5935 ||:  19%|#8        | 24/129 [00:31<02:21,  1.34s/it]
2025-02-04 05:21:15,927 - INFO - tqdm - perplexity: 13.4738, batch_loss: 3.0009, loss: 2.6007 ||:  26%|##5       | 33/129 [00:42<01:57,  1.22s/it]
2025-02-04 05:21:26,640 - INFO - tqdm - perplexity: 13.2695, batch_loss: 2.6831, loss: 2.5855 ||:  32%|###1      | 41/129 [00:53<01:56,  1.32s/it]
2025-02-04 05:21:36,832 - INFO - tqdm - perplexity: 13.0602, batch_loss: 2.3500, loss: 2.5696 ||:  38%|###7      | 49/129 [01:03<01:43,  1.30s/it]
2025-02-04 05:21:47,156 - INFO - tqdm - perplexity: 13.0176, batch_loss: 2.7055, loss: 2.5663 ||:  44%|####4     | 57/129 [01:13<01:35,  1.32s/it]
2025-02-04 05:21:57,179 - INFO - tqdm - perplexity: 13.4130, batch_loss: 2.7485, loss: 2.5962 ||:  50%|#####     | 65/129 [01:23<01:24,  1.32s/it]
2025-02-04 05:22:07,403 - INFO - tqdm - perplexity: 13.5448, batch_loss: 2.8324, loss: 2.6060 ||:  57%|#####6    | 73/129 [01:33<01:13,  1.31s/it]
2025-02-04 05:22:17,565 - INFO - tqdm - perplexity: 13.6229, batch_loss: 2.6529, loss: 2.6118 ||:  63%|######2   | 81/129 [01:43<01:02,  1.29s/it]
2025-02-04 05:22:27,667 - INFO - tqdm - perplexity: 13.8056, batch_loss: 2.9316, loss: 2.6251 ||:  69%|######8   | 89/129 [01:54<00:52,  1.30s/it]
2025-02-04 05:22:38,667 - INFO - tqdm - perplexity: 13.8003, batch_loss: 2.8219, loss: 2.6247 ||:  76%|#######5  | 98/129 [02:05<00:37,  1.20s/it]
2025-02-04 05:22:48,715 - INFO - tqdm - perplexity: 13.8398, batch_loss: 2.6095, loss: 2.6276 ||:  82%|########2 | 106/129 [02:15<00:28,  1.24s/it]
2025-02-04 05:22:59,834 - INFO - tqdm - perplexity: 13.8992, batch_loss: 2.7121, loss: 2.6318 ||:  89%|########9 | 115/129 [02:26<00:16,  1.21s/it]
2025-02-04 05:23:10,242 - INFO - tqdm - perplexity: 14.0096, batch_loss: 2.8069, loss: 2.6397 ||:  95%|#########5| 123/129 [02:36<00:08,  1.37s/it]
2025-02-04 05:23:17,452 - INFO - tqdm - perplexity: 13.9457, batch_loss: 2.4715, loss: 2.6352 ||: 100%|##########| 129/129 [02:43<00:00,  1.20s/it]
2025-02-04 05:23:17,452 - INFO - tqdm - perplexity: 13.9457, batch_loss: 2.4715, loss: 2.6352 ||: 100%|##########| 129/129 [02:43<00:00,  1.27s/it]
2025-02-04 05:23:17,454 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2025-02-04 05:23:17,454 - INFO - allennlp.training.callbacks.console_logger - loss               |     2.065  |     2.635
2025-02-04 05:23:17,454 - INFO - allennlp.training.callbacks.console_logger - perplexity         |     7.882  |    13.946
2025-02-04 05:23:17,454 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |     0.000  |       N/A
2025-02-04 05:23:18,077 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:45:25.778379
2025-02-04 05:23:18,077 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 19:38:53
2025-02-04 05:23:18,078 - INFO - allennlp.training.gradient_descent_trainer - Epoch 6/31
2025-02-04 05:23:18,078 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 0B
2025-02-04 05:23:18,078 - INFO - allennlp.training.gradient_descent_trainer - Training
2025-02-04 05:23:18,078 - INFO - tqdm - 0%|          | 0/516 [00:00<?, ?it/s]
2025-02-04 05:23:34,553 - INFO - tqdm - perplexity: 5.5003, batch_loss: 1.6668, loss: 1.7048 ||:   1%|          | 3/516 [00:16<49:56,  5.84s/it]
2025-02-04 05:23:45,874 - INFO - tqdm - perplexity: 5.8799, batch_loss: 1.8399, loss: 1.7715 ||:   1%|          | 5/516 [00:27<49:42,  5.84s/it]
2025-02-04 05:23:58,391 - INFO - tqdm - perplexity: 5.9632, batch_loss: 1.9161, loss: 1.7856 ||:   1%|1         | 7/516 [00:40<53:08,  6.26s/it]
2025-02-04 05:24:13,393 - INFO - tqdm - perplexity: 5.7438, batch_loss: 1.5327, loss: 1.7481 ||:   2%|1         | 10/516 [00:55<47:50,  5.67s/it]
2025-02-04 05:24:26,780 - INFO - tqdm - perplexity: 5.7228, batch_loss: 1.8791, loss: 1.7445 ||:   3%|2         | 13/516 [01:08<40:31,  4.83s/it]
2025-02-04 05:24:39,611 - INFO - tqdm - perplexity: 5.7699, batch_loss: 1.8337, loss: 1.7527 ||:   3%|2         | 15/516 [01:21<46:56,  5.62s/it]
2025-02-04 05:24:53,093 - INFO - tqdm - perplexity: 5.7755, batch_loss: 1.8370, loss: 1.7536 ||:   3%|3         | 18/516 [01:35<40:27,  4.87s/it]
2025-02-04 05:25:07,408 - INFO - tqdm - perplexity: 5.8100, batch_loss: 1.7096, loss: 1.7596 ||:   4%|4         | 21/516 [01:49<40:19,  4.89s/it]
2025-02-04 05:25:21,345 - INFO - tqdm - perplexity: 5.8230, batch_loss: 1.7310, loss: 1.7618 ||:   5%|4         | 24/516 [02:03<38:21,  4.68s/it]
2025-02-04 05:25:35,074 - INFO - tqdm - perplexity: 5.8979, batch_loss: 1.7631, loss: 1.7746 ||:   5%|5         | 27/516 [02:16<37:24,  4.59s/it]
2025-02-04 05:25:49,192 - INFO - tqdm - perplexity: 5.9211, batch_loss: 1.6873, loss: 1.7785 ||:   6%|5         | 30/516 [02:31<37:26,  4.62s/it]
2025-02-04 05:26:01,193 - INFO - tqdm - perplexity: 5.9596, batch_loss: 1.8796, loss: 1.7850 ||:   6%|6         | 33/516 [02:43<33:35,  4.17s/it]
2025-02-04 05:26:13,995 - INFO - tqdm - perplexity: 5.9973, batch_loss: 1.9222, loss: 1.7913 ||:   7%|6         | 36/516 [02:55<33:45,  4.22s/it]
2025-02-04 05:26:28,804 - INFO - tqdm - perplexity: 6.0379, batch_loss: 1.7299, loss: 1.7981 ||:   8%|7         | 39/516 [03:10<37:22,  4.70s/it]
2025-02-04 05:26:39,945 - INFO - tqdm - perplexity: 6.0328, batch_loss: 1.6966, loss: 1.7972 ||:   8%|7         | 41/516 [03:21<40:51,  5.16s/it]
2025-02-04 05:26:53,907 - INFO - tqdm - perplexity: 6.0226, batch_loss: 1.7071, loss: 1.7955 ||:   9%|8         | 44/516 [03:35<38:21,  4.88s/it]
2025-02-04 05:27:08,009 - INFO - tqdm - perplexity: 5.9742, batch_loss: 1.5981, loss: 1.7875 ||:   9%|9         | 47/516 [03:49<38:00,  4.86s/it]
2025-02-04 05:27:21,807 - INFO - tqdm - perplexity: 5.9836, batch_loss: 1.6895, loss: 1.7890 ||:  10%|9         | 50/516 [04:03<36:08,  4.65s/it]
2025-02-04 05:27:36,307 - INFO - tqdm - perplexity: 5.9931, batch_loss: 1.9602, loss: 1.7906 ||:  10%|#         | 53/516 [04:18<36:53,  4.78s/it]
2025-02-04 05:27:50,515 - INFO - tqdm - perplexity: 5.9592, batch_loss: 1.8203, loss: 1.7849 ||:  11%|#         | 56/516 [04:32<36:33,  4.77s/it]
2025-02-04 05:28:00,739 - INFO - tqdm - perplexity: 5.9369, batch_loss: 1.7768, loss: 1.7812 ||:  11%|#1        | 58/516 [04:42<37:04,  4.86s/it]
2025-02-04 05:28:11,207 - INFO - tqdm - perplexity: 5.9185, batch_loss: 1.7864, loss: 1.7781 ||:  12%|#1        | 60/516 [04:53<37:34,  4.94s/it]
2025-02-04 05:28:24,146 - INFO - tqdm - perplexity: 5.8956, batch_loss: 1.7927, loss: 1.7742 ||:  12%|#2        | 62/516 [05:06<42:56,  5.67s/it]
2025-02-04 05:28:38,634 - INFO - tqdm - perplexity: 5.9391, batch_loss: 1.8561, loss: 1.7816 ||:  13%|#2        | 65/516 [05:20<38:44,  5.15s/it]
2025-02-04 05:28:49,482 - INFO - tqdm - perplexity: 5.9374, batch_loss: 1.8337, loss: 1.7813 ||:  13%|#2        | 67/516 [05:31<39:24,  5.27s/it]
2025-02-04 05:29:04,334 - INFO - tqdm - perplexity: 5.9398, batch_loss: 1.7571, loss: 1.7817 ||:  14%|#3        | 70/516 [05:46<37:58,  5.11s/it]
2025-02-04 05:29:18,251 - INFO - tqdm - perplexity: 5.9508, batch_loss: 1.8642, loss: 1.7835 ||:  14%|#4        | 73/516 [06:00<35:18,  4.78s/it]
2025-02-04 05:29:32,907 - INFO - tqdm - perplexity: 5.9433, batch_loss: 1.6592, loss: 1.7823 ||:  15%|#4        | 76/516 [06:14<36:50,  5.02s/it]
2025-02-04 05:29:45,131 - INFO - tqdm - perplexity: 5.9410, batch_loss: 1.8274, loss: 1.7819 ||:  15%|#5        | 78/516 [06:27<41:31,  5.69s/it]
2025-02-04 05:29:59,197 - INFO - tqdm - perplexity: 5.9228, batch_loss: 1.6382, loss: 1.7788 ||:  16%|#5        | 81/516 [06:41<36:34,  5.04s/it]
2025-02-04 05:30:15,038 - INFO - tqdm - perplexity: 5.9021, batch_loss: 1.6729, loss: 1.7753 ||:  16%|#6        | 84/516 [06:56<37:20,  5.19s/it]
2025-02-04 05:30:29,039 - INFO - tqdm - perplexity: 5.9188, batch_loss: 1.8364, loss: 1.7781 ||:  17%|#6        | 87/516 [07:10<34:34,  4.84s/it]
2025-02-04 05:30:42,687 - INFO - tqdm - perplexity: 5.9111, batch_loss: 1.6936, loss: 1.7768 ||:  17%|#7        | 90/516 [07:24<33:34,  4.73s/it]
2025-02-04 05:30:57,031 - INFO - tqdm - perplexity: 5.9240, batch_loss: 1.9701, loss: 1.7790 ||:  18%|#8        | 93/516 [07:38<33:38,  4.77s/it]
2025-02-04 05:31:07,838 - INFO - tqdm - perplexity: 5.9319, batch_loss: 1.8918, loss: 1.7803 ||:  18%|#8        | 95/516 [07:49<35:51,  5.11s/it]
2025-02-04 05:31:22,134 - INFO - tqdm - perplexity: 5.9285, batch_loss: 1.7807, loss: 1.7798 ||:  19%|#8        | 98/516 [08:04<33:45,  4.85s/it]
2025-02-04 05:31:33,831 - INFO - tqdm - perplexity: 5.9354, batch_loss: 1.9782, loss: 1.7809 ||:  19%|#9        | 100/516 [08:15<36:36,  5.28s/it]
2025-02-04 05:31:46,256 - INFO - tqdm - perplexity: 5.9170, batch_loss: 1.6910, loss: 1.7778 ||:  20%|#9        | 102/516 [08:28<39:44,  5.76s/it]
2025-02-04 05:31:57,047 - INFO - tqdm - perplexity: 5.9098, batch_loss: 1.7613, loss: 1.7766 ||:  20%|##        | 104/516 [08:38<37:47,  5.50s/it]
2025-02-04 05:32:11,297 - INFO - tqdm - perplexity: 5.9110, batch_loss: 1.8243, loss: 1.7768 ||:  21%|##        | 107/516 [08:53<33:59,  4.99s/it]
2025-02-04 05:32:21,974 - INFO - tqdm - perplexity: 5.9088, batch_loss: 1.7602, loss: 1.7764 ||:  21%|##1       | 109/516 [09:03<34:26,  5.08s/it]
2025-02-04 05:32:32,213 - INFO - tqdm - perplexity: 5.9103, batch_loss: 1.7397, loss: 1.7767 ||:  22%|##1       | 111/516 [09:14<34:14,  5.07s/it]
2025-02-04 05:32:47,482 - INFO - tqdm - perplexity: 5.9057, batch_loss: 1.8600, loss: 1.7759 ||:  22%|##2       | 114/516 [09:29<34:21,  5.13s/it]
2025-02-04 05:33:02,801 - INFO - tqdm - perplexity: 5.9039, batch_loss: 1.8559, loss: 1.7756 ||:  23%|##2       | 117/516 [09:44<34:26,  5.18s/it]
2025-02-04 05:33:14,019 - INFO - tqdm - perplexity: 5.8981, batch_loss: 1.7670, loss: 1.7746 ||:  23%|##3       | 119/516 [09:55<36:13,  5.48s/it]
2025-02-04 05:33:25,758 - INFO - tqdm - perplexity: 5.9064, batch_loss: 2.1049, loss: 1.7760 ||:  23%|##3       | 121/516 [10:07<38:20,  5.83s/it]
2025-02-04 05:33:40,355 - INFO - tqdm - perplexity: 5.9172, batch_loss: 1.8335, loss: 1.7779 ||:  24%|##4       | 124/516 [10:22<34:26,  5.27s/it]
2025-02-04 05:33:50,468 - INFO - tqdm - perplexity: 5.9102, batch_loss: 1.7361, loss: 1.7767 ||:  24%|##4       | 126/516 [10:32<33:35,  5.17s/it]
2025-02-04 05:34:00,955 - INFO - tqdm - perplexity: 5.9155, batch_loss: 1.8469, loss: 1.7776 ||:  25%|##4       | 128/516 [10:42<33:32,  5.19s/it]
2025-02-04 05:34:12,008 - INFO - tqdm - perplexity: 5.9089, batch_loss: 1.7435, loss: 1.7765 ||:  25%|##5       | 130/516 [10:53<34:28,  5.36s/it]
2025-02-04 05:34:26,765 - INFO - tqdm - perplexity: 5.9093, batch_loss: 1.7598, loss: 1.7765 ||:  26%|##5       | 133/516 [11:08<32:18,  5.06s/it]
2025-02-04 05:34:39,903 - INFO - tqdm - perplexity: 5.9261, batch_loss: 1.9062, loss: 1.7794 ||:  26%|##6       | 136/516 [11:21<29:43,  4.69s/it]
2025-02-04 05:34:52,815 - INFO - tqdm - perplexity: 5.9084, batch_loss: 1.6372, loss: 1.7764 ||:  27%|##6       | 139/516 [11:34<28:04,  4.47s/it]
2025-02-04 05:35:03,154 - INFO - tqdm - perplexity: 5.9024, batch_loss: 1.5350, loss: 1.7754 ||:  27%|##7       | 141/516 [11:45<30:30,  4.88s/it]
2025-02-04 05:35:13,933 - INFO - tqdm - perplexity: 5.9068, batch_loss: 1.8601, loss: 1.7761 ||:  28%|##7       | 143/516 [11:55<32:12,  5.18s/it]
2025-02-04 05:35:27,561 - INFO - tqdm - perplexity: 5.9090, batch_loss: 1.8001, loss: 1.7765 ||:  28%|##8       | 146/516 [12:09<29:48,  4.83s/it]
2025-02-04 05:35:41,856 - INFO - tqdm - perplexity: 5.9122, batch_loss: 1.7159, loss: 1.7770 ||:  29%|##8       | 149/516 [12:23<29:26,  4.81s/it]
2025-02-04 05:35:55,291 - INFO - tqdm - perplexity: 5.9175, batch_loss: 1.8281, loss: 1.7779 ||:  29%|##9       | 152/516 [12:37<28:03,  4.62s/it]
2025-02-04 05:36:07,154 - INFO - tqdm - perplexity: 5.9271, batch_loss: 1.8686, loss: 1.7795 ||:  30%|###       | 155/516 [12:49<25:09,  4.18s/it]
2025-02-04 05:36:17,301 - INFO - tqdm - perplexity: 5.9274, batch_loss: 1.6887, loss: 1.7796 ||:  30%|###       | 157/516 [12:59<28:06,  4.70s/it]
2025-02-04 05:36:31,297 - INFO - tqdm - perplexity: 5.9264, batch_loss: 1.7078, loss: 1.7794 ||:  31%|###1      | 160/516 [13:13<27:19,  4.61s/it]
2025-02-04 05:36:46,062 - INFO - tqdm - perplexity: 5.9342, batch_loss: 1.9829, loss: 1.7807 ||:  32%|###1      | 163/516 [13:27<28:20,  4.82s/it]
2025-02-04 05:37:00,015 - INFO - tqdm - perplexity: 5.9322, batch_loss: 1.6964, loss: 1.7804 ||:  32%|###2      | 166/516 [13:41<27:31,  4.72s/it]
2025-02-04 05:37:10,735 - INFO - tqdm - perplexity: 5.9259, batch_loss: 1.6809, loss: 1.7793 ||:  33%|###2      | 168/516 [13:52<29:32,  5.09s/it]
2025-02-04 05:37:25,485 - INFO - tqdm - perplexity: 5.9181, batch_loss: 1.6193, loss: 1.7780 ||:  33%|###3      | 171/516 [14:07<29:25,  5.12s/it]
2025-02-04 05:37:40,051 - INFO - tqdm - perplexity: 5.9207, batch_loss: 1.6708, loss: 1.7785 ||:  34%|###3      | 174/516 [14:21<27:53,  4.89s/it]
2025-02-04 05:37:50,643 - INFO - tqdm - perplexity: 5.9188, batch_loss: 1.6029, loss: 1.7781 ||:  34%|###4      | 176/516 [14:32<29:05,  5.13s/it]
2025-02-04 05:38:05,184 - INFO - tqdm - perplexity: 5.9126, batch_loss: 1.6960, loss: 1.7771 ||:  35%|###4      | 179/516 [14:47<27:41,  4.93s/it]
2025-02-04 05:38:17,147 - INFO - tqdm - perplexity: 5.9101, batch_loss: 1.5847, loss: 1.7767 ||:  35%|###5      | 182/516 [14:59<24:20,  4.37s/it]
2025-02-04 05:38:27,728 - INFO - tqdm - perplexity: 5.8946, batch_loss: 1.5564, loss: 1.7740 ||:  36%|###5      | 184/516 [15:09<27:07,  4.90s/it]
2025-02-04 05:38:40,464 - INFO - tqdm - perplexity: 5.8871, batch_loss: 1.7428, loss: 1.7728 ||:  36%|###6      | 187/516 [15:22<24:43,  4.51s/it]
2025-02-04 05:38:56,014 - INFO - tqdm - perplexity: 5.8892, batch_loss: 1.9745, loss: 1.7731 ||:  37%|###6      | 190/516 [15:37<27:45,  5.11s/it]
2025-02-04 05:39:10,755 - INFO - tqdm - perplexity: 5.8846, batch_loss: 1.6550, loss: 1.7723 ||:  37%|###7      | 193/516 [15:52<27:14,  5.06s/it]
2025-02-04 05:39:21,065 - INFO - tqdm - perplexity: 5.8840, batch_loss: 1.8490, loss: 1.7722 ||:  38%|###7      | 195/516 [16:02<27:15,  5.10s/it]
2025-02-04 05:39:35,096 - INFO - tqdm - perplexity: 5.8786, batch_loss: 1.6298, loss: 1.7713 ||:  38%|###8      | 198/516 [16:17<25:24,  4.79s/it]
2025-02-04 05:39:50,464 - INFO - tqdm - perplexity: 5.8764, batch_loss: 1.6665, loss: 1.7709 ||:  39%|###8      | 201/516 [16:32<26:17,  5.01s/it]
2025-02-04 05:40:01,641 - INFO - tqdm - perplexity: 5.8722, batch_loss: 1.6497, loss: 1.7702 ||:  39%|###9      | 203/516 [16:43<27:44,  5.32s/it]
2025-02-04 05:40:14,291 - INFO - tqdm - perplexity: 5.8778, batch_loss: 1.8316, loss: 1.7712 ||:  40%|###9      | 206/516 [16:56<24:17,  4.70s/it]
2025-02-04 05:40:28,771 - INFO - tqdm - perplexity: 5.8746, batch_loss: 1.4192, loss: 1.7706 ||:  41%|####      | 209/516 [17:10<25:04,  4.90s/it]
2025-02-04 05:40:39,180 - INFO - tqdm - perplexity: 5.8728, batch_loss: 1.5441, loss: 1.7703 ||:  41%|####      | 211/516 [17:21<26:17,  5.17s/it]
2025-02-04 05:40:51,216 - INFO - tqdm - perplexity: 5.8625, batch_loss: 1.4042, loss: 1.7686 ||:  41%|####1     | 213/516 [17:33<28:49,  5.71s/it]
2025-02-04 05:41:02,022 - INFO - tqdm - perplexity: 5.8571, batch_loss: 1.5973, loss: 1.7677 ||:  42%|####1     | 215/516 [17:43<27:56,  5.57s/it]
2025-02-04 05:41:15,549 - INFO - tqdm - perplexity: 5.8507, batch_loss: 1.7052, loss: 1.7666 ||:  42%|####2     | 218/516 [17:57<24:12,  4.87s/it]
2025-02-04 05:41:26,966 - INFO - tqdm - perplexity: 5.8529, batch_loss: 1.8846, loss: 1.7669 ||:  43%|####2     | 220/516 [18:08<26:22,  5.35s/it]
2025-02-04 05:41:38,476 - INFO - tqdm - perplexity: 5.8503, batch_loss: 1.7054, loss: 1.7665 ||:  43%|####3     | 222/516 [18:20<27:17,  5.57s/it]
2025-02-04 05:41:48,480 - INFO - tqdm - perplexity: 5.8455, batch_loss: 1.6667, loss: 1.7657 ||:  43%|####3     | 224/516 [18:30<25:25,  5.22s/it]
2025-02-04 05:42:02,156 - INFO - tqdm - perplexity: 5.8428, batch_loss: 1.5942, loss: 1.7652 ||:  44%|####3     | 227/516 [18:44<22:43,  4.72s/it]
2025-02-04 05:42:12,238 - INFO - tqdm - perplexity: 5.8358, batch_loss: 1.7079, loss: 1.7640 ||:  44%|####4     | 229/516 [18:54<23:03,  4.82s/it]
2025-02-04 05:42:24,705 - INFO - tqdm - perplexity: 5.8466, batch_loss: 1.8826, loss: 1.7659 ||:  45%|####4     | 232/516 [19:06<20:27,  4.32s/it]
2025-02-04 05:42:37,710 - INFO - tqdm - perplexity: 5.8517, batch_loss: 1.9115, loss: 1.7667 ||:  46%|####5     | 235/516 [19:19<20:09,  4.30s/it]
2025-02-04 05:42:51,011 - INFO - tqdm - perplexity: 5.8440, batch_loss: 1.6303, loss: 1.7654 ||:  46%|####6     | 238/516 [19:32<20:13,  4.37s/it]
2025-02-04 05:43:04,716 - INFO - tqdm - perplexity: 5.8486, batch_loss: 1.7469, loss: 1.7662 ||:  47%|####6     | 241/516 [19:46<20:41,  4.52s/it]
2025-02-04 05:43:19,909 - INFO - tqdm - perplexity: 5.8368, batch_loss: 1.5752, loss: 1.7642 ||:  47%|####7     | 244/516 [20:01<22:22,  4.94s/it]
2025-02-04 05:43:30,428 - INFO - tqdm - perplexity: 5.8392, batch_loss: 1.8630, loss: 1.7646 ||:  48%|####7     | 246/516 [20:12<23:01,  5.12s/it]
2025-02-04 05:43:43,049 - INFO - tqdm - perplexity: 5.8292, batch_loss: 1.3619, loss: 1.7629 ||:  48%|####8     | 248/516 [20:24<26:14,  5.88s/it]
2025-02-04 05:43:55,054 - INFO - tqdm - perplexity: 5.8289, batch_loss: 1.6085, loss: 1.7628 ||:  48%|####8     | 250/516 [20:36<25:39,  5.79s/it]
2025-02-04 05:44:06,491 - INFO - tqdm - perplexity: 5.8212, batch_loss: 1.6546, loss: 1.7615 ||:  49%|####8     | 252/516 [20:48<25:03,  5.69s/it]
2025-02-04 05:44:16,504 - INFO - tqdm - perplexity: 5.8170, batch_loss: 1.8281, loss: 1.7608 ||:  49%|####9     | 254/516 [20:58<23:08,  5.30s/it]
2025-02-04 05:44:26,629 - INFO - tqdm - perplexity: 5.8150, batch_loss: 1.7853, loss: 1.7604 ||:  50%|####9     | 256/516 [21:08<22:14,  5.13s/it]
2025-02-04 05:44:42,234 - INFO - tqdm - perplexity: 5.8147, batch_loss: 1.7595, loss: 1.7604 ||:  50%|#####     | 259/516 [21:24<23:02,  5.38s/it]
2025-02-04 05:44:57,173 - INFO - tqdm - perplexity: 5.8110, batch_loss: 1.6249, loss: 1.7598 ||:  51%|#####     | 262/516 [21:39<22:21,  5.28s/it]
2025-02-04 05:45:08,124 - INFO - tqdm - perplexity: 5.8025, batch_loss: 1.4508, loss: 1.7583 ||:  51%|#####1    | 264/516 [21:50<22:50,  5.44s/it]
2025-02-04 05:45:20,511 - INFO - tqdm - perplexity: 5.8025, batch_loss: 1.6752, loss: 1.7583 ||:  52%|#####1    | 267/516 [22:02<19:14,  4.64s/it]
2025-02-04 05:45:31,558 - INFO - tqdm - perplexity: 5.8038, batch_loss: 1.9276, loss: 1.7585 ||:  52%|#####2    | 269/516 [22:13<21:07,  5.13s/it]
2025-02-04 05:45:44,647 - INFO - tqdm - perplexity: 5.7933, batch_loss: 1.7908, loss: 1.7567 ||:  53%|#####2    | 272/516 [22:26<19:00,  4.67s/it]
2025-02-04 05:45:57,296 - INFO - tqdm - perplexity: 5.7897, batch_loss: 1.8161, loss: 1.7561 ||:  53%|#####3    | 275/516 [22:39<17:35,  4.38s/it]
2025-02-04 05:46:11,823 - INFO - tqdm - perplexity: 5.7832, batch_loss: 1.7077, loss: 1.7550 ||:  54%|#####3    | 278/516 [22:53<18:30,  4.66s/it]
2025-02-04 05:46:24,199 - INFO - tqdm - perplexity: 5.7775, batch_loss: 1.6621, loss: 1.7540 ||:  54%|#####4    | 280/516 [23:06<21:00,  5.34s/it]
2025-02-04 05:46:35,156 - INFO - tqdm - perplexity: 5.7782, batch_loss: 1.8391, loss: 1.7541 ||:  55%|#####4    | 282/516 [23:17<21:17,  5.46s/it]
2025-02-04 05:46:49,129 - INFO - tqdm - perplexity: 5.7763, batch_loss: 1.7452, loss: 1.7538 ||:  55%|#####5    | 285/516 [23:31<19:17,  5.01s/it]
2025-02-04 05:46:59,497 - INFO - tqdm - perplexity: 5.7714, batch_loss: 1.6386, loss: 1.7529 ||:  56%|#####5    | 287/516 [23:41<19:04,  5.00s/it]
2025-02-04 05:47:13,432 - INFO - tqdm - perplexity: 5.7758, batch_loss: 2.0214, loss: 1.7537 ||:  56%|#####6    | 290/516 [23:55<18:12,  4.83s/it]
2025-02-04 05:47:27,441 - INFO - tqdm - perplexity: 5.7733, batch_loss: 1.7161, loss: 1.7533 ||:  57%|#####6    | 293/516 [24:09<17:22,  4.67s/it]
2025-02-04 05:47:40,426 - INFO - tqdm - perplexity: 5.7758, batch_loss: 1.8874, loss: 1.7537 ||:  57%|#####7    | 296/516 [24:22<16:08,  4.40s/it]
2025-02-04 05:47:54,287 - INFO - tqdm - perplexity: 5.7707, batch_loss: 1.5292, loss: 1.7528 ||:  58%|#####7    | 299/516 [24:36<16:25,  4.54s/it]
2025-02-04 05:48:06,847 - INFO - tqdm - perplexity: 5.7712, batch_loss: 1.7036, loss: 1.7529 ||:  59%|#####8    | 302/516 [24:48<15:38,  4.39s/it]
2025-02-04 05:48:20,168 - INFO - tqdm - perplexity: 5.7655, batch_loss: 1.7203, loss: 1.7519 ||:  59%|#####9    | 305/516 [25:02<15:06,  4.30s/it]
2025-02-04 05:48:35,036 - INFO - tqdm - perplexity: 5.7629, batch_loss: 1.7557, loss: 1.7514 ||:  60%|#####9    | 308/516 [25:16<16:57,  4.89s/it]
2025-02-04 05:48:48,320 - INFO - tqdm - perplexity: 5.7588, batch_loss: 1.9000, loss: 1.7507 ||:  60%|######    | 311/516 [25:30<15:31,  4.54s/it]
2025-02-04 05:49:01,361 - INFO - tqdm - perplexity: 5.7531, batch_loss: 1.7781, loss: 1.7497 ||:  61%|######    | 314/516 [25:43<15:00,  4.46s/it]
2025-02-04 05:49:14,480 - INFO - tqdm - perplexity: 5.7539, batch_loss: 1.7510, loss: 1.7499 ||:  61%|######1   | 317/516 [25:56<14:39,  4.42s/it]
2025-02-04 05:49:24,653 - INFO - tqdm - perplexity: 5.7512, batch_loss: 1.6506, loss: 1.7494 ||:  62%|######1   | 319/516 [26:06<15:32,  4.73s/it]
2025-02-04 05:49:38,512 - INFO - tqdm - perplexity: 5.7447, batch_loss: 1.5473, loss: 1.7483 ||:  62%|######2   | 322/516 [26:20<14:46,  4.57s/it]
2025-02-04 05:49:51,295 - INFO - tqdm - perplexity: 5.7446, batch_loss: 1.6408, loss: 1.7483 ||:  63%|######2   | 325/516 [26:33<14:15,  4.48s/it]
2025-02-04 05:50:06,823 - INFO - tqdm - perplexity: 5.7436, batch_loss: 1.6094, loss: 1.7481 ||:  64%|######3   | 328/516 [26:48<15:47,  5.04s/it]
2025-02-04 05:50:17,640 - INFO - tqdm - perplexity: 5.7402, batch_loss: 1.7327, loss: 1.7475 ||:  64%|######3   | 330/516 [26:59<16:12,  5.23s/it]
2025-02-04 05:50:31,132 - INFO - tqdm - perplexity: 5.7357, batch_loss: 1.7129, loss: 1.7467 ||:  65%|######4   | 333/516 [27:13<14:17,  4.69s/it]
2025-02-04 05:50:45,855 - INFO - tqdm - perplexity: 5.7357, batch_loss: 1.7374, loss: 1.7467 ||:  65%|######5   | 336/516 [27:27<14:37,  4.88s/it]
2025-02-04 05:50:56,113 - INFO - tqdm - perplexity: 5.7281, batch_loss: 1.4840, loss: 1.7454 ||:  66%|######5   | 338/516 [27:38<14:44,  4.97s/it]
2025-02-04 05:51:09,627 - INFO - tqdm - perplexity: 5.7297, batch_loss: 1.8713, loss: 1.7457 ||:  66%|######6   | 341/516 [27:51<13:42,  4.70s/it]
2025-02-04 05:51:19,734 - INFO - tqdm - perplexity: 5.7252, batch_loss: 1.5474, loss: 1.7449 ||:  66%|######6   | 343/516 [28:01<14:01,  4.86s/it]
2025-02-04 05:51:33,446 - INFO - tqdm - perplexity: 5.7223, batch_loss: 1.6682, loss: 1.7444 ||:  67%|######7   | 346/516 [28:15<13:23,  4.73s/it]
2025-02-04 05:51:53,470 - INFO - tqdm - perplexity: 5.7130, batch_loss: 1.3517, loss: 1.7427 ||:  68%|######7   | 349/516 [28:35<18:17,  6.57s/it]
2025-02-04 05:52:06,331 - INFO - tqdm - perplexity: 5.7126, batch_loss: 1.6518, loss: 1.7427 ||:  68%|######8   | 352/516 [28:48<14:02,  5.14s/it]
2025-02-04 05:52:17,965 - INFO - tqdm - perplexity: 5.7077, batch_loss: 1.6162, loss: 1.7418 ||:  69%|######8   | 354/516 [28:59<14:37,  5.42s/it]
2025-02-04 05:52:32,216 - INFO - tqdm - perplexity: 5.7115, batch_loss: 1.9735, loss: 1.7425 ||:  69%|######9   | 357/516 [29:14<13:12,  4.98s/it]
2025-02-04 05:52:46,890 - INFO - tqdm - perplexity: 5.7115, batch_loss: 1.7601, loss: 1.7425 ||:  70%|######9   | 360/516 [29:28<12:56,  4.98s/it]
2025-02-04 05:53:00,723 - INFO - tqdm - perplexity: 5.7085, batch_loss: 1.6600, loss: 1.7420 ||:  70%|#######   | 363/516 [29:42<11:49,  4.64s/it]
2025-02-04 05:53:14,359 - INFO - tqdm - perplexity: 5.7074, batch_loss: 1.5820, loss: 1.7418 ||:  71%|#######   | 366/516 [29:56<11:50,  4.74s/it]
2025-02-04 05:53:24,774 - INFO - tqdm - perplexity: 5.7070, batch_loss: 1.5656, loss: 1.7417 ||:  71%|#######1  | 368/516 [30:06<12:26,  5.04s/it]
2025-02-04 05:53:37,863 - INFO - tqdm - perplexity: 5.7053, batch_loss: 1.5401, loss: 1.7414 ||:  72%|#######1  | 370/516 [30:19<14:33,  5.98s/it]
2025-02-04 05:53:48,285 - INFO - tqdm - perplexity: 5.6983, batch_loss: 1.6177, loss: 1.7402 ||:  72%|#######2  | 372/516 [30:30<13:13,  5.51s/it]
2025-02-04 05:54:01,530 - INFO - tqdm - perplexity: 5.6972, batch_loss: 1.9934, loss: 1.7400 ||:  73%|#######2  | 375/516 [30:43<11:07,  4.73s/it]
2025-02-04 05:54:12,067 - INFO - tqdm - perplexity: 5.6930, batch_loss: 1.5006, loss: 1.7392 ||:  73%|#######3  | 377/516 [30:53<11:53,  5.14s/it]
2025-02-04 05:54:27,079 - INFO - tqdm - perplexity: 5.6930, batch_loss: 1.6652, loss: 1.7392 ||:  74%|#######3  | 380/516 [31:08<11:39,  5.14s/it]
2025-02-04 05:54:37,107 - INFO - tqdm - perplexity: 5.6925, batch_loss: 1.7900, loss: 1.7391 ||:  74%|#######4  | 382/516 [31:19<11:10,  5.00s/it]
2025-02-04 05:54:47,732 - INFO - tqdm - perplexity: 5.6911, batch_loss: 1.8372, loss: 1.7389 ||:  74%|#######4  | 384/516 [31:29<11:14,  5.11s/it]
2025-02-04 05:54:58,056 - INFO - tqdm - perplexity: 5.6879, batch_loss: 1.7094, loss: 1.7383 ||:  75%|#######4  | 386/516 [31:39<11:02,  5.10s/it]
2025-02-04 05:55:10,982 - INFO - tqdm - perplexity: 5.6840, batch_loss: 1.5717, loss: 1.7377 ||:  75%|#######5  | 389/516 [31:52<09:41,  4.58s/it]
2025-02-04 05:55:21,810 - INFO - tqdm - perplexity: 5.6800, batch_loss: 1.6289, loss: 1.7369 ||:  76%|#######5  | 391/516 [32:03<10:25,  5.01s/it]
2025-02-04 05:55:33,076 - INFO - tqdm - perplexity: 5.6703, batch_loss: 1.4241, loss: 1.7352 ||:  76%|#######6  | 393/516 [32:14<10:59,  5.36s/it]
2025-02-04 05:55:47,902 - INFO - tqdm - perplexity: 5.6690, batch_loss: 1.6917, loss: 1.7350 ||:  77%|#######6  | 396/516 [32:29<10:13,  5.11s/it]
2025-02-04 05:55:59,373 - INFO - tqdm - perplexity: 5.6682, batch_loss: 1.9098, loss: 1.7349 ||:  77%|#######7  | 399/516 [32:41<08:26,  4.33s/it]
2025-02-04 05:56:14,029 - INFO - tqdm - perplexity: 5.6671, batch_loss: 1.5568, loss: 1.7347 ||:  78%|#######7  | 402/516 [32:55<08:57,  4.72s/it]
2025-02-04 05:56:25,081 - INFO - tqdm - perplexity: 5.6614, batch_loss: 1.5169, loss: 1.7337 ||:  78%|#######8  | 404/516 [33:07<09:31,  5.11s/it]
2025-02-04 05:56:35,820 - INFO - tqdm - perplexity: 5.6537, batch_loss: 1.5859, loss: 1.7323 ||:  79%|#######8  | 406/516 [33:17<09:20,  5.09s/it]
2025-02-04 05:56:51,178 - INFO - tqdm - perplexity: 5.6576, batch_loss: 1.8465, loss: 1.7330 ||:  79%|#######9  | 409/516 [33:33<09:04,  5.09s/it]
2025-02-04 05:57:07,294 - INFO - tqdm - perplexity: 5.6503, batch_loss: 1.6033, loss: 1.7317 ||:  80%|#######9  | 412/516 [33:49<09:29,  5.47s/it]
2025-02-04 05:57:17,423 - INFO - tqdm - perplexity: 5.6434, batch_loss: 1.4534, loss: 1.7305 ||:  80%|########  | 414/516 [33:59<08:49,  5.20s/it]
2025-02-04 05:57:32,186 - INFO - tqdm - perplexity: 5.6416, batch_loss: 1.5967, loss: 1.7302 ||:  81%|########  | 417/516 [34:14<08:15,  5.00s/it]
2025-02-04 05:57:44,871 - INFO - tqdm - perplexity: 5.6364, batch_loss: 1.4922, loss: 1.7292 ||:  81%|########1 | 419/516 [34:26<09:08,  5.66s/it]
2025-02-04 05:57:58,998 - INFO - tqdm - perplexity: 5.6299, batch_loss: 1.4696, loss: 1.7281 ||:  82%|########1 | 421/516 [34:40<09:56,  6.27s/it]
2025-02-04 05:58:12,969 - INFO - tqdm - perplexity: 5.6262, batch_loss: 1.6728, loss: 1.7274 ||:  82%|########2 | 424/516 [34:54<07:59,  5.21s/it]
2025-02-04 05:58:26,304 - INFO - tqdm - perplexity: 5.6241, batch_loss: 1.7130, loss: 1.7271 ||:  83%|########2 | 427/516 [35:08<07:06,  4.79s/it]
2025-02-04 05:58:38,006 - INFO - tqdm - perplexity: 5.6220, batch_loss: 1.6796, loss: 1.7267 ||:  83%|########3 | 430/516 [35:19<05:58,  4.17s/it]
2025-02-04 05:58:48,451 - INFO - tqdm - perplexity: 5.6221, batch_loss: 1.5881, loss: 1.7267 ||:  84%|########3 | 432/516 [35:30<06:26,  4.60s/it]
2025-02-04 05:59:00,140 - INFO - tqdm - perplexity: 5.6209, batch_loss: 1.6804, loss: 1.7265 ||:  84%|########4 | 435/516 [35:42<05:34,  4.13s/it]
2025-02-04 05:59:12,065 - INFO - tqdm - perplexity: 5.6209, batch_loss: 1.7683, loss: 1.7265 ||:  85%|########4 | 438/516 [35:53<05:10,  3.98s/it]
2025-02-04 05:59:25,189 - INFO - tqdm - perplexity: 5.6159, batch_loss: 1.4622, loss: 1.7256 ||:  85%|########5 | 441/516 [36:07<05:17,  4.23s/it]
2025-02-04 05:59:37,166 - INFO - tqdm - perplexity: 5.6183, batch_loss: 1.7363, loss: 1.7260 ||:  86%|########6 | 444/516 [36:19<04:49,  4.02s/it]
2025-02-04 05:59:49,572 - INFO - tqdm - perplexity: 5.6166, batch_loss: 1.7082, loss: 1.7257 ||:  87%|########6 | 447/516 [36:31<04:35,  4.00s/it]
2025-02-04 06:00:03,746 - INFO - tqdm - perplexity: 5.6128, batch_loss: 1.5403, loss: 1.7250 ||:  87%|########7 | 450/516 [36:45<04:53,  4.45s/it]
2025-02-04 06:00:15,034 - INFO - tqdm - perplexity: 5.6063, batch_loss: 1.6148, loss: 1.7239 ||:  88%|########7 | 452/516 [36:56<05:18,  4.98s/it]
2025-02-04 06:00:25,112 - INFO - tqdm - perplexity: 5.6020, batch_loss: 1.5373, loss: 1.7231 ||:  88%|########7 | 454/516 [37:07<05:10,  5.00s/it]
2025-02-04 06:00:39,095 - INFO - tqdm - perplexity: 5.6037, batch_loss: 1.6052, loss: 1.7234 ||:  89%|########8 | 457/516 [37:21<04:41,  4.76s/it]
2025-02-04 06:00:49,220 - INFO - tqdm - perplexity: 5.5999, batch_loss: 1.5588, loss: 1.7228 ||:  89%|########8 | 459/516 [37:31<04:35,  4.83s/it]
2025-02-04 06:01:02,328 - INFO - tqdm - perplexity: 5.6005, batch_loss: 1.5767, loss: 1.7229 ||:  90%|########9 | 462/516 [37:44<04:06,  4.56s/it]
2025-02-04 06:01:15,851 - INFO - tqdm - perplexity: 5.5959, batch_loss: 1.7008, loss: 1.7220 ||:  90%|######### | 465/516 [37:57<03:54,  4.60s/it]
2025-02-04 06:01:29,554 - INFO - tqdm - perplexity: 5.5955, batch_loss: 1.7743, loss: 1.7220 ||:  91%|######### | 468/516 [38:11<03:42,  4.63s/it]
2025-02-04 06:01:41,562 - INFO - tqdm - perplexity: 5.5921, batch_loss: 1.4052, loss: 1.7214 ||:  91%|#########1| 470/516 [38:23<04:09,  5.42s/it]
2025-02-04 06:01:54,139 - INFO - tqdm - perplexity: 5.5903, batch_loss: 1.6676, loss: 1.7210 ||:  91%|#########1| 472/516 [38:36<04:20,  5.91s/it]
2025-02-04 06:02:08,812 - INFO - tqdm - perplexity: 5.5859, batch_loss: 1.7282, loss: 1.7202 ||:  92%|#########2| 475/516 [38:50<03:35,  5.26s/it]
2025-02-04 06:02:22,403 - INFO - tqdm - perplexity: 5.5860, batch_loss: 1.7505, loss: 1.7203 ||:  93%|#########2| 478/516 [39:04<03:02,  4.80s/it]
2025-02-04 06:02:32,516 - INFO - tqdm - perplexity: 5.5815, batch_loss: 1.4945, loss: 1.7195 ||:  93%|#########3| 480/516 [39:14<02:57,  4.93s/it]
2025-02-04 06:02:46,734 - INFO - tqdm - perplexity: 5.5772, batch_loss: 1.6047, loss: 1.7187 ||:  94%|#########3| 483/516 [39:28<02:38,  4.82s/it]
2025-02-04 06:03:02,441 - INFO - tqdm - perplexity: 5.5703, batch_loss: 1.2902, loss: 1.7174 ||:  94%|#########4| 486/516 [39:44<02:33,  5.11s/it]
2025-02-04 06:03:15,305 - INFO - tqdm - perplexity: 5.5676, batch_loss: 1.5495, loss: 1.7170 ||:  95%|#########4| 489/516 [39:57<02:01,  4.50s/it]
2025-02-04 06:03:27,189 - INFO - tqdm - perplexity: 5.5658, batch_loss: 1.8168, loss: 1.7166 ||:  95%|#########5| 492/516 [40:09<01:39,  4.14s/it]
2025-02-04 06:03:40,943 - INFO - tqdm - perplexity: 5.5610, batch_loss: 1.6343, loss: 1.7158 ||:  96%|#########5| 495/516 [40:22<01:34,  4.50s/it]
2025-02-04 06:03:53,618 - INFO - tqdm - perplexity: 5.5584, batch_loss: 1.7590, loss: 1.7153 ||:  97%|#########6| 498/516 [40:35<01:19,  4.43s/it]
2025-02-04 06:04:07,605 - INFO - tqdm - perplexity: 5.5526, batch_loss: 1.4384, loss: 1.7143 ||:  97%|#########7| 501/516 [40:49<01:10,  4.69s/it]
2025-02-04 06:04:23,117 - INFO - tqdm - perplexity: 5.5502, batch_loss: 1.4910, loss: 1.7138 ||:  98%|#########7| 504/516 [41:05<01:00,  5.03s/it]
2025-02-04 06:04:36,387 - INFO - tqdm - perplexity: 5.5505, batch_loss: 1.8440, loss: 1.7139 ||:  98%|#########8| 507/516 [41:18<00:42,  4.69s/it]
2025-02-04 06:04:46,846 - INFO - tqdm - perplexity: 5.5462, batch_loss: 1.5245, loss: 1.7131 ||:  99%|#########8| 509/516 [41:28<00:35,  5.00s/it]
2025-02-04 06:05:00,926 - INFO - tqdm - perplexity: 5.5403, batch_loss: 1.2662, loss: 1.7120 ||:  99%|#########9| 512/516 [41:42<00:20,  5.04s/it]
2025-02-04 06:05:12,357 - INFO - tqdm - perplexity: 5.5378, batch_loss: 1.6862, loss: 1.7116 ||: 100%|#########9| 514/516 [41:54<00:10,  5.25s/it]
2025-02-04 06:05:17,731 - INFO - tqdm - perplexity: 5.5358, batch_loss: 1.5273, loss: 1.7112 ||: 100%|#########9| 515/516 [41:59<00:05,  5.29s/it]
2025-02-04 06:05:19,451 - INFO - tqdm - perplexity: 5.5309, batch_loss: 1.2486, loss: 1.7103 ||: 100%|##########| 516/516 [42:01<00:00,  4.22s/it]
2025-02-04 06:05:19,452 - INFO - tqdm - perplexity: 5.5309, batch_loss: 1.2486, loss: 1.7103 ||: 100%|##########| 516/516 [42:01<00:00,  4.89s/it]
2025-02-04 06:05:19,465 - INFO - allennlp.training.gradient_descent_trainer - Validating
2025-02-04 06:05:19,466 - INFO - tqdm - 0%|          | 0/129 [00:00<?, ?it/s]
2025-02-04 06:05:30,290 - INFO - tqdm - perplexity: 11.2764, batch_loss: 2.4651, loss: 2.4227 ||:   6%|6         | 8/129 [00:10<02:39,  1.31s/it]
2025-02-04 06:05:40,780 - INFO - tqdm - perplexity: 11.1057, batch_loss: 2.3112, loss: 2.4075 ||:  12%|#2        | 16/129 [00:21<02:30,  1.33s/it]
2025-02-04 06:05:51,191 - INFO - tqdm - perplexity: 11.3187, batch_loss: 2.6279, loss: 2.4265 ||:  19%|#8        | 24/129 [00:31<02:14,  1.28s/it]
2025-02-04 06:06:01,604 - INFO - tqdm - perplexity: 11.2299, batch_loss: 2.2505, loss: 2.4186 ||:  25%|##4       | 32/129 [00:42<02:03,  1.27s/it]
2025-02-04 06:06:11,818 - INFO - tqdm - perplexity: 11.3428, batch_loss: 2.5178, loss: 2.4286 ||:  31%|###1      | 40/129 [00:52<01:53,  1.28s/it]
2025-02-04 06:06:22,465 - INFO - tqdm - perplexity: 11.3790, batch_loss: 2.4488, loss: 2.4318 ||:  37%|###7      | 48/129 [01:02<01:45,  1.30s/it]
2025-02-04 06:06:32,605 - INFO - tqdm - perplexity: 11.5713, batch_loss: 2.4759, loss: 2.4485 ||:  43%|####3     | 56/129 [01:13<01:29,  1.22s/it]
2025-02-04 06:06:43,612 - INFO - tqdm - perplexity: 11.7281, batch_loss: 2.8433, loss: 2.4620 ||:  50%|#####     | 65/129 [01:24<01:19,  1.24s/it]
2025-02-04 06:06:54,071 - INFO - tqdm - perplexity: 11.7093, batch_loss: 2.0044, loss: 2.4604 ||:  57%|#####7    | 74/129 [01:34<01:05,  1.19s/it]
2025-02-04 06:07:05,147 - INFO - tqdm - perplexity: 11.7208, batch_loss: 2.5069, loss: 2.4614 ||:  64%|######4   | 83/129 [01:45<00:55,  1.21s/it]
2025-02-04 06:07:15,631 - INFO - tqdm - perplexity: 11.6364, batch_loss: 2.2448, loss: 2.4541 ||:  71%|#######   | 91/129 [01:56<00:49,  1.31s/it]
2025-02-04 06:07:26,679 - INFO - tqdm - perplexity: 11.6064, batch_loss: 2.5738, loss: 2.4516 ||:  78%|#######7  | 100/129 [02:07<00:36,  1.25s/it]
2025-02-04 06:07:37,590 - INFO - tqdm - perplexity: 11.5096, batch_loss: 2.3211, loss: 2.4432 ||:  84%|########3 | 108/129 [02:18<00:28,  1.37s/it]
2025-02-04 06:07:47,793 - INFO - tqdm - perplexity: 11.5816, batch_loss: 2.6529, loss: 2.4494 ||:  90%|########9 | 116/129 [02:28<00:16,  1.29s/it]
2025-02-04 06:07:58,788 - INFO - tqdm - perplexity: 11.5601, batch_loss: 2.5026, loss: 2.4476 ||:  97%|#########6| 125/129 [02:39<00:04,  1.24s/it]
2025-02-04 06:08:03,442 - INFO - tqdm - perplexity: 11.5530, batch_loss: 2.6374, loss: 2.4469 ||: 100%|##########| 129/129 [02:43<00:00,  1.18s/it]
2025-02-04 06:08:03,443 - INFO - tqdm - perplexity: 11.5530, batch_loss: 2.6374, loss: 2.4469 ||: 100%|##########| 129/129 [02:43<00:00,  1.27s/it]
2025-02-04 06:08:03,444 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2025-02-04 06:08:03,444 - INFO - allennlp.training.callbacks.console_logger - loss               |     1.710  |     2.447
2025-02-04 06:08:03,445 - INFO - allennlp.training.callbacks.console_logger - perplexity         |     5.531  |    11.553
2025-02-04 06:08:03,445 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |     0.000  |       N/A
2025-02-04 06:08:04,076 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:44:45.998229
2025-02-04 06:08:04,076 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 18:51:29
2025-02-04 06:08:04,076 - INFO - allennlp.training.gradient_descent_trainer - Epoch 7/31
2025-02-04 06:08:04,077 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 0B
2025-02-04 06:08:04,077 - INFO - allennlp.training.gradient_descent_trainer - Training
2025-02-04 06:08:04,077 - INFO - tqdm - 0%|          | 0/516 [00:00<?, ?it/s]
2025-02-04 06:08:15,342 - INFO - tqdm - perplexity: 4.4950, batch_loss: 1.3942, loss: 1.5030 ||:   0%|          | 2/516 [00:11<49:39,  5.80s/it]
2025-02-04 06:08:30,301 - INFO - tqdm - perplexity: 4.2085, batch_loss: 1.2626, loss: 1.4371 ||:   1%|          | 5/516 [00:26<45:35,  5.35s/it]
2025-02-04 06:08:44,324 - INFO - tqdm - perplexity: 4.3113, batch_loss: 1.4668, loss: 1.4612 ||:   2%|1         | 8/516 [00:40<41:43,  4.93s/it]
2025-02-04 06:08:59,631 - INFO - tqdm - perplexity: 4.4589, batch_loss: 1.6695, loss: 1.4949 ||:   2%|2         | 11/516 [00:55<43:16,  5.14s/it]
2025-02-04 06:09:12,391 - INFO - tqdm - perplexity: 4.3829, batch_loss: 1.4193, loss: 1.4777 ||:   3%|2         | 14/516 [01:08<38:34,  4.61s/it]
2025-02-04 06:09:24,204 - INFO - tqdm - perplexity: 4.3899, batch_loss: 1.6003, loss: 1.4793 ||:   3%|3         | 17/516 [01:20<35:15,  4.24s/it]
2025-02-04 06:09:35,011 - INFO - tqdm - perplexity: 4.3953, batch_loss: 1.4347, loss: 1.4805 ||:   4%|3         | 19/516 [01:30<39:51,  4.81s/it]
2025-02-04 06:09:45,363 - INFO - tqdm - perplexity: 4.3617, batch_loss: 1.3613, loss: 1.4729 ||:   4%|4         | 21/516 [01:41<42:21,  5.13s/it]
2025-02-04 06:10:00,288 - INFO - tqdm - perplexity: 4.4287, batch_loss: 1.5197, loss: 1.4881 ||:   5%|4         | 24/516 [01:56<42:21,  5.17s/it]
2025-02-04 06:10:13,211 - INFO - tqdm - perplexity: 4.4558, batch_loss: 1.4662, loss: 1.4942 ||:   5%|5         | 27/516 [02:09<38:12,  4.69s/it]
2025-02-04 06:10:26,618 - INFO - tqdm - perplexity: 4.4772, batch_loss: 1.4016, loss: 1.4990 ||:   6%|5         | 30/516 [02:22<36:28,  4.50s/it]
2025-02-04 06:10:39,683 - INFO - tqdm - perplexity: 4.4570, batch_loss: 1.3899, loss: 1.4945 ||:   6%|6         | 33/516 [02:35<35:19,  4.39s/it]
2025-02-04 06:10:53,634 - INFO - tqdm - perplexity: 4.4348, batch_loss: 1.4478, loss: 1.4895 ||:   7%|6         | 36/516 [02:49<37:40,  4.71s/it]
2025-02-04 06:11:06,797 - INFO - tqdm - perplexity: 4.4376, batch_loss: 1.6088, loss: 1.4901 ||:   8%|7         | 39/516 [03:02<35:43,  4.49s/it]
2025-02-04 06:11:22,770 - INFO - tqdm - perplexity: 4.3932, batch_loss: 1.2663, loss: 1.4801 ||:   8%|8         | 42/516 [03:18<41:12,  5.22s/it]
2025-02-04 06:11:37,695 - INFO - tqdm - perplexity: 4.3833, batch_loss: 1.2696, loss: 1.4778 ||:   9%|8         | 45/516 [03:33<39:24,  5.02s/it]
2025-02-04 06:11:48,348 - INFO - tqdm - perplexity: 4.3763, batch_loss: 1.4817, loss: 1.4762 ||:   9%|9         | 47/516 [03:44<40:28,  5.18s/it]
2025-02-04 06:12:02,343 - INFO - tqdm - perplexity: 4.3878, batch_loss: 1.6133, loss: 1.4788 ||:  10%|9         | 50/516 [03:58<37:16,  4.80s/it]
2025-02-04 06:12:14,990 - INFO - tqdm - perplexity: 4.3849, batch_loss: 1.5867, loss: 1.4782 ||:  10%|#         | 53/516 [04:10<33:34,  4.35s/it]
2025-02-04 06:12:29,583 - INFO - tqdm - perplexity: 4.3560, batch_loss: 1.3067, loss: 1.4715 ||:  11%|#         | 56/516 [04:25<37:08,  4.84s/it]
2025-02-04 06:12:40,696 - INFO - tqdm - perplexity: 4.3427, batch_loss: 1.3942, loss: 1.4685 ||:  11%|#1        | 58/516 [04:36<38:58,  5.11s/it]
2025-02-04 06:12:54,337 - INFO - tqdm - perplexity: 4.3288, batch_loss: 1.4102, loss: 1.4653 ||:  12%|#1        | 61/516 [04:50<36:02,  4.75s/it]
2025-02-04 06:13:08,063 - INFO - tqdm - perplexity: 4.3252, batch_loss: 1.4085, loss: 1.4645 ||:  12%|#2        | 64/516 [05:03<34:46,  4.62s/it]
2025-02-04 06:13:22,212 - INFO - tqdm - perplexity: 4.3260, batch_loss: 1.4971, loss: 1.4646 ||:  13%|#2        | 67/516 [05:18<35:41,  4.77s/it]
2025-02-04 06:13:36,061 - INFO - tqdm - perplexity: 4.3173, batch_loss: 1.3747, loss: 1.4626 ||:  14%|#3        | 70/516 [05:31<34:51,  4.69s/it]
2025-02-04 06:13:47,956 - INFO - tqdm - perplexity: 4.3211, batch_loss: 1.4423, loss: 1.4635 ||:  14%|#4        | 73/516 [05:43<31:19,  4.24s/it]
2025-02-04 06:14:01,314 - INFO - tqdm - perplexity: 4.3210, batch_loss: 1.5301, loss: 1.4635 ||:  15%|#4        | 76/516 [05:57<31:52,  4.35s/it]
2025-02-04 06:14:14,952 - INFO - tqdm - perplexity: 4.3117, batch_loss: 1.4711, loss: 1.4613 ||:  15%|#5        | 79/516 [06:10<33:19,  4.58s/it]
2025-02-04 06:14:25,488 - INFO - tqdm - perplexity: 4.2926, batch_loss: 1.3544, loss: 1.4569 ||:  16%|#5        | 81/516 [06:21<35:25,  4.89s/it]
2025-02-04 06:14:36,431 - INFO - tqdm - perplexity: 4.2916, batch_loss: 1.3354, loss: 1.4567 ||:  16%|#6        | 83/516 [06:32<37:09,  5.15s/it]
2025-02-04 06:14:48,377 - INFO - tqdm - perplexity: 4.2865, batch_loss: 1.3034, loss: 1.4555 ||:  16%|#6        | 85/516 [06:44<40:51,  5.69s/it]
2025-02-04 06:14:58,918 - INFO - tqdm - perplexity: 4.2832, batch_loss: 1.4819, loss: 1.4547 ||:  17%|#6        | 87/516 [06:54<38:39,  5.41s/it]
2025-02-04 06:15:09,398 - INFO - tqdm - perplexity: 4.2855, batch_loss: 1.4693, loss: 1.4552 ||:  17%|#7        | 89/516 [07:05<37:07,  5.22s/it]
2025-02-04 06:15:19,436 - INFO - tqdm - perplexity: 4.2770, batch_loss: 1.3220, loss: 1.4533 ||:  18%|#7        | 91/516 [07:15<36:39,  5.17s/it]
2025-02-04 06:15:33,256 - INFO - tqdm - perplexity: 4.2902, batch_loss: 1.5829, loss: 1.4563 ||:  18%|#8        | 94/516 [07:29<33:49,  4.81s/it]
2025-02-04 06:15:47,149 - INFO - tqdm - perplexity: 4.2980, batch_loss: 1.3451, loss: 1.4581 ||:  19%|#8        | 97/516 [07:43<32:53,  4.71s/it]
2025-02-04 06:15:58,460 - INFO - tqdm - perplexity: 4.2904, batch_loss: 1.3541, loss: 1.4564 ||:  19%|#9        | 99/516 [07:54<35:56,  5.17s/it]
2025-02-04 06:16:13,596 - INFO - tqdm - perplexity: 4.2893, batch_loss: 1.2242, loss: 1.4561 ||:  20%|#9        | 102/516 [08:09<36:03,  5.23s/it]
2025-02-04 06:16:26,645 - INFO - tqdm - perplexity: 4.2832, batch_loss: 1.4733, loss: 1.4547 ||:  20%|##        | 105/516 [08:22<31:38,  4.62s/it]
2025-02-04 06:16:37,521 - INFO - tqdm - perplexity: 4.2755, batch_loss: 1.2184, loss: 1.4529 ||:  21%|##        | 107/516 [08:33<34:32,  5.07s/it]
2025-02-04 06:16:49,917 - INFO - tqdm - perplexity: 4.2649, batch_loss: 1.2864, loss: 1.4504 ||:  21%|##1       | 110/516 [08:45<29:55,  4.42s/it]
2025-02-04 06:17:01,772 - INFO - tqdm - perplexity: 4.2773, batch_loss: 1.5832, loss: 1.4533 ||:  22%|##1       | 113/516 [08:57<27:38,  4.12s/it]
2025-02-04 06:17:14,505 - INFO - tqdm - perplexity: 4.2865, batch_loss: 1.5457, loss: 1.4555 ||:  22%|##2       | 116/516 [09:10<27:34,  4.14s/it]
2025-02-04 06:17:27,393 - INFO - tqdm - perplexity: 4.2946, batch_loss: 1.5126, loss: 1.4574 ||:  23%|##3       | 119/516 [09:23<27:48,  4.20s/it]
2025-02-04 06:17:41,136 - INFO - tqdm - perplexity: 4.2941, batch_loss: 1.3932, loss: 1.4572 ||:  24%|##3       | 122/516 [09:37<28:59,  4.42s/it]
2025-02-04 06:17:54,965 - INFO - tqdm - perplexity: 4.2958, batch_loss: 1.3725, loss: 1.4576 ||:  24%|##4       | 125/516 [09:50<29:57,  4.60s/it]
2025-02-04 06:18:07,856 - INFO - tqdm - perplexity: 4.2994, batch_loss: 1.5401, loss: 1.4585 ||:  25%|##4       | 128/516 [10:03<28:47,  4.45s/it]
2025-02-04 06:18:18,852 - INFO - tqdm - perplexity: 4.3034, batch_loss: 1.5502, loss: 1.4594 ||:  25%|##5       | 130/516 [10:14<32:17,  5.02s/it]
2025-02-04 06:18:31,276 - INFO - tqdm - perplexity: 4.3023, batch_loss: 1.4540, loss: 1.4591 ||:  26%|##5       | 132/516 [10:27<36:33,  5.71s/it]
2025-02-04 06:18:47,076 - INFO - tqdm - perplexity: 4.2913, batch_loss: 1.1251, loss: 1.4566 ||:  26%|##6       | 135/516 [10:42<35:38,  5.61s/it]
2025-02-04 06:18:59,841 - INFO - tqdm - perplexity: 4.2918, batch_loss: 1.3353, loss: 1.4567 ||:  27%|##6       | 138/516 [10:55<30:20,  4.82s/it]
2025-02-04 06:19:15,021 - INFO - tqdm - perplexity: 4.2795, batch_loss: 1.2954, loss: 1.4538 ||:  27%|##7       | 141/516 [11:10<31:56,  5.11s/it]
2025-02-04 06:19:28,770 - INFO - tqdm - perplexity: 4.2763, batch_loss: 1.3968, loss: 1.4531 ||:  28%|##7       | 144/516 [11:24<30:04,  4.85s/it]
2025-02-04 06:19:39,684 - INFO - tqdm - perplexity: 4.2783, batch_loss: 1.4241, loss: 1.4536 ||:  28%|##8       | 146/516 [11:35<31:07,  5.05s/it]
2025-02-04 06:19:54,393 - INFO - tqdm - perplexity: 4.2780, batch_loss: 1.4643, loss: 1.4535 ||:  29%|##8       | 149/516 [11:50<30:17,  4.95s/it]
2025-02-04 06:20:06,900 - INFO - tqdm - perplexity: 4.2851, batch_loss: 1.6437, loss: 1.4551 ||:  29%|##9       | 152/516 [12:02<26:50,  4.42s/it]
2025-02-04 06:20:19,814 - INFO - tqdm - perplexity: 4.2833, batch_loss: 1.4031, loss: 1.4547 ||:  30%|###       | 155/516 [12:15<25:45,  4.28s/it]
2025-02-04 06:20:31,422 - INFO - tqdm - perplexity: 4.2836, batch_loss: 1.5487, loss: 1.4548 ||:  31%|###       | 158/516 [12:27<23:47,  3.99s/it]
2025-02-04 06:20:43,422 - INFO - tqdm - perplexity: 4.2862, batch_loss: 1.6197, loss: 1.4554 ||:  31%|###1      | 161/516 [12:39<23:23,  3.95s/it]
2025-02-04 06:20:54,088 - INFO - tqdm - perplexity: 4.2780, batch_loss: 1.2360, loss: 1.4535 ||:  32%|###1      | 163/516 [12:50<27:46,  4.72s/it]
2025-02-04 06:21:07,710 - INFO - tqdm - perplexity: 4.2863, batch_loss: 1.5239, loss: 1.4554 ||:  32%|###2      | 166/516 [13:03<27:06,  4.65s/it]
2025-02-04 06:21:18,537 - INFO - tqdm - perplexity: 4.2865, batch_loss: 1.5273, loss: 1.4555 ||:  33%|###2      | 169/516 [13:14<22:35,  3.91s/it]
2025-02-04 06:21:32,306 - INFO - tqdm - perplexity: 4.2783, batch_loss: 1.4156, loss: 1.4535 ||:  33%|###3      | 172/516 [13:28<25:28,  4.44s/it]
2025-02-04 06:21:45,353 - INFO - tqdm - perplexity: 4.2778, batch_loss: 1.4438, loss: 1.4535 ||:  34%|###3      | 175/516 [13:41<24:39,  4.34s/it]
2025-02-04 06:22:01,138 - INFO - tqdm - perplexity: 4.2679, batch_loss: 1.2618, loss: 1.4511 ||:  34%|###4      | 178/516 [13:57<28:18,  5.03s/it]
2025-02-04 06:22:12,969 - INFO - tqdm - perplexity: 4.2696, batch_loss: 1.5530, loss: 1.4515 ||:  35%|###4      | 180/516 [14:08<31:13,  5.57s/it]
2025-02-04 06:22:29,352 - INFO - tqdm - perplexity: 4.2728, batch_loss: 1.5397, loss: 1.4523 ||:  35%|###5      | 182/516 [14:25<39:00,  7.01s/it]
2025-02-04 06:22:43,763 - INFO - tqdm - perplexity: 4.2734, batch_loss: 1.5177, loss: 1.4524 ||:  36%|###5      | 184/516 [14:39<39:01,  7.05s/it]
2025-02-04 06:22:56,869 - INFO - tqdm - perplexity: 4.2748, batch_loss: 1.5447, loss: 1.4527 ||:  36%|###6      | 187/516 [14:52<28:44,  5.24s/it]
2025-02-04 06:23:09,560 - INFO - tqdm - perplexity: 4.2771, batch_loss: 1.5563, loss: 1.4533 ||:  37%|###6      | 190/516 [15:05<25:35,  4.71s/it]
2025-02-04 06:23:19,591 - INFO - tqdm - perplexity: 4.2756, batch_loss: 1.3416, loss: 1.4529 ||:  37%|###7      | 192/516 [15:15<25:55,  4.80s/it]
2025-02-04 06:23:34,520 - INFO - tqdm - perplexity: 4.2708, batch_loss: 1.1464, loss: 1.4518 ||:  38%|###7      | 194/516 [15:30<34:45,  6.48s/it]
2025-02-04 06:23:44,806 - INFO - tqdm - perplexity: 4.2671, batch_loss: 1.2729, loss: 1.4509 ||:  38%|###7      | 196/516 [15:40<30:51,  5.78s/it]
2025-02-04 06:23:54,977 - INFO - tqdm - perplexity: 4.2667, batch_loss: 1.4390, loss: 1.4508 ||:  38%|###8      | 198/516 [15:50<28:43,  5.42s/it]
2025-02-04 06:24:08,144 - INFO - tqdm - perplexity: 4.2692, batch_loss: 1.5291, loss: 1.4514 ||:  39%|###8      | 200/516 [16:04<32:17,  6.13s/it]
2025-02-04 06:24:18,596 - INFO - tqdm - perplexity: 4.2712, batch_loss: 1.4455, loss: 1.4519 ||:  39%|###9      | 202/516 [16:14<29:35,  5.65s/it]
2025-02-04 06:24:31,895 - INFO - tqdm - perplexity: 4.2756, batch_loss: 1.6013, loss: 1.4529 ||:  40%|###9      | 205/516 [16:27<25:31,  4.92s/it]
2025-02-04 06:24:43,669 - INFO - tqdm - perplexity: 4.2794, batch_loss: 1.6111, loss: 1.4538 ||:  40%|####      | 207/516 [16:39<28:12,  5.48s/it]
2025-02-04 06:24:56,622 - INFO - tqdm - perplexity: 4.2760, batch_loss: 1.3450, loss: 1.4530 ||:  41%|####      | 209/516 [16:52<30:38,  5.99s/it]
2025-02-04 06:25:11,237 - INFO - tqdm - perplexity: 4.2766, batch_loss: 1.4727, loss: 1.4532 ||:  41%|####1     | 212/516 [17:07<27:13,  5.37s/it]
2025-02-04 06:25:23,763 - INFO - tqdm - perplexity: 4.2756, batch_loss: 1.4705, loss: 1.4529 ||:  42%|####1     | 215/516 [17:19<22:46,  4.54s/it]
2025-02-04 06:25:37,111 - INFO - tqdm - perplexity: 4.2704, batch_loss: 1.4463, loss: 1.4517 ||:  42%|####2     | 217/516 [17:33<27:19,  5.48s/it]
2025-02-04 06:25:51,671 - INFO - tqdm - perplexity: 4.2608, batch_loss: 1.1368, loss: 1.4494 ||:  43%|####2     | 220/516 [17:47<24:59,  5.06s/it]
2025-02-04 06:26:02,126 - INFO - tqdm - perplexity: 4.2613, batch_loss: 1.3978, loss: 1.4496 ||:  43%|####3     | 222/516 [17:58<24:37,  5.03s/it]
2025-02-04 06:26:17,289 - INFO - tqdm - perplexity: 4.2558, batch_loss: 1.3443, loss: 1.4483 ||:  44%|####3     | 225/516 [18:13<25:24,  5.24s/it]
2025-02-04 06:26:29,850 - INFO - tqdm - perplexity: 4.2515, batch_loss: 1.4006, loss: 1.4473 ||:  44%|####3     | 227/516 [18:25<27:34,  5.72s/it]
2025-02-04 06:26:40,589 - INFO - tqdm - perplexity: 4.2550, batch_loss: 1.5931, loss: 1.4481 ||:  44%|####4     | 229/516 [18:36<26:48,  5.60s/it]
2025-02-04 06:26:53,242 - INFO - tqdm - perplexity: 4.2494, batch_loss: 1.3533, loss: 1.4468 ||:  45%|####4     | 232/516 [18:49<21:23,  4.52s/it]
2025-02-04 06:27:08,457 - INFO - tqdm - perplexity: 4.2467, batch_loss: 1.1495, loss: 1.4461 ||:  46%|####5     | 235/516 [19:04<23:25,  5.00s/it]
2025-02-04 06:27:20,646 - INFO - tqdm - perplexity: 4.2433, batch_loss: 1.4528, loss: 1.4453 ||:  46%|####5     | 237/516 [19:16<25:25,  5.47s/it]
2025-02-04 06:27:33,793 - INFO - tqdm - perplexity: 4.2450, batch_loss: 1.4407, loss: 1.4458 ||:  47%|####6     | 240/516 [19:29<21:36,  4.70s/it]
2025-02-04 06:27:46,559 - INFO - tqdm - perplexity: 4.2395, batch_loss: 1.3034, loss: 1.4444 ||:  47%|####7     | 243/516 [19:42<19:51,  4.37s/it]
2025-02-04 06:28:02,416 - INFO - tqdm - perplexity: 4.2482, batch_loss: 1.5354, loss: 1.4465 ||:  48%|####7     | 246/516 [19:58<23:52,  5.31s/it]
2025-02-04 06:28:17,062 - INFO - tqdm - perplexity: 4.2522, batch_loss: 1.7026, loss: 1.4474 ||:  48%|####8     | 249/516 [20:12<22:31,  5.06s/it]
2025-02-04 06:28:31,832 - INFO - tqdm - perplexity: 4.2533, batch_loss: 1.4903, loss: 1.4477 ||:  49%|####8     | 252/516 [20:27<22:07,  5.03s/it]
2025-02-04 06:28:47,158 - INFO - tqdm - perplexity: 4.2512, batch_loss: 1.2130, loss: 1.4472 ||:  49%|####9     | 255/516 [20:43<22:04,  5.07s/it]
2025-02-04 06:29:00,561 - INFO - tqdm - perplexity: 4.2467, batch_loss: 1.3617, loss: 1.4461 ||:  50%|#####     | 258/516 [20:56<20:15,  4.71s/it]
2025-02-04 06:29:10,805 - INFO - tqdm - perplexity: 4.2457, batch_loss: 1.4789, loss: 1.4459 ||:  50%|#####     | 260/516 [21:06<20:45,  4.86s/it]
2025-02-04 06:29:23,011 - INFO - tqdm - perplexity: 4.2440, batch_loss: 1.5361, loss: 1.4455 ||:  51%|#####     | 263/516 [21:18<17:57,  4.26s/it]
2025-02-04 06:29:36,065 - INFO - tqdm - perplexity: 4.2414, batch_loss: 1.4046, loss: 1.4449 ||:  52%|#####1    | 266/516 [21:31<17:41,  4.25s/it]
2025-02-04 06:29:49,438 - INFO - tqdm - perplexity: 4.2380, batch_loss: 1.5678, loss: 1.4441 ||:  52%|#####2    | 269/516 [21:45<17:52,  4.34s/it]
2025-02-04 06:30:00,780 - INFO - tqdm - perplexity: 4.2352, batch_loss: 1.2653, loss: 1.4434 ||:  53%|#####2    | 271/516 [21:56<20:53,  5.12s/it]
2025-02-04 06:30:15,332 - INFO - tqdm - perplexity: 4.2305, batch_loss: 1.3195, loss: 1.4423 ||:  53%|#####3    | 274/516 [22:11<20:15,  5.02s/it]
2025-02-04 06:30:28,677 - INFO - tqdm - perplexity: 4.2328, batch_loss: 1.6111, loss: 1.4429 ||:  54%|#####3    | 277/516 [22:24<18:44,  4.70s/it]
2025-02-04 06:30:42,429 - INFO - tqdm - perplexity: 4.2318, batch_loss: 1.4943, loss: 1.4426 ||:  54%|#####4    | 280/516 [22:38<18:08,  4.61s/it]
2025-02-04 06:30:52,637 - INFO - tqdm - perplexity: 4.2322, batch_loss: 1.3491, loss: 1.4427 ||:  55%|#####4    | 282/516 [22:48<18:36,  4.77s/it]
2025-02-04 06:31:03,623 - INFO - tqdm - perplexity: 4.2292, batch_loss: 1.2573, loss: 1.4420 ||:  55%|#####5    | 284/516 [22:59<20:15,  5.24s/it]
2025-02-04 06:31:18,497 - INFO - tqdm - perplexity: 4.2308, batch_loss: 1.2801, loss: 1.4424 ||:  56%|#####5    | 287/516 [23:14<20:14,  5.31s/it]
2025-02-04 06:31:35,806 - INFO - tqdm - perplexity: 4.2294, batch_loss: 1.4522, loss: 1.4421 ||:  56%|#####6    | 289/516 [23:31<26:47,  7.08s/it]
2025-02-04 06:31:50,897 - INFO - tqdm - perplexity: 4.2237, batch_loss: 1.4011, loss: 1.4407 ||:  57%|#####6    | 292/516 [23:46<21:07,  5.66s/it]
2025-02-04 06:32:03,913 - INFO - tqdm - perplexity: 4.2182, batch_loss: 1.2710, loss: 1.4394 ||:  57%|#####6    | 294/516 [23:59<22:04,  5.97s/it]
2025-02-04 06:32:15,307 - INFO - tqdm - perplexity: 4.2160, batch_loss: 1.5464, loss: 1.4389 ||:  57%|#####7    | 296/516 [24:11<21:02,  5.74s/it]
2025-02-04 06:32:26,980 - INFO - tqdm - perplexity: 4.2164, batch_loss: 1.3907, loss: 1.4390 ||:  58%|#####7    | 298/516 [24:22<21:18,  5.86s/it]
2025-02-04 06:32:37,456 - INFO - tqdm - perplexity: 4.2140, batch_loss: 1.2868, loss: 1.4384 ||:  58%|#####8    | 300/516 [24:33<20:22,  5.66s/it]
2025-02-04 06:32:51,451 - INFO - tqdm - perplexity: 4.2146, batch_loss: 1.4382, loss: 1.4385 ||:  59%|#####8    | 303/516 [24:47<17:27,  4.92s/it]
2025-02-04 06:33:05,758 - INFO - tqdm - perplexity: 4.2127, batch_loss: 1.4763, loss: 1.4381 ||:  59%|#####9    | 306/516 [25:01<17:08,  4.90s/it]
2025-02-04 06:33:16,583 - INFO - tqdm - perplexity: 4.2152, batch_loss: 1.5756, loss: 1.4387 ||:  60%|#####9    | 308/516 [25:12<18:00,  5.20s/it]
2025-02-04 06:33:30,454 - INFO - tqdm - perplexity: 4.2173, batch_loss: 1.5215, loss: 1.4392 ||:  60%|######    | 311/516 [25:26<16:30,  4.83s/it]
2025-02-04 06:33:43,444 - INFO - tqdm - perplexity: 4.2161, batch_loss: 1.4568, loss: 1.4389 ||:  61%|######    | 314/516 [25:39<14:57,  4.44s/it]
2025-02-04 06:33:54,558 - INFO - tqdm - perplexity: 4.2194, batch_loss: 1.3791, loss: 1.4397 ||:  61%|######1   | 317/516 [25:50<13:05,  3.95s/it]
2025-02-04 06:34:07,525 - INFO - tqdm - perplexity: 4.2198, batch_loss: 1.2464, loss: 1.4398 ||:  62%|######2   | 320/516 [26:03<13:52,  4.25s/it]
2025-02-04 06:34:18,769 - INFO - tqdm - perplexity: 4.2159, batch_loss: 1.4572, loss: 1.4389 ||:  63%|######2   | 323/516 [26:14<12:16,  3.81s/it]
2025-02-04 06:34:30,594 - INFO - tqdm - perplexity: 4.2167, batch_loss: 1.4572, loss: 1.4391 ||:  63%|######3   | 326/516 [26:26<12:21,  3.91s/it]
2025-02-04 06:34:42,075 - INFO - tqdm - perplexity: 4.2148, batch_loss: 1.4140, loss: 1.4386 ||:  64%|######3   | 329/516 [26:37<12:08,  3.89s/it]
2025-02-04 06:34:53,520 - INFO - tqdm - perplexity: 4.2140, batch_loss: 1.4590, loss: 1.4384 ||:  64%|######4   | 332/516 [26:49<11:48,  3.85s/it]
2025-02-04 06:35:09,009 - INFO - tqdm - perplexity: 4.2107, batch_loss: 1.2133, loss: 1.4376 ||:  65%|######4   | 335/516 [27:04<14:33,  4.83s/it]
2025-02-04 06:35:22,438 - INFO - tqdm - perplexity: 4.2101, batch_loss: 1.3783, loss: 1.4375 ||:  66%|######5   | 338/516 [27:18<13:29,  4.55s/it]
2025-02-04 06:35:32,734 - INFO - tqdm - perplexity: 4.2086, batch_loss: 1.3225, loss: 1.4371 ||:  66%|######5   | 340/516 [27:28<14:10,  4.83s/it]
2025-02-04 06:35:46,439 - INFO - tqdm - perplexity: 4.2097, batch_loss: 1.5741, loss: 1.4374 ||:  66%|######6   | 343/516 [27:42<13:27,  4.67s/it]
2025-02-04 06:35:59,973 - INFO - tqdm - perplexity: 4.2086, batch_loss: 1.2948, loss: 1.4371 ||:  67%|######7   | 346/516 [27:55<13:02,  4.60s/it]
2025-02-04 06:36:15,630 - INFO - tqdm - perplexity: 4.2045, batch_loss: 1.1805, loss: 1.4361 ||:  68%|######7   | 349/516 [28:11<14:24,  5.18s/it]
2025-02-04 06:36:29,811 - INFO - tqdm - perplexity: 4.2040, batch_loss: 1.4441, loss: 1.4360 ||:  68%|######8   | 352/516 [28:25<13:24,  4.90s/it]
2025-02-04 06:36:43,404 - INFO - tqdm - perplexity: 4.2038, batch_loss: 1.3519, loss: 1.4360 ||:  69%|######8   | 354/516 [28:39<16:09,  5.98s/it]
2025-02-04 06:36:55,984 - INFO - tqdm - perplexity: 4.2022, batch_loss: 1.2955, loss: 1.4356 ||:  69%|######8   | 356/516 [28:51<16:35,  6.22s/it]
2025-02-04 06:37:08,097 - INFO - tqdm - perplexity: 4.1971, batch_loss: 1.2928, loss: 1.4344 ||:  70%|######9   | 359/516 [29:04<12:24,  4.74s/it]
2025-02-04 06:37:20,683 - INFO - tqdm - perplexity: 4.1959, batch_loss: 1.3973, loss: 1.4341 ||:  70%|#######   | 362/516 [29:16<11:11,  4.36s/it]
2025-02-04 06:37:32,189 - INFO - tqdm - perplexity: 4.1977, batch_loss: 1.4124, loss: 1.4345 ||:  71%|#######   | 365/516 [29:28<10:13,  4.06s/it]
2025-02-04 06:37:45,486 - INFO - tqdm - perplexity: 4.1943, batch_loss: 1.2334, loss: 1.4337 ||:  71%|#######1  | 368/516 [29:41<10:55,  4.43s/it]
2025-02-04 06:37:59,769 - INFO - tqdm - perplexity: 4.1910, batch_loss: 1.4117, loss: 1.4330 ||:  72%|#######1  | 371/516 [29:55<10:57,  4.53s/it]
2025-02-04 06:38:10,903 - INFO - tqdm - perplexity: 4.1913, batch_loss: 1.4736, loss: 1.4330 ||:  72%|#######2  | 374/516 [30:06<09:37,  4.07s/it]
2025-02-04 06:38:21,026 - INFO - tqdm - perplexity: 4.1921, batch_loss: 1.5163, loss: 1.4332 ||:  73%|#######2  | 376/516 [30:16<10:26,  4.48s/it]
2025-02-04 06:38:32,586 - INFO - tqdm - perplexity: 4.1912, batch_loss: 1.3571, loss: 1.4330 ||:  73%|#######3  | 379/516 [30:28<09:16,  4.06s/it]
2025-02-04 06:38:45,183 - INFO - tqdm - perplexity: 4.1918, batch_loss: 1.3735, loss: 1.4331 ||:  74%|#######4  | 382/516 [30:41<09:06,  4.08s/it]
2025-02-04 06:38:55,596 - INFO - tqdm - perplexity: 4.1914, batch_loss: 1.3712, loss: 1.4330 ||:  74%|#######4  | 384/516 [30:51<10:10,  4.63s/it]
2025-02-04 06:39:06,342 - INFO - tqdm - perplexity: 4.1908, batch_loss: 1.3967, loss: 1.4329 ||:  75%|#######5  | 387/516 [31:02<08:25,  3.92s/it]
2025-02-04 06:39:17,391 - INFO - tqdm - perplexity: 4.1881, batch_loss: 1.3380, loss: 1.4323 ||:  76%|#######5  | 390/516 [31:13<07:57,  3.79s/it]
2025-02-04 06:39:29,085 - INFO - tqdm - perplexity: 4.1879, batch_loss: 1.4685, loss: 1.4322 ||:  76%|#######6  | 393/516 [31:25<07:43,  3.77s/it]
2025-02-04 06:39:40,701 - INFO - tqdm - perplexity: 4.1858, batch_loss: 1.2609, loss: 1.4317 ||:  77%|#######6  | 396/516 [31:36<07:37,  3.81s/it]
2025-02-04 06:39:54,447 - INFO - tqdm - perplexity: 4.1838, batch_loss: 1.3929, loss: 1.4312 ||:  77%|#######7  | 399/516 [31:50<08:07,  4.17s/it]
2025-02-04 06:40:07,926 - INFO - tqdm - perplexity: 4.1789, batch_loss: 1.2406, loss: 1.4300 ||:  78%|#######7  | 402/516 [32:03<08:27,  4.45s/it]
2025-02-04 06:40:18,237 - INFO - tqdm - perplexity: 4.1738, batch_loss: 1.2818, loss: 1.4288 ||:  78%|#######8  | 404/516 [32:14<08:53,  4.76s/it]
2025-02-04 06:40:32,754 - INFO - tqdm - perplexity: 4.1695, batch_loss: 1.3842, loss: 1.4278 ||:  79%|#######8  | 407/516 [32:28<08:36,  4.74s/it]
2025-02-04 06:40:43,704 - INFO - tqdm - perplexity: 4.1693, batch_loss: 1.4394, loss: 1.4277 ||:  79%|#######9  | 410/516 [32:39<07:05,  4.02s/it]
2025-02-04 06:40:56,244 - INFO - tqdm - perplexity: 4.1671, batch_loss: 1.4194, loss: 1.4272 ||:  80%|########  | 413/516 [32:52<07:06,  4.15s/it]
2025-02-04 06:41:08,459 - INFO - tqdm - perplexity: 4.1680, batch_loss: 1.4168, loss: 1.4274 ||:  81%|########  | 416/516 [33:04<07:04,  4.25s/it]
2025-02-04 06:41:21,704 - INFO - tqdm - perplexity: 4.1673, batch_loss: 1.3757, loss: 1.4273 ||:  81%|########1 | 419/516 [33:17<07:08,  4.41s/it]
2025-02-04 06:41:35,102 - INFO - tqdm - perplexity: 4.1687, batch_loss: 1.6456, loss: 1.4276 ||:  82%|########1 | 422/516 [33:31<06:46,  4.33s/it]
2025-02-04 06:41:49,352 - INFO - tqdm - perplexity: 4.1645, batch_loss: 1.3374, loss: 1.4266 ||:  82%|########2 | 425/516 [33:45<07:02,  4.64s/it]
2025-02-04 06:42:01,013 - INFO - tqdm - perplexity: 4.1626, batch_loss: 1.1996, loss: 1.4261 ||:  83%|########2 | 428/516 [33:56<06:05,  4.15s/it]
2025-02-04 06:42:13,396 - INFO - tqdm - perplexity: 4.1640, batch_loss: 1.5357, loss: 1.4265 ||:  84%|########3 | 431/516 [34:09<05:49,  4.12s/it]
2025-02-04 06:42:25,339 - INFO - tqdm - perplexity: 4.1637, batch_loss: 1.3468, loss: 1.4264 ||:  84%|########4 | 434/516 [34:21<05:40,  4.15s/it]
2025-02-04 06:42:36,407 - INFO - tqdm - perplexity: 4.1597, batch_loss: 1.3110, loss: 1.4254 ||:  85%|########4 | 437/516 [34:32<05:04,  3.85s/it]
2025-02-04 06:42:50,803 - INFO - tqdm - perplexity: 4.1551, batch_loss: 1.2493, loss: 1.4243 ||:  85%|########5 | 440/516 [34:46<05:47,  4.57s/it]
2025-02-04 06:43:02,800 - INFO - tqdm - perplexity: 4.1536, batch_loss: 1.2174, loss: 1.4240 ||:  86%|########5 | 443/516 [34:58<05:06,  4.20s/it]
2025-02-04 06:43:18,591 - INFO - tqdm - perplexity: 4.1495, batch_loss: 1.2753, loss: 1.4230 ||:  86%|########6 | 446/516 [35:14<05:54,  5.07s/it]
2025-02-04 06:43:30,691 - INFO - tqdm - perplexity: 4.1471, batch_loss: 1.2515, loss: 1.4224 ||:  87%|########7 | 449/516 [35:26<04:52,  4.36s/it]
2025-02-04 06:43:43,773 - INFO - tqdm - perplexity: 4.1426, batch_loss: 1.2427, loss: 1.4213 ||:  88%|########7 | 452/516 [35:39<04:35,  4.31s/it]
2025-02-04 06:43:56,701 - INFO - tqdm - perplexity: 4.1406, batch_loss: 1.3663, loss: 1.4208 ||:  88%|########8 | 455/516 [35:52<04:18,  4.24s/it]
2025-02-04 06:44:10,910 - INFO - tqdm - perplexity: 4.1370, batch_loss: 1.2979, loss: 1.4200 ||:  89%|########8 | 458/516 [36:06<04:29,  4.64s/it]
2025-02-04 06:44:22,132 - INFO - tqdm - perplexity: 4.1379, batch_loss: 1.3713, loss: 1.4202 ||:  89%|########9 | 461/516 [36:18<03:45,  4.10s/it]
2025-02-04 06:44:35,742 - INFO - tqdm - perplexity: 4.1352, batch_loss: 1.3465, loss: 1.4195 ||:  90%|########9 | 464/516 [36:31<03:53,  4.49s/it]
2025-02-04 06:44:48,050 - INFO - tqdm - perplexity: 4.1343, batch_loss: 1.4293, loss: 1.4193 ||:  91%|######### | 467/516 [36:43<03:27,  4.24s/it]
2025-02-04 06:44:58,686 - INFO - tqdm - perplexity: 4.1344, batch_loss: 1.3153, loss: 1.4193 ||:  91%|######### | 469/516 [36:54<03:43,  4.74s/it]
2025-02-04 06:45:12,066 - INFO - tqdm - perplexity: 4.1320, batch_loss: 1.3710, loss: 1.4188 ||:  91%|#########1| 472/516 [37:07<03:13,  4.41s/it]
2025-02-04 06:45:27,464 - INFO - tqdm - perplexity: 4.1286, batch_loss: 1.2150, loss: 1.4179 ||:  92%|#########2| 475/516 [37:23<03:25,  5.00s/it]
2025-02-04 06:45:39,975 - INFO - tqdm - perplexity: 4.1296, batch_loss: 1.4757, loss: 1.4182 ||:  93%|#########2| 478/516 [37:35<02:47,  4.41s/it]
2025-02-04 06:45:54,492 - INFO - tqdm - perplexity: 4.1285, batch_loss: 1.2856, loss: 1.4179 ||:  93%|#########3| 481/516 [37:50<02:46,  4.76s/it]
2025-02-04 06:46:06,243 - INFO - tqdm - perplexity: 4.1282, batch_loss: 1.3487, loss: 1.4178 ||:  94%|#########3| 484/516 [38:02<02:12,  4.14s/it]
2025-02-04 06:46:17,852 - INFO - tqdm - perplexity: 4.1286, batch_loss: 1.4150, loss: 1.4179 ||:  94%|#########4| 487/516 [38:13<01:54,  3.95s/it]
2025-02-04 06:46:29,167 - INFO - tqdm - perplexity: 4.1262, batch_loss: 1.4974, loss: 1.4174 ||:  95%|#########4| 490/516 [38:25<01:38,  3.80s/it]
2025-02-04 06:46:40,345 - INFO - tqdm - perplexity: 4.1257, batch_loss: 1.3032, loss: 1.4172 ||:  96%|#########5| 493/516 [38:36<01:26,  3.75s/it]
2025-02-04 06:46:50,533 - INFO - tqdm - perplexity: 4.1223, batch_loss: 1.1433, loss: 1.4164 ||:  96%|#########5| 495/516 [38:46<01:33,  4.47s/it]
2025-02-04 06:47:04,525 - INFO - tqdm - perplexity: 4.1194, batch_loss: 1.2476, loss: 1.4157 ||:  97%|#########6| 498/516 [39:00<01:24,  4.71s/it]
2025-02-04 06:47:17,250 - INFO - tqdm - perplexity: 4.1178, batch_loss: 1.3759, loss: 1.4153 ||:  97%|#########7| 501/516 [39:13<01:05,  4.38s/it]
2025-02-04 06:47:30,357 - INFO - tqdm - perplexity: 4.1157, batch_loss: 1.3810, loss: 1.4148 ||:  98%|#########7| 504/516 [39:26<00:52,  4.41s/it]
2025-02-04 06:47:41,951 - INFO - tqdm - perplexity: 4.1126, batch_loss: 1.3013, loss: 1.4141 ||:  98%|#########8| 507/516 [39:37<00:35,  3.99s/it]
2025-02-04 06:47:54,201 - INFO - tqdm - perplexity: 4.1116, batch_loss: 1.3578, loss: 1.4138 ||:  99%|#########8| 510/516 [39:50<00:24,  4.16s/it]
2025-02-04 06:48:06,402 - INFO - tqdm - perplexity: 4.1117, batch_loss: 1.5904, loss: 1.4138 ||:  99%|#########9| 513/516 [40:02<00:12,  4.12s/it]
2025-02-04 06:48:10,251 - INFO - tqdm - perplexity: 4.1113, batch_loss: 1.3616, loss: 1.4137 ||: 100%|#########9| 514/516 [40:06<00:08,  4.04s/it]
2025-02-04 06:48:14,312 - INFO - tqdm - perplexity: 4.1105, batch_loss: 1.3177, loss: 1.4135 ||: 100%|#########9| 515/516 [40:10<00:04,  4.04s/it]
2025-02-04 06:48:15,119 - INFO - tqdm - perplexity: 4.1091, batch_loss: 1.2347, loss: 1.4132 ||: 100%|##########| 516/516 [40:11<00:00,  3.07s/it]
2025-02-04 06:48:15,119 - INFO - tqdm - perplexity: 4.1091, batch_loss: 1.2347, loss: 1.4132 ||: 100%|##########| 516/516 [40:11<00:00,  4.67s/it]
2025-02-04 06:48:15,128 - INFO - allennlp.training.gradient_descent_trainer - Validating
2025-02-04 06:48:15,128 - INFO - tqdm - 0%|          | 0/129 [00:00<?, ?it/s]
2025-02-04 06:48:25,335 - INFO - tqdm - perplexity: 9.6525, batch_loss: 2.5848, loss: 2.2672 ||:   8%|7         | 10/129 [00:10<02:00,  1.01s/it]
2025-02-04 06:48:35,593 - INFO - tqdm - perplexity: 10.2944, batch_loss: 2.0172, loss: 2.3316 ||:  16%|#5        | 20/129 [00:20<01:55,  1.06s/it]
2025-02-04 06:48:46,479 - INFO - tqdm - perplexity: 10.2490, batch_loss: 2.3711, loss: 2.3272 ||:  24%|##4       | 31/129 [00:31<01:39,  1.01s/it]
2025-02-04 06:48:56,756 - INFO - tqdm - perplexity: 10.2480, batch_loss: 2.4777, loss: 2.3271 ||:  32%|###1      | 41/129 [00:41<01:30,  1.03s/it]
2025-02-04 06:49:06,880 - INFO - tqdm - perplexity: 10.2660, batch_loss: 1.7683, loss: 2.3288 ||:  40%|###9      | 51/129 [00:51<01:17,  1.01it/s]
2025-02-04 06:49:17,000 - INFO - tqdm - perplexity: 10.3317, batch_loss: 2.5454, loss: 2.3352 ||:  47%|####7     | 61/129 [01:01<01:06,  1.02it/s]
2025-02-04 06:49:27,559 - INFO - tqdm - perplexity: 10.2201, batch_loss: 2.5175, loss: 2.3244 ||:  55%|#####5    | 71/129 [01:12<01:01,  1.06s/it]
2025-02-04 06:49:37,592 - INFO - tqdm - perplexity: 10.0593, batch_loss: 2.0491, loss: 2.3085 ||:  63%|######2   | 81/129 [01:22<00:46,  1.03it/s]
2025-02-04 06:49:47,847 - INFO - tqdm - perplexity: 10.0270, batch_loss: 2.1823, loss: 2.3053 ||:  71%|#######   | 91/129 [01:32<00:39,  1.05s/it]
2025-02-04 06:49:58,710 - INFO - tqdm - perplexity: 10.0547, batch_loss: 2.2423, loss: 2.3080 ||:  78%|#######8  | 101/129 [01:43<00:28,  1.04s/it]
2025-02-04 06:50:08,971 - INFO - tqdm - perplexity: 10.0007, batch_loss: 2.2841, loss: 2.3027 ||:  86%|########6 | 111/129 [01:53<00:19,  1.09s/it]
2025-02-04 06:50:20,059 - INFO - tqdm - perplexity: 9.9814, batch_loss: 2.1838, loss: 2.3007 ||:  94%|#########3| 121/129 [02:04<00:08,  1.10s/it]
2025-02-04 06:50:28,033 - INFO - tqdm - perplexity: 9.9048, batch_loss: 1.8363, loss: 2.2930 ||: 100%|##########| 129/129 [02:12<00:00,  1.02s/it]
2025-02-04 06:50:28,033 - INFO - tqdm - perplexity: 9.9048, batch_loss: 1.8363, loss: 2.2930 ||: 100%|##########| 129/129 [02:12<00:00,  1.03s/it]
2025-02-04 06:50:28,035 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2025-02-04 06:50:28,035 - INFO - allennlp.training.callbacks.console_logger - loss               |     1.413  |     2.293
2025-02-04 06:50:28,035 - INFO - allennlp.training.callbacks.console_logger - perplexity         |     4.109  |     9.905
2025-02-04 06:50:28,035 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |     0.000  |       N/A
2025-02-04 06:50:28,621 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:42:24.545670
2025-02-04 06:50:28,621 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 17:57:41
2025-02-04 06:50:28,621 - INFO - allennlp.training.gradient_descent_trainer - Epoch 8/31
2025-02-04 06:50:28,622 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 0B
2025-02-04 06:50:28,622 - INFO - allennlp.training.gradient_descent_trainer - Training
2025-02-04 06:50:28,622 - INFO - tqdm - 0%|          | 0/516 [00:00<?, ?it/s]
2025-02-04 06:50:41,800 - INFO - tqdm - perplexity: 3.2696, batch_loss: 1.1927, loss: 1.1847 ||:   1%|          | 3/516 [00:13<38:26,  4.50s/it]
2025-02-04 06:50:58,253 - INFO - tqdm - perplexity: 3.1450, batch_loss: 1.1659, loss: 1.1458 ||:   1%|1         | 6/516 [00:29<46:27,  5.47s/it]
2025-02-04 06:51:09,764 - INFO - tqdm - perplexity: 3.2459, batch_loss: 1.2575, loss: 1.1774 ||:   2%|1         | 9/516 [00:41<36:17,  4.29s/it]
2025-02-04 06:51:20,259 - INFO - tqdm - perplexity: 3.1860, batch_loss: 1.1058, loss: 1.1588 ||:   2%|2         | 11/516 [00:51<40:00,  4.75s/it]
2025-02-04 06:51:34,138 - INFO - tqdm - perplexity: 3.1821, batch_loss: 1.2485, loss: 1.1575 ||:   3%|2         | 14/516 [01:05<39:11,  4.68s/it]
2025-02-04 06:51:44,847 - INFO - tqdm - perplexity: 3.1537, batch_loss: 1.1826, loss: 1.1486 ||:   3%|3         | 17/516 [01:16<32:55,  3.96s/it]
2025-02-04 06:51:56,863 - INFO - tqdm - perplexity: 3.1891, batch_loss: 1.1451, loss: 1.1597 ||:   4%|3         | 20/516 [01:28<32:50,  3.97s/it]
2025-02-04 06:52:10,015 - INFO - tqdm - perplexity: 3.2070, batch_loss: 1.1726, loss: 1.1653 ||:   4%|4         | 23/516 [01:41<35:38,  4.34s/it]
2025-02-04 06:52:22,700 - INFO - tqdm - perplexity: 3.2259, batch_loss: 1.1528, loss: 1.1712 ||:   5%|5         | 26/516 [01:54<35:24,  4.34s/it]
2025-02-04 06:52:36,441 - INFO - tqdm - perplexity: 3.2410, batch_loss: 1.0933, loss: 1.1759 ||:   6%|5         | 29/516 [02:07<37:07,  4.57s/it]
2025-02-04 06:52:48,962 - INFO - tqdm - perplexity: 3.2362, batch_loss: 1.1982, loss: 1.1744 ||:   6%|6         | 32/516 [02:20<34:52,  4.32s/it]
2025-02-04 06:52:59,928 - INFO - tqdm - perplexity: 3.2785, batch_loss: 1.4161, loss: 1.1874 ||:   7%|6         | 35/516 [02:31<31:40,  3.95s/it]
2025-02-04 06:53:11,123 - INFO - tqdm - perplexity: 3.2610, batch_loss: 1.2506, loss: 1.1820 ||:   7%|7         | 38/516 [02:42<30:01,  3.77s/it]
2025-02-04 06:53:23,586 - INFO - tqdm - perplexity: 3.2754, batch_loss: 1.1305, loss: 1.1865 ||:   8%|7         | 41/516 [02:54<31:41,  4.00s/it]
2025-02-04 06:53:36,761 - INFO - tqdm - perplexity: 3.2923, batch_loss: 1.3318, loss: 1.1916 ||:   9%|8         | 44/516 [03:08<34:51,  4.43s/it]
2025-02-04 06:53:50,720 - INFO - tqdm - perplexity: 3.2817, batch_loss: 1.2250, loss: 1.1884 ||:   9%|9         | 47/516 [03:22<36:00,  4.61s/it]
2025-02-04 06:54:02,046 - INFO - tqdm - perplexity: 3.2877, batch_loss: 1.2269, loss: 1.1902 ||:  10%|9         | 50/516 [03:33<31:35,  4.07s/it]
2025-02-04 06:54:14,515 - INFO - tqdm - perplexity: 3.2718, batch_loss: 1.1501, loss: 1.1853 ||:  10%|#         | 53/516 [03:45<31:08,  4.04s/it]
2025-02-04 06:54:27,992 - INFO - tqdm - perplexity: 3.2673, batch_loss: 1.0994, loss: 1.1840 ||:  11%|#         | 56/516 [03:59<33:12,  4.33s/it]
2025-02-04 06:54:41,245 - INFO - tqdm - perplexity: 3.2547, batch_loss: 1.0742, loss: 1.1801 ||:  11%|#1        | 59/516 [04:12<34:51,  4.58s/it]
2025-02-04 06:54:53,194 - INFO - tqdm - perplexity: 3.2488, batch_loss: 1.1484, loss: 1.1783 ||:  12%|#2        | 62/516 [04:24<31:40,  4.19s/it]
2025-02-04 06:55:05,892 - INFO - tqdm - perplexity: 3.2471, batch_loss: 1.2106, loss: 1.1778 ||:  13%|#2        | 65/516 [04:37<30:59,  4.12s/it]
2025-02-04 06:55:16,476 - INFO - tqdm - perplexity: 3.2364, batch_loss: 1.1325, loss: 1.1745 ||:  13%|#2        | 67/516 [04:47<34:31,  4.61s/it]
2025-02-04 06:55:28,043 - INFO - tqdm - perplexity: 3.2437, batch_loss: 1.2228, loss: 1.1767 ||:  14%|#3        | 70/516 [04:59<31:07,  4.19s/it]
2025-02-04 06:55:39,541 - INFO - tqdm - perplexity: 3.2472, batch_loss: 1.2945, loss: 1.1778 ||:  14%|#4        | 73/516 [05:10<28:45,  3.90s/it]
2025-02-04 06:55:51,868 - INFO - tqdm - perplexity: 3.2549, batch_loss: 1.2773, loss: 1.1802 ||:  15%|#4        | 76/516 [05:23<29:43,  4.05s/it]
2025-02-04 06:56:01,887 - INFO - tqdm - perplexity: 3.2477, batch_loss: 1.0380, loss: 1.1779 ||:  15%|#5        | 78/516 [05:33<32:38,  4.47s/it]
2025-02-04 06:56:14,700 - INFO - tqdm - perplexity: 3.2568, batch_loss: 1.1570, loss: 1.1807 ||:  16%|#5        | 81/516 [05:46<31:28,  4.34s/it]
2025-02-04 06:56:26,942 - INFO - tqdm - perplexity: 3.2594, batch_loss: 1.1219, loss: 1.1816 ||:  16%|#6        | 84/516 [05:58<29:35,  4.11s/it]
2025-02-04 06:56:37,520 - INFO - tqdm - perplexity: 3.2577, batch_loss: 1.1437, loss: 1.1810 ||:  17%|#6        | 86/516 [06:08<34:51,  4.86s/it]
2025-02-04 06:56:49,557 - INFO - tqdm - perplexity: 3.2669, batch_loss: 1.2159, loss: 1.1839 ||:  17%|#7        | 89/516 [06:20<30:14,  4.25s/it]
2025-02-04 06:57:02,977 - INFO - tqdm - perplexity: 3.2581, batch_loss: 1.0857, loss: 1.1811 ||:  18%|#7        | 92/516 [06:34<31:30,  4.46s/it]
2025-02-04 06:57:17,294 - INFO - tqdm - perplexity: 3.2503, batch_loss: 1.0626, loss: 1.1788 ||:  18%|#8        | 95/516 [06:48<33:40,  4.80s/it]
2025-02-04 06:57:28,666 - INFO - tqdm - perplexity: 3.2473, batch_loss: 1.1728, loss: 1.1778 ||:  19%|#8        | 98/516 [07:00<28:38,  4.11s/it]
2025-02-04 06:57:39,387 - INFO - tqdm - perplexity: 3.2491, batch_loss: 1.0916, loss: 1.1784 ||:  20%|#9        | 101/516 [07:10<25:31,  3.69s/it]
2025-02-04 06:57:53,489 - INFO - tqdm - perplexity: 3.2558, batch_loss: 1.2450, loss: 1.1804 ||:  20%|##        | 104/516 [07:24<29:24,  4.28s/it]
2025-02-04 06:58:06,031 - INFO - tqdm - perplexity: 3.2532, batch_loss: 1.0841, loss: 1.1796 ||:  21%|##        | 107/516 [07:37<28:49,  4.23s/it]
2025-02-04 06:58:19,921 - INFO - tqdm - perplexity: 3.2524, batch_loss: 1.1928, loss: 1.1794 ||:  21%|##1       | 110/516 [07:51<29:36,  4.38s/it]
2025-02-04 06:58:34,021 - INFO - tqdm - perplexity: 3.2463, batch_loss: 1.1232, loss: 1.1775 ||:  22%|##1       | 113/516 [08:05<31:05,  4.63s/it]
2025-02-04 06:58:48,501 - INFO - tqdm - perplexity: 3.2466, batch_loss: 1.1091, loss: 1.1776 ||:  22%|##2       | 116/516 [08:19<31:53,  4.78s/it]
2025-02-04 06:59:00,317 - INFO - tqdm - perplexity: 3.2477, batch_loss: 1.1424, loss: 1.1779 ||:  23%|##3       | 119/516 [08:31<28:08,  4.25s/it]
2025-02-04 06:59:14,649 - INFO - tqdm - perplexity: 3.2490, batch_loss: 1.1808, loss: 1.1783 ||:  24%|##3       | 122/516 [08:46<30:49,  4.69s/it]
2025-02-04 06:59:25,691 - INFO - tqdm - perplexity: 3.2412, batch_loss: 0.9978, loss: 1.1759 ||:  24%|##4       | 124/516 [08:57<33:22,  5.11s/it]
2025-02-04 06:59:37,542 - INFO - tqdm - perplexity: 3.2410, batch_loss: 1.2142, loss: 1.1759 ||:  25%|##4       | 127/516 [09:08<27:59,  4.32s/it]
2025-02-04 06:59:51,210 - INFO - tqdm - perplexity: 3.2382, batch_loss: 1.1236, loss: 1.1750 ||:  25%|##5       | 130/516 [09:22<28:53,  4.49s/it]
2025-02-04 07:00:02,291 - INFO - tqdm - perplexity: 3.2364, batch_loss: 1.2662, loss: 1.1745 ||:  26%|##5       | 133/516 [09:33<25:12,  3.95s/it]
2025-02-04 07:00:13,702 - INFO - tqdm - perplexity: 3.2354, batch_loss: 1.1577, loss: 1.1742 ||:  26%|##6       | 136/516 [09:45<25:06,  3.96s/it]
2025-02-04 07:00:27,136 - INFO - tqdm - perplexity: 3.2367, batch_loss: 1.2425, loss: 1.1745 ||:  27%|##6       | 139/516 [09:58<27:07,  4.32s/it]
2025-02-04 07:00:38,314 - INFO - tqdm - perplexity: 3.2347, batch_loss: 1.1439, loss: 1.1739 ||:  28%|##7       | 142/516 [10:09<24:12,  3.88s/it]
2025-02-04 07:00:51,119 - INFO - tqdm - perplexity: 3.2342, batch_loss: 1.2440, loss: 1.1738 ||:  28%|##8       | 145/516 [10:22<24:59,  4.04s/it]
2025-02-04 07:01:01,198 - INFO - tqdm - perplexity: 3.2420, batch_loss: 1.2060, loss: 1.1762 ||:  28%|##8       | 147/516 [10:32<27:46,  4.52s/it]
2025-02-04 07:01:12,073 - INFO - tqdm - perplexity: 3.2417, batch_loss: 1.1321, loss: 1.1761 ||:  29%|##9       | 150/516 [10:43<24:05,  3.95s/it]
2025-02-04 07:01:22,569 - INFO - tqdm - perplexity: 3.2467, batch_loss: 1.1063, loss: 1.1776 ||:  30%|##9       | 153/516 [10:53<21:40,  3.58s/it]
2025-02-04 07:01:33,890 - INFO - tqdm - perplexity: 3.2470, batch_loss: 1.0889, loss: 1.1777 ||:  30%|###       | 156/516 [11:05<22:00,  3.67s/it]
2025-02-04 07:01:44,391 - INFO - tqdm - perplexity: 3.2471, batch_loss: 1.1542, loss: 1.1778 ||:  31%|###       | 159/516 [11:15<21:33,  3.62s/it]
2025-02-04 07:01:59,548 - INFO - tqdm - perplexity: 3.2420, batch_loss: 0.9695, loss: 1.1762 ||:  31%|###1      | 162/516 [11:30<26:54,  4.56s/it]
2025-02-04 07:02:11,977 - INFO - tqdm - perplexity: 3.2396, batch_loss: 1.1657, loss: 1.1754 ||:  32%|###1      | 165/516 [11:43<25:24,  4.34s/it]
2025-02-04 07:02:24,238 - INFO - tqdm - perplexity: 3.2404, batch_loss: 1.0777, loss: 1.1757 ||:  33%|###2      | 168/516 [11:55<25:07,  4.33s/it]
2025-02-04 07:02:35,683 - INFO - tqdm - perplexity: 3.2435, batch_loss: 1.1601, loss: 1.1766 ||:  33%|###3      | 171/516 [12:07<23:24,  4.07s/it]
2025-02-04 07:02:48,519 - INFO - tqdm - perplexity: 3.2428, batch_loss: 1.1514, loss: 1.1764 ||:  34%|###3      | 174/516 [12:19<24:16,  4.26s/it]
2025-02-04 07:02:58,610 - INFO - tqdm - perplexity: 3.2392, batch_loss: 1.0412, loss: 1.1753 ||:  34%|###4      | 176/516 [12:29<26:30,  4.68s/it]
2025-02-04 07:03:11,467 - INFO - tqdm - perplexity: 3.2381, batch_loss: 1.1570, loss: 1.1750 ||:  35%|###4      | 179/516 [12:42<25:17,  4.50s/it]
2025-02-04 07:03:21,631 - INFO - tqdm - perplexity: 3.2328, batch_loss: 1.1239, loss: 1.1733 ||:  35%|###5      | 181/516 [12:53<26:16,  4.71s/it]
2025-02-04 07:03:34,100 - INFO - tqdm - perplexity: 3.2351, batch_loss: 1.2083, loss: 1.1741 ||:  36%|###5      | 184/516 [13:05<24:18,  4.39s/it]
2025-02-04 07:03:47,104 - INFO - tqdm - perplexity: 3.2371, batch_loss: 1.0568, loss: 1.1747 ||:  36%|###6      | 187/516 [13:18<24:33,  4.48s/it]
2025-02-04 07:03:58,898 - INFO - tqdm - perplexity: 3.2379, batch_loss: 1.1393, loss: 1.1749 ||:  37%|###6      | 190/516 [13:30<22:11,  4.08s/it]
2025-02-04 07:04:11,584 - INFO - tqdm - perplexity: 3.2354, batch_loss: 1.1393, loss: 1.1742 ||:  37%|###7      | 193/516 [13:42<22:21,  4.15s/it]
2025-02-04 07:04:23,377 - INFO - tqdm - perplexity: 3.2351, batch_loss: 1.1961, loss: 1.1741 ||:  38%|###7      | 196/516 [13:54<21:04,  3.95s/it]
2025-02-04 07:04:37,013 - INFO - tqdm - perplexity: 3.2355, batch_loss: 1.1946, loss: 1.1742 ||:  39%|###8      | 199/516 [14:08<23:19,  4.42s/it]
2025-02-04 07:04:51,369 - INFO - tqdm - perplexity: 3.2346, batch_loss: 1.1778, loss: 1.1739 ||:  39%|###9      | 202/516 [14:22<24:57,  4.77s/it]
2025-02-04 07:05:03,383 - INFO - tqdm - perplexity: 3.2305, batch_loss: 1.1109, loss: 1.1726 ||:  40%|###9      | 205/516 [14:34<22:18,  4.30s/it]
2025-02-04 07:05:16,358 - INFO - tqdm - perplexity: 3.2292, batch_loss: 1.1139, loss: 1.1722 ||:  40%|####      | 208/516 [14:47<22:01,  4.29s/it]
2025-02-04 07:05:28,535 - INFO - tqdm - perplexity: 3.2266, batch_loss: 1.1782, loss: 1.1714 ||:  41%|####      | 211/516 [14:59<20:34,  4.05s/it]
2025-02-04 07:05:41,200 - INFO - tqdm - perplexity: 3.2285, batch_loss: 1.1033, loss: 1.1720 ||:  41%|####1     | 214/516 [15:12<20:55,  4.16s/it]
2025-02-04 07:05:54,099 - INFO - tqdm - perplexity: 3.2249, batch_loss: 1.1566, loss: 1.1709 ||:  42%|####2     | 217/516 [15:25<21:09,  4.25s/it]
2025-02-04 07:06:06,172 - INFO - tqdm - perplexity: 3.2240, batch_loss: 1.0970, loss: 1.1706 ||:  43%|####2     | 220/516 [15:37<19:51,  4.03s/it]
2025-02-04 07:06:21,682 - INFO - tqdm - perplexity: 3.2205, batch_loss: 0.8957, loss: 1.1695 ||:  43%|####3     | 223/516 [15:53<24:16,  4.97s/it]
2025-02-04 07:06:34,435 - INFO - tqdm - perplexity: 3.2167, batch_loss: 1.1944, loss: 1.1684 ||:  44%|####3     | 226/516 [16:05<21:46,  4.50s/it]
2025-02-04 07:06:48,159 - INFO - tqdm - perplexity: 3.2174, batch_loss: 1.2508, loss: 1.1686 ||:  44%|####4     | 229/516 [16:19<21:40,  4.53s/it]
2025-02-04 07:06:58,329 - INFO - tqdm - perplexity: 3.2126, batch_loss: 0.9655, loss: 1.1671 ||:  45%|####4     | 231/516 [16:29<22:32,  4.75s/it]
2025-02-04 07:07:10,831 - INFO - tqdm - perplexity: 3.2125, batch_loss: 1.1867, loss: 1.1671 ||:  45%|####5     | 234/516 [16:42<20:28,  4.36s/it]
2025-02-04 07:07:23,320 - INFO - tqdm - perplexity: 3.2114, batch_loss: 1.1309, loss: 1.1667 ||:  46%|####5     | 236/516 [16:54<24:35,  5.27s/it]
2025-02-04 07:07:35,737 - INFO - tqdm - perplexity: 3.2108, batch_loss: 1.1921, loss: 1.1665 ||:  46%|####6     | 239/516 [17:07<21:01,  4.56s/it]
2025-02-04 07:07:48,755 - INFO - tqdm - perplexity: 3.2084, batch_loss: 1.0002, loss: 1.1658 ||:  47%|####6     | 242/516 [17:20<20:35,  4.51s/it]
2025-02-04 07:08:02,721 - INFO - tqdm - perplexity: 3.2100, batch_loss: 1.3016, loss: 1.1663 ||:  47%|####7     | 245/516 [17:34<20:47,  4.60s/it]
2025-02-04 07:08:16,608 - INFO - tqdm - perplexity: 3.2104, batch_loss: 1.2283, loss: 1.1664 ||:  48%|####8     | 248/516 [17:47<20:24,  4.57s/it]
2025-02-04 07:08:29,248 - INFO - tqdm - perplexity: 3.2112, batch_loss: 1.3542, loss: 1.1667 ||:  49%|####8     | 251/516 [18:00<18:50,  4.27s/it]
2025-02-04 07:08:40,038 - INFO - tqdm - perplexity: 3.2119, batch_loss: 0.9058, loss: 1.1669 ||:  49%|####9     | 253/516 [18:11<21:30,  4.91s/it]
2025-02-04 07:08:53,448 - INFO - tqdm - perplexity: 3.2127, batch_loss: 1.1610, loss: 1.1671 ||:  50%|####9     | 256/516 [18:24<19:56,  4.60s/it]
2025-02-04 07:09:05,516 - INFO - tqdm - perplexity: 3.2148, batch_loss: 1.1673, loss: 1.1678 ||:  50%|#####     | 259/516 [18:36<18:07,  4.23s/it]
2025-02-04 07:09:15,556 - INFO - tqdm - perplexity: 3.2135, batch_loss: 1.0070, loss: 1.1674 ||:  51%|#####     | 261/516 [18:46<19:51,  4.67s/it]
2025-02-04 07:09:27,908 - INFO - tqdm - perplexity: 3.2131, batch_loss: 1.2136, loss: 1.1672 ||:  51%|#####1    | 264/516 [18:59<18:23,  4.38s/it]
2025-02-04 07:09:38,202 - INFO - tqdm - perplexity: 3.2109, batch_loss: 1.0861, loss: 1.1666 ||:  52%|#####1    | 266/516 [19:09<20:10,  4.84s/it]
2025-02-04 07:09:50,353 - INFO - tqdm - perplexity: 3.2118, batch_loss: 1.1876, loss: 1.1668 ||:  52%|#####2    | 269/516 [19:21<17:52,  4.34s/it]
2025-02-04 07:10:03,107 - INFO - tqdm - perplexity: 3.2101, batch_loss: 1.1724, loss: 1.1663 ||:  53%|#####2    | 272/516 [19:34<17:16,  4.25s/it]
2025-02-04 07:10:13,246 - INFO - tqdm - perplexity: 3.2054, batch_loss: 0.7636, loss: 1.1648 ||:  53%|#####2    | 273/516 [19:44<24:21,  6.01s/it]
2025-02-04 07:10:24,994 - INFO - tqdm - perplexity: 3.2044, batch_loss: 1.2022, loss: 1.1645 ||:  53%|#####3    | 276/516 [19:56<18:47,  4.70s/it]
2025-02-04 07:10:37,560 - INFO - tqdm - perplexity: 3.2058, batch_loss: 1.0952, loss: 1.1650 ||:  54%|#####4    | 279/516 [20:08<17:04,  4.32s/it]
2025-02-04 07:10:51,149 - INFO - tqdm - perplexity: 3.2021, batch_loss: 1.0643, loss: 1.1638 ||:  55%|#####4    | 282/516 [20:22<17:11,  4.41s/it]
2025-02-04 07:11:04,315 - INFO - tqdm - perplexity: 3.2017, batch_loss: 1.2471, loss: 1.1637 ||:  55%|#####5    | 285/516 [20:35<16:36,  4.32s/it]
2025-02-04 07:11:16,093 - INFO - tqdm - perplexity: 3.2013, batch_loss: 1.0981, loss: 1.1636 ||:  56%|#####5    | 288/516 [20:47<15:30,  4.08s/it]
2025-02-04 07:11:29,312 - INFO - tqdm - perplexity: 3.2014, batch_loss: 1.1142, loss: 1.1636 ||:  56%|#####6    | 291/516 [21:00<16:05,  4.29s/it]
2025-02-04 07:11:44,244 - INFO - tqdm - perplexity: 3.2058, batch_loss: 1.1961, loss: 1.1650 ||:  57%|#####6    | 294/516 [21:15<17:34,  4.75s/it]
2025-02-04 07:11:56,308 - INFO - tqdm - perplexity: 3.2083, batch_loss: 1.3309, loss: 1.1657 ||:  58%|#####7    | 297/516 [21:27<15:27,  4.23s/it]
2025-02-04 07:12:07,012 - INFO - tqdm - perplexity: 3.2075, batch_loss: 1.1481, loss: 1.1655 ||:  58%|#####7    | 299/516 [21:38<17:35,  4.86s/it]
2025-02-04 07:12:20,120 - INFO - tqdm - perplexity: 3.2066, batch_loss: 1.1494, loss: 1.1652 ||:  59%|#####8    | 302/516 [21:51<16:34,  4.65s/it]
2025-02-04 07:12:32,223 - INFO - tqdm - perplexity: 3.2043, batch_loss: 1.0745, loss: 1.1645 ||:  59%|#####9    | 305/516 [22:03<15:12,  4.32s/it]
2025-02-04 07:12:42,770 - INFO - tqdm - perplexity: 3.2074, batch_loss: 1.2255, loss: 1.1655 ||:  60%|#####9    | 308/516 [22:14<13:07,  3.78s/it]
2025-02-04 07:12:55,692 - INFO - tqdm - perplexity: 3.2069, batch_loss: 1.0936, loss: 1.1653 ||:  60%|######    | 311/516 [22:27<14:06,  4.13s/it]
2025-02-04 07:13:09,901 - INFO - tqdm - perplexity: 3.2055, batch_loss: 1.0321, loss: 1.1649 ||:  61%|######    | 314/516 [22:41<15:27,  4.59s/it]
2025-02-04 07:13:21,010 - INFO - tqdm - perplexity: 3.2049, batch_loss: 1.0963, loss: 1.1647 ||:  61%|######1   | 316/516 [22:52<17:12,  5.16s/it]
2025-02-04 07:13:31,587 - INFO - tqdm - perplexity: 3.2051, batch_loss: 1.0757, loss: 1.1647 ||:  62%|######1   | 319/516 [23:02<13:38,  4.16s/it]
2025-02-04 07:13:44,377 - INFO - tqdm - perplexity: 3.2025, batch_loss: 0.9436, loss: 1.1639 ||:  62%|######2   | 322/516 [23:15<13:17,  4.11s/it]
2025-02-04 07:13:55,876 - INFO - tqdm - perplexity: 3.2012, batch_loss: 1.1754, loss: 1.1635 ||:  63%|######2   | 325/516 [23:27<12:25,  3.90s/it]
2025-02-04 07:14:10,249 - INFO - tqdm - perplexity: 3.2001, batch_loss: 1.1289, loss: 1.1632 ||:  64%|######3   | 328/516 [23:41<13:56,  4.45s/it]
2025-02-04 07:14:25,330 - INFO - tqdm - perplexity: 3.2022, batch_loss: 1.1711, loss: 1.1638 ||:  64%|######4   | 331/516 [23:56<15:21,  4.98s/it]
2025-02-04 07:14:36,532 - INFO - tqdm - perplexity: 3.2018, batch_loss: 1.2652, loss: 1.1637 ||:  65%|######4   | 333/516 [24:07<16:13,  5.32s/it]
2025-02-04 07:14:49,342 - INFO - tqdm - perplexity: 3.1985, batch_loss: 1.1410, loss: 1.1627 ||:  65%|######5   | 336/516 [24:20<13:53,  4.63s/it]
2025-02-04 07:14:59,501 - INFO - tqdm - perplexity: 3.1990, batch_loss: 1.0543, loss: 1.1628 ||:  66%|######5   | 338/516 [24:30<14:24,  4.86s/it]
2025-02-04 07:15:12,836 - INFO - tqdm - perplexity: 3.2000, batch_loss: 1.2253, loss: 1.1632 ||:  66%|######6   | 341/516 [24:44<13:10,  4.52s/it]
2025-02-04 07:15:23,864 - INFO - tqdm - perplexity: 3.1992, batch_loss: 1.1442, loss: 1.1629 ||:  66%|######6   | 343/516 [24:55<14:25,  5.00s/it]
2025-02-04 07:15:36,660 - INFO - tqdm - perplexity: 3.1983, batch_loss: 1.0956, loss: 1.1626 ||:  67%|######7   | 346/516 [25:08<13:04,  4.61s/it]
2025-02-04 07:15:49,063 - INFO - tqdm - perplexity: 3.1970, batch_loss: 1.0802, loss: 1.1622 ||:  68%|######7   | 349/516 [25:20<11:34,  4.16s/it]
2025-02-04 07:16:01,639 - INFO - tqdm - perplexity: 3.1968, batch_loss: 1.2554, loss: 1.1622 ||:  68%|######8   | 352/516 [25:33<11:27,  4.19s/it]
2025-02-04 07:16:12,537 - INFO - tqdm - perplexity: 3.1980, batch_loss: 1.1998, loss: 1.1625 ||:  69%|######8   | 355/516 [25:43<10:14,  3.82s/it]
2025-02-04 07:16:23,396 - INFO - tqdm - perplexity: 3.1980, batch_loss: 1.1775, loss: 1.1625 ||:  69%|######9   | 357/516 [25:54<12:06,  4.57s/it]
2025-02-04 07:16:33,851 - INFO - tqdm - perplexity: 3.1964, batch_loss: 1.1648, loss: 1.1620 ||:  70%|######9   | 359/516 [26:05<12:35,  4.81s/it]
2025-02-04 07:16:46,301 - INFO - tqdm - perplexity: 3.1964, batch_loss: 1.1375, loss: 1.1620 ||:  70%|#######   | 362/516 [26:17<11:23,  4.44s/it]
2025-02-04 07:16:56,765 - INFO - tqdm - perplexity: 3.1943, batch_loss: 1.0842, loss: 1.1614 ||:  71%|#######   | 364/516 [26:28<12:00,  4.74s/it]
2025-02-04 07:17:10,950 - INFO - tqdm - perplexity: 3.1916, batch_loss: 1.0910, loss: 1.1605 ||:  71%|#######1  | 367/516 [26:42<11:47,  4.75s/it]
2025-02-04 07:17:23,137 - INFO - tqdm - perplexity: 3.1903, batch_loss: 1.0663, loss: 1.1601 ||:  72%|#######1  | 369/516 [26:54<13:24,  5.47s/it]
2025-02-04 07:17:34,782 - INFO - tqdm - perplexity: 3.1892, batch_loss: 0.9832, loss: 1.1598 ||:  72%|#######1  | 371/516 [27:06<13:41,  5.67s/it]
2025-02-04 07:17:45,482 - INFO - tqdm - perplexity: 3.1867, batch_loss: 1.1247, loss: 1.1590 ||:  72%|#######2  | 373/516 [27:16<13:10,  5.53s/it]
2025-02-04 07:17:58,091 - INFO - tqdm - perplexity: 3.1876, batch_loss: 1.2120, loss: 1.1593 ||:  73%|#######2  | 376/516 [27:29<10:46,  4.62s/it]
2025-02-04 07:18:11,003 - INFO - tqdm - perplexity: 3.1879, batch_loss: 1.0506, loss: 1.1594 ||:  73%|#######3  | 379/516 [27:42<10:19,  4.52s/it]
2025-02-04 07:18:24,135 - INFO - tqdm - perplexity: 3.1862, batch_loss: 1.0754, loss: 1.1588 ||:  74%|#######4  | 382/516 [27:55<09:46,  4.38s/it]
